{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Multi-Label Video Classification** <a class=\"tocSkip\"></a>\n",
    "\n",
    "## A TensorFlow 2.0 Journey, and More <a class=\"tocSkip\"></a>\n",
    "\n",
    "*Kyle Chung*\n",
    "\n",
    "*2019-07-22 04:40:50 Last Updated*\n",
    "\n",
    "------\n",
    "\n",
    "This is a project I've done in the open project week during my training on Goole's Advanced Solutions Labs at Singapore site.\n",
    "The dataset used is [YouTue-8M](https://research.google.com/youtube8m/): a public dataset created by Google Research.\n",
    "It contains pre-processed video features in [TensorFlow TFRecord](https://www.tensorflow.org/tutorials/load_data/tf_records) format.\n",
    "\n",
    "These are the stuff I've done during the week:\n",
    "\n",
    "1. Use [TensorFlow](https://github.com/tensorflow/tensorflow) to build a baseline model for youtube video label prediction\n",
    "2. Deploy the model onto Google Cloud [AI Platform](https://cloud.google.com/ai-platform/) as a prediction service\n",
    "3. Create an asynchronous web appilication that user can input a youtube video share link and get the predicted labels\n",
    "\n",
    "This notebook is a final wrap-up of the project, with some additional follow-up improvement.\n",
    "It can also serve as a reference point for my experience on TensorFlow 2.0, which is still in its beta stage as write-up of this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.0-beta1\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "logging.getLogger(\"tensorflow\").setLevel(logging.ERROR)\n",
    "\n",
    "#  Configure array print width.\n",
    "np.set_printoptions(edgeitems=2)\n",
    "np.core.arrayprint._line_width = 160\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deal with TFRecords\n",
    "\n",
    "TFRecords are an efficient way of storing data and performing batch training in TensorFlow modeling.\n",
    "The advatnage is that it can handle virtually any data type, not limited to a tabular representation.\n",
    "Of course it comes at a cost: We do need some extra engineering effort to handle data in tfrecord format.\n",
    "\n",
    "In this section we'll talk about how to read, create, and process data in tfrecord format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-rw-rw- 1 kylechung kylechung 4819410 Jul 20 07:41 data/video/train0000.tfrecord\n",
      "-rw-rw-rw- 1 kylechung kylechung 4729969 Jul 20 06:49 data/video/train0001.tfrecord\n",
      "-rw-rw-rw- 1 kylechung kylechung 4763009 Jul 20 07:54 data/video/train0002.tfrecord\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "# List some files from youtube-8m video-level training data.\n",
    "ls -d -l data/video/*  | grep \"train.*tfrecord$\" | head -n3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfrec_file = \"data/video/train0001.tfrecord\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspection of TFRecords\n",
    "\n",
    "To inspect a given tfrecord without any schema, it is best to take a single example and print its content in plain text.\n",
    "Since all kinds of features are serialized to byte strings in tfrecord, the inspection is agnostic to the original feature format.\n",
    "In TF 2.0 the data handling framework has been unified to the `tf.data` API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "limit_output": 500,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features {\n",
      "  feature {\n",
      "    key: \"id\"\n",
      "    value {\n",
      "      bytes_list {\n",
      "        value: \"2vab\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  feature {\n",
      "    key: \"labels\"\n",
      "    value {\n",
      "      int64_list {\n",
      "        value: 14\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  feature {\n",
      "    key: \"mean_audio\"\n",
      "    value {\n",
      "      float_list {\n",
      "        value: 0.4810151159763336\n",
      "        value: -0.1483372300863266\n",
      "        value: -0.6934748888015747\n",
      "        value: 1.2363330125808716\n",
      "        value: 0.53005450963974\n",
      "        value: 1.1766775846481323\n",
      "        value: -1.10115957\n"
     ]
    }
   ],
   "source": [
    "# Create a dataset iterator of tfrecords.\n",
    "rec_iter = tf.data.TFRecordDataset(tfrec_file)\n",
    "\n",
    "# Take just one example.\n",
    "# The return object is a Tensor of a single byte string.\n",
    "rec_ex = next(rec_iter.__iter__())\n",
    "\n",
    "# To parse it we'd like to get rid of the Tensor wrapping,\n",
    "# leaving the byte string alone.\n",
    "rec_ex = rec_ex.numpy()\n",
    "example = tf.train.Example()  # For sequence examples use tf.train.SequenceExample.\n",
    "example.ParseFromString(rec_ex)\n",
    "\n",
    "# The parsed result will be a human-readable protobuf.\n",
    "print(str(example)[:500])  # Truncated to print less."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parse TFRecords with Schema\n",
    "\n",
    "To really make use of data stored in tfrecords, we need to specify a schema to parse the underlying protobuf.\n",
    "We use `tf.io.FixedLenFeature` and `tf.io.VarLenFeature` to specify the nature of the features to be parsed, including its length, type, and dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': \"2vab\",\n",
      " 'labels': 'SparseTensor(indices=[[0]], values=[14], shape=[1])',\n",
      " 'mean_audio': [0.481015116 -0.14833723 -0.693474889 ... 0.0226273146 0.0394227207 0.36170435],\n",
      " 'mean_rgb': [0.388561159 -1.41187453 -0.171391308 ... 0.157069772 -0.233502612 -0.605695128]}\r\n"
     ]
    }
   ],
   "source": [
    "# Specify feature spec.\n",
    "video_feature_spec = {\n",
    "    \"id\": tf.io.FixedLenFeature([], tf.string),\n",
    "    \"labels\": tf.io.VarLenFeature(tf.int64),\n",
    "    \"mean_rgb\": tf.io.FixedLenFeature((1024,), tf.float32),\n",
    "    \"mean_audio\": tf.io.FixedLenFeature((128,), tf.float32)\n",
    "}\n",
    "\n",
    "# Take another example in the tfrecords.\n",
    "rec_ex = next(rec_iter.__iter__())\n",
    "\n",
    "# Parse with feature spec.\n",
    "# To parse multiple examples as a batch, use tf.io.parse_example instead.\n",
    "parsed_example = tf.io.parse_single_example(rec_ex, features=video_feature_spec)\n",
    "\n",
    "# The result is a dict of feature Tensors.\n",
    "tf.print(parsed_example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making Input Function for Modeling\n",
    "\n",
    "For training using custom `tf.estimator.Estimator` we need a so-called input function to handle the data pipeline.\n",
    "An input function takes the source files, convert records into `tf.data.Dataset` and apply necessary transofrmation.\n",
    "Conventionally the resulting dataset will return a tuple of features and labels, where the features are warpped in a dict of tensors and labels are purely a tensor.\n",
    "\n",
    "This time we will use [Feature Column](https://www.tensorflow.org/guide/feature_columns) to define our features, the corresponding feature spec can be derived from feature columns.\n",
    "\n",
    "One special treatment is required to encode the label tensor.\n",
    "Since the task is a multi-label classification, meaning that each example can have more than one correct labels, we need to transform the label tensor into a dense multi-hot encoding representation in order to perform loss calculation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'labels': VarLenFeature(dtype=tf.int64),\n",
      " 'mean_audio': FixedLenFeature(shape=(128,), dtype=tf.float32, default_value=None),\n",
      " 'mean_rgb': FixedLenFeature(shape=(1024,), dtype=tf.float32, default_value=None)}\r\n"
     ]
    }
   ],
   "source": [
    "from tensorflow import feature_column as fc\n",
    "\n",
    "# Define feature columns.\n",
    "# We will ignore the id column since we don't need it.\n",
    "# Also we drop the audio feature for now.\n",
    "feat_col_video = [\n",
    "    fc.numeric_column(key=\"mean_rgb\", shape=(1024,), dtype=tf.float32),\n",
    "    fc.numeric_column(key=\"mean_audio\", shape=(128,), dtype=tf.float32),\n",
    "    fc.indicator_column(fc.categorical_column_with_identity(\n",
    "        key=\"labels\", num_buckets=3862))\n",
    "]\n",
    "\n",
    "# Derive feature spec from feature columns.\n",
    "feat_spec_video = fc.make_parse_example_spec(feat_col_video)\n",
    "tf.print(feat_spec_video)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "multi_hot_encoder = tf.keras.layers.DenseFeatures(feat_col_video[-1])\n",
    "\n",
    "def _parse(examples, spec, batch_size, n_class, multi_hot_y):\n",
    "    features = tf.io.parse_example(examples, features=spec)\n",
    "    labels = features.pop(\"labels\")\n",
    "    if multi_hot_y:\n",
    "        labels = multi_hot_encoder({\"labels\": labels})\n",
    "    return features, labels\n",
    "\n",
    "def input_fn(infiles, spec, batch_size, n_class, multi_hot_y=True, mode=tf.estimator.ModeKeys.TRAIN):\n",
    "    dataset = tf.data.TFRecordDataset(tf.io.gfile.glob(infiles))\n",
    "    if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "        dataset = dataset.shuffle(buffer_size=1000).repeat(count=None)\n",
    "        dataset = dataset.batch(batch_size, drop_remainder=True)\n",
    "    else:\n",
    "        dataset = dataset.batch(batch_size, drop_remainder=False)\n",
    "    dataset = dataset.map(partial(_parse, spec=spec, batch_size=batch_size,\n",
    "                                  n_class=n_class, multi_hot_y=multi_hot_y))\n",
    "    dataset = dataset.prefetch(batch_size)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======Feature Dict======\n",
      "{'mean_audio': <tf.Tensor: id=184, shape=(32, 128), dtype=float32, numpy=\n",
      "array([[ 1.1536757 ,  0.07907027, ...,  1.1729966 , -1.1919962 ],\n",
      "       [-1.3457038 , -0.94000447, ..., -0.3672986 , -0.28494567],\n",
      "       ...,\n",
      "       [-0.9571638 ,  0.31504712, ...,  0.1950336 , -0.34763038],\n",
      "       [-0.8208411 , -0.21984763, ..., -0.27861887,  0.5389497 ]],\n",
      "      dtype=float32)>, 'mean_rgb': <tf.Tensor: id=185, shape=(32, 1024), dtype=float32, numpy=\n",
      "array([[ 0.71723384, -1.9921875 , ..., -1.360241  , -1.0664103 ],\n",
      "       [ 0.558453  , -0.08332475, ...,  0.13487132, -0.1268803 ],\n",
      "       ...,\n",
      "       [ 0.54121345, -0.7769069 , ...,  0.6358044 , -0.19725849],\n",
      "       [ 0.5345576 ,  1.3082047 , ...,  0.7280216 , -0.33963847]],\n",
      "      dtype=float32)>}\n",
      "======Label Tensor:======\n",
      "tf.Tensor(\n",
      "[[0. 0. ... 0. 0.]\n",
      " [1. 1. ... 0. 0.]\n",
      " ...\n",
      " [1. 1. ... 0. 0.]\n",
      " [0. 0. ... 0. 0.]], shape=(32, 3862), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# Test the input function.\n",
    "train_files = \"data/video/train*\"\n",
    "dataset = input_fn(train_files, spec=feat_spec_video, batch_size=32, n_class=3862)\n",
    "\n",
    "# Dataset is iterable.\n",
    "for x, y in dataset:\n",
    "    print(\"======Feature Dict======\")\n",
    "    print(x)\n",
    "    print(\"======Label Tensor:======\")\n",
    "    print(y)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we don't do multi-hot dense encoding, the resulting label tensor will be a sparse tensor recording the label indices with variable length.\n",
    "We can print also its dense representation for a clear view:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[  61 1059    0    0    0    0    0]\n",
      " [ 210    0    0    0    0    0    0]\n",
      " [   5   84  169    0    0    0    0]\n",
      " [   0    1    5   16    0    0    0]\n",
      " [  61    0    0    0    0    0    0]], shape=(5, 7), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "dataset_test = input_fn(train_files, spec=feat_spec_video,\n",
    "                        batch_size=32, n_class=3862, multi_hot_y=False)\n",
    "for x_test, y_test in dataset_test:\n",
    "    break\n",
    "\n",
    "print(tf.sparse.to_dense(y_test)[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Debug Network Layers\n",
    "\n",
    "We will use `tf.keras.layers` API to construct our model.\n",
    "Before we build the model, it is good to first observe how feature columns are parsed into network layers.\n",
    "We can use `tf.keras.layers.DenseFeatures` to create a layer from a list of feature columns.\n",
    "Essentially what it does is just to concatenate the vector to form a single tensor ready for connection to subsequent hidden layers.\n",
    "\n",
    "### Feed Forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[ 1.1536757   0.07907027 ... -1.360241   -1.0664103 ]\n",
      " [-1.3457038  -0.94000447 ...  0.13487132 -0.1268803 ]\n",
      " ...\n",
      " [-0.9571638   0.31504712 ...  0.6358044  -0.19725849]\n",
      " [-0.8208411  -0.21984763 ...  0.7280216  -0.33963847]], shape=(32, 1152), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# Specify feature columns that only contains features but not labels.\n",
    "xname = [\"mean_rgb\", \"mean_audio\"]\n",
    "feat_col_x = [col for col in feat_col_video if col.name in xname]\n",
    "\n",
    "# Create a layer out of feature columns.\n",
    "input_layer = tf.keras.layers.DenseFeatures(feat_col_x)\n",
    "\n",
    "# Feed the layer with one batch of features.\n",
    "print(input_layer(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a final output layer to have number of neurons equal to number of classes, which is 3862 in this dataset.\n",
    "And we feed the input layer derived from feature columns to this output layer to arrive the model output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[0.45948967 0.6169331  ... 0.5617791  0.43415514]\n",
      " [0.47317007 0.59798837 ... 0.4790778  0.56553406]\n",
      " ...\n",
      " [0.54685074 0.43166158 ... 0.64432395 0.58275044]\n",
      " [0.5436692  0.30143052 ... 0.59891266 0.47318396]], shape=(32, 3862), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "final_layer = tf.keras.layers.Dense(3862, activation=\"sigmoid\")\n",
    "\n",
    "# Feed-forward.\n",
    "logits = input_layer(x)\n",
    "output = final_layer(logits)\n",
    "\n",
    "# The network output.\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To verify what's going on here, we can extract the initialized weights and biases from the final layer.\n",
    "The output should be nothing more than just a sigmoid applying on a dot product of the input and final layer.\n",
    "Sp if we manually implement the matrix operation, the result shoudl coincide with that of using `tf.keras.layers`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[0.45948967 0.6169331  ... 0.5617791  0.43415514]\n",
      " [0.47317007 0.59798837 ... 0.4790778  0.56553406]\n",
      " ...\n",
      " [0.54685074 0.43166158 ... 0.64432395 0.58275044]\n",
      " [0.5436692  0.30143052 ... 0.59891266 0.47318396]], shape=(32, 3862), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# By default the bias is zero-initialized,\n",
    "# so it won't affect our comparison before the model start to learn.\n",
    "weights, biases = final_layer.get_weights()\n",
    "output_check = tf.sigmoid(tf.matmul(logits, weights) + biases)\n",
    "print(output_check)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss\n",
    "\n",
    "Now we have our output we can calculate the loss comparing to the label.\n",
    "There are lots of high level API for loss calculation in the `tf.keras.losses` module.\n",
    "Let's examine the API for binary cross entropy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.70918345\n"
     ]
    }
   ],
   "source": [
    "bce = tf.keras.losses.BinaryCrossentropy(from_logits=False)\n",
    "print(bce(y, output).numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cross entropy is defined as:\n",
    "\n",
    "$$\n",
    "\\mbox{Cross Entropy Loss} = - \\sum_{i=1}^N y_i\\ln(q_i) + (1 - y_i)\\ln(1 - q_i),\n",
    "$$\n",
    "\n",
    "where $q_i$ is the model predicted probability for the $i$-th example.\n",
    "\n",
    "For multi-label case the API simply apply the calculation across both batch and label dimension.\n",
    "That is, the summation is over both number of examples and number of labels.\n",
    "To verify the result, we can manually code the calculation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.70918363\n"
     ]
    }
   ],
   "source": [
    "# This is not numerically stable but we don't mind for a quick dirty check.\n",
    "z = y * tf.math.log(output) + (1 - y) * tf.math.log(1 - output)\n",
    "print(tf.reduce_mean(-z).numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The resulting number won't be exactly the same due to smoothing done by default in the API to avoid numerical instability.\n",
    "\n",
    "We can also use a lower level API from the `tf.keras.backend` module to gain more control on how we'd like to aggregate individual losses.\n",
    "For example, we may want to calculate instead the [Hamming Loss](https://en.wikipedia.org/wiki/Multi-label_classification#Statistics_and_evaluation_metrics) which suits more for a multi-label scenario.\n",
    "Essentially we are still calculating individual cross entropy, but will sum over the label dimension first, then do the averagging to arrive the final loss value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2738.8672\n"
     ]
    }
   ],
   "source": [
    "# Retain batch_size x n_class dimension.\n",
    "losses = tf.keras.backend.binary_crossentropy(y, output, from_logits=False)\n",
    "hamming_loss = tf.reduce_mean(tf.reduce_sum(-z, axis=1))\n",
    "print(hamming_loss.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation Metrics\n",
    "\n",
    "Evaluation metric is usually different from loss.\n",
    "The former is used to guide the model in how weights should be updated, while the latter is the final criteria to judge the performance of a trained model. (Of course the ideal case is to align the two but for practical reason this is not always doable.)\n",
    "\n",
    "The `tf.keras.metrics` module contains lots of built-in classifcal evaluation metrics at our disposal.\n",
    "To calculate the metric given a batch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.00069678505"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use precision as an example.\n",
    "# The result is really bad since we haven't started to train the model.\n",
    "# All weights are just randomly initialized.\n",
    "m = tf.keras.metrics.Precision()\n",
    "m.update_state(y_true=y, y_pred=output)\n",
    "m.result().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Models\n",
    "\n",
    "For model not very complicated and customized, `tf.keras.models` API is a very fast way for prototyping.\n",
    "We will simply connect the input layer to output layer, essentially build K-logistic regression model for K labels independently.\n",
    "This is a classical baseline model for a multi-label learning task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_files = \"data/video/train*\"\n",
    "valid_files = \"data/video/valid*\"\n",
    "\n",
    "train_dataset = input_fn(train_files, spec=feat_spec_video,\n",
    "                         batch_size=1024, n_class=3862)\n",
    "valid_dataset = input_fn(valid_files, spec=feat_spec_video,\n",
    "                         batch_size=1024, n_class=3862,\n",
    "                         mode=tf.estimator.ModeKeys.EVAL)\n",
    "\n",
    "l2_reg = tf.keras.regularizers.l2(1e-8)\n",
    "model = tf.keras.models.Sequential(name=\"baseline\")\n",
    "model.add(tf.keras.layers.DenseFeatures(feat_col_x))\n",
    "model.add(tf.keras.layers.Dense(3862, activation=\"sigmoid\",\n",
    "                                kernel_regularizer=l2_reg))\n",
    "model.compile(\n",
    "    optimizer=\"adam\",\n",
    "    loss=\"binary_crossentropy\",\n",
    "    metrics=[\n",
    "        tf.keras.metrics.Precision(),\n",
    "        tf.keras.metrics.Recall()\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To summarize the network architecture we can train a dummy step in order for the model to figure out the input dimension (since we don't bother define them in advance)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"baseline\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_features_2 (DenseFeatu multiple                  0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              multiple                  4452886   \n",
      "=================================================================\n",
      "Total params: 4,452,886\n",
      "Trainable params: 4,452,886\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.fit(dataset, epochs=1, steps_per_epoch=1, verbose=0)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - 2s 20ms/step - loss: 0.6672 - precision_5: 0.0012 - recall: 0.6014 - val_loss: 0.6315 - val_precision_5: 0.0016 - val_recall: 0.6446\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x13d9266e908>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test with more steps, also with validation.\n",
    "model.fit(dataset, validation_data=valid_dataset, epochs=1,\n",
    "          steps_per_epoch=100, validation_steps=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use Tensorboard with Keras Model\n",
    "\n",
    "To use `tensorboard` for keras model on tracking the model losses, we need to explicitly implement a callback function and pass it to the `fit` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Create directory to save pre-trained models.\n",
    "mkdir -p models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x22588fe68d0>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensorboard_callback = tf.keras.callbacks.TensorBoard()  # Default to ./logs.\n",
    "model.fit(dataset, validation_data=valid_dataset, epochs=1,\n",
    "          steps_per_epoch=100, validation_steps=10,\n",
    "          callbacks=[tensorboard_callback],\n",
    "          verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can run TensorBoard as usual:\n",
    "\n",
    "```sh\n",
    "tensorboard --logdir ./logs\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Loss Function\n",
    "\n",
    "We'd like to use Hamming Loss, optionally with class weight balancing since the labels are heavily skewed in this dataset.\n",
    "The class weight to fully re-balance the training data distribution for class $i$ is defined as:\n",
    "\n",
    "$$\n",
    "\\mbox{Class Weight i} = \\frac{\\mbox{No. of Samples}}{\\mbox{No. of Classes} \\times \\mbox{No. of Class i}}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "vocab_file = \"data/vocabulary.csv\"\n",
    "\n",
    "def calc_class_weight(infile, scale=1):\n",
    "    \"\"\"Calculate class weight to re-balance label distribution.\n",
    "    The class weight for class i (w_i) is determined by:\n",
    "    w_i = total no. samples / (n_class * count(class i))\n",
    "    \"\"\"\n",
    "    vocab = pd.read_csv(infile).sort_values(\"Index\")\n",
    "    cnt = vocab[\"TrainVideoCount\"]\n",
    "    w = cnt.sum() / (len(vocab) * cnt)\n",
    "    w = w.values.astype(np.float32)\n",
    "    return pow(w, scale)\n",
    "\n",
    "class_weights = calc_class_weight(vocab_file, scale=2)\n",
    "\n",
    "def hamming_loss(y_true, y_pred):\n",
    "    loss = tf.keras.backend.binary_crossentropy(y_true, y_pred, from_logits=False)\n",
    "    return tf.reduce_mean(tf.reduce_sum(loss, axis=1))\n",
    "\n",
    "def weighted_hamming_loss(y_true, y_pred):\n",
    "    loss = tf.keras.backend.binary_crossentropy(y_true, y_pred, from_logits=False)\n",
    "    loss *= class_weights  # By-element product broadcast over rows.\n",
    "    return tf.reduce_mean(tf.reduce_sum(loss, axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Evaluation Metrics\n",
    "\n",
    "There are two ways to supply custom evaluation metrics for a `keras` model.\n",
    "The first approach is to simply pass a function calculating the metric given `y_true` and `y_pred`, just similar as what we did for the custom loss function.\n",
    "This approach, however, has a flaw that it only calculates by-batch metrics.\n",
    "For a metric that will cumulate the results from multiple batches from the validation dataset,\n",
    "it is best to implement a stateful metric class as a subclass of `tf.keras.metrics.Metric`.\n",
    "\n",
    "For this multi-label task we'd like to imeplemnt a metric to health-check number of predicted classes for our model.\n",
    "It should not tru to predict too many labels on average since the overall average number of labels is only 3.\n",
    "\n",
    "We will also implement a \"Hit@One\" metric that calculate how many examples we have the opt predicted class to be a correct prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.ops import init_ops\n",
    "\n",
    "class AverageNClass(tf.keras.metrics.Metric):\n",
    "\n",
    "    def __init__(self, name=\"average_n_class\", **kwargs):\n",
    "        super(tf.keras.metrics.Metric, self).__init__(name=name, **kwargs)\n",
    "        self.n_example = self.add_weight(\n",
    "            \"n_example\",\n",
    "            shape=(),\n",
    "            dtype=tf.float32,\n",
    "            initializer=init_ops.zeros_initializer)\n",
    "        self.n_predicted_class = self.add_weight(\n",
    "            \"n_predicted_class\",\n",
    "            shape=(),\n",
    "            dtype=tf.float32,\n",
    "            initializer=init_ops.zeros_initializer)\n",
    "        \n",
    "    def update_state(self, y_true, y_pred, **kwargs):\n",
    "        # Accumulate sample size.\n",
    "        batch_size = tf.cast(len(y_true), tf.float32)\n",
    "        self.n_example.assign_add(batch_size)\n",
    "        # Accumulate number of predicted classes.\n",
    "        batch_n_class = tf.reduce_sum(tf.cast(y_pred > .5, tf.float32))\n",
    "        self.n_predicted_class.assign_add(batch_n_class)\n",
    "\n",
    "    def result(self):\n",
    "        return self.n_predicted_class / self.n_example\n",
    "\n",
    "class HitAtOne(tf.keras.metrics.Metric):\n",
    "\n",
    "    def __init__(self, name=\"hit_at_one\", **kwargs):\n",
    "        super(tf.keras.metrics.Metric, self).__init__(name=name, **kwargs)\n",
    "        self.n_example = self.add_weight(\n",
    "            \"n_example\",\n",
    "            shape=(),\n",
    "            dtype=tf.float32,\n",
    "            initializer=init_ops.zeros_initializer)\n",
    "        self.hit_at_one = self.add_weight(\n",
    "            \"hit_at_one\",\n",
    "            shape=(),\n",
    "            dtype=tf.float32,\n",
    "            initializer=init_ops.zeros_initializer)\n",
    "        \n",
    "    def update_state(self, y_true, y_pred, **kwargs):\n",
    "        # Accumulate sample size.\n",
    "        batch_size = tf.cast(len(y_true), tf.float32)\n",
    "        self.n_example.assign_add(batch_size)\n",
    "        # Count number of hit@one.\n",
    "        tops = tf.math.argmax(y_pred, axis=1, output_type=tf.int32)\n",
    "        top_idx = tf.stack([tf.range(len(y_true)), tops], axis=1)\n",
    "        hits = tf.gather_nd(y_true, indices=top_idx)\n",
    "        self.hit_at_one.assign_add(tf.reduce_sum(hits))\n",
    "\n",
    "    def result(self):\n",
    "        return self.hit_at_one / self.n_example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_baseline_model(name, loss_fn):\n",
    "    l2_reg = tf.keras.regularizers.l2(1e-8)\n",
    "    model = tf.keras.models.Sequential(name=\"baseline_\" + name)\n",
    "    model.add(tf.keras.layers.DenseFeatures(feat_col_x))\n",
    "    model.add(tf.keras.layers.Dense(3862, activation=\"sigmoid\", kernel_regularizer=l2_reg))\n",
    "    model.compile(\n",
    "        optimizer=\"adam\",\n",
    "        loss=loss_fn,\n",
    "        metrics=[\n",
    "            tf.keras.metrics.Precision(),\n",
    "            tf.keras.metrics.Recall(),\n",
    "            AverageNClass(),\n",
    "            HitAtOne()\n",
    "        ]\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline Benchmark\n",
    "\n",
    "Let's benchmark models with different losses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "3000/3000 [==============================] - 35s 12ms/step - loss: 0.2786 - precision_6: 0.0043 - recall_1: 0.5738 - average_n_class: 540.5688 - hit_at_one: 0.5958 - val_loss: 0.1014 - val_precision_6: 0.2591 - val_recall_1: 0.4638 - val_average_n_class: 5.3762 - val_hit_at_one: 0.7603\n",
      "Epoch 2/10\n",
      "3000/3000 [==============================] - 34s 11ms/step - loss: 0.0542 - precision_6: 0.4295 - recall_1: 0.4618 - average_n_class: 3.4680 - hit_at_one: 0.7635 - val_loss: 0.0260 - val_precision_6: 0.8545 - val_recall_1: 0.4749 - val_average_n_class: 1.6692 - val_hit_at_one: 0.7773\n",
      "Epoch 3/10\n",
      "3000/3000 [==============================] - 34s 11ms/step - loss: 0.0153 - precision_6: 0.8577 - recall_1: 0.4839 - average_n_class: 1.6979 - hit_at_one: 0.7840 - val_loss: 0.0085 - val_precision_6: 0.8523 - val_recall_1: 0.5070 - val_average_n_class: 1.7867 - val_hit_at_one: 0.7905\n",
      "Epoch 4/10\n",
      "3000/3000 [==============================] - 33s 11ms/step - loss: 0.0056 - precision_6: 0.8476 - recall_1: 0.5145 - average_n_class: 1.8253 - hit_at_one: 0.7944 - val_loss: 0.0038 - val_precision_6: 0.8444 - val_recall_1: 0.5359 - val_average_n_class: 1.9062 - val_hit_at_one: 0.7999\n",
      "Epoch 5/10\n",
      "3000/3000 [==============================] - 33s 11ms/step - loss: 0.0030 - precision_6: 0.8410 - recall_1: 0.5414 - average_n_class: 1.9377 - hit_at_one: 0.8048 - val_loss: 0.0025 - val_precision_6: 0.8375 - val_recall_1: 0.5550 - val_average_n_class: 1.9902 - val_hit_at_one: 0.8043\n",
      "Epoch 6/10\n",
      "3000/3000 [==============================] - 33s 11ms/step - loss: 0.0023 - precision_6: 0.8334 - recall_1: 0.5567 - average_n_class: 2.0151 - hit_at_one: 0.8078 - val_loss: 0.0021 - val_precision_6: 0.8305 - val_recall_1: 0.5650 - val_average_n_class: 2.0430 - val_hit_at_one: 0.8070\n",
      "Epoch 7/10\n",
      "3000/3000 [==============================] - 34s 11ms/step - loss: 0.0020 - precision_6: 0.8296 - recall_1: 0.5664 - average_n_class: 2.0555 - hit_at_one: 0.8118 - val_loss: 0.0020 - val_precision_6: 0.8291 - val_recall_1: 0.5739 - val_average_n_class: 2.0790 - val_hit_at_one: 0.8109\n",
      "Epoch 8/10\n",
      "3000/3000 [==============================] - 34s 11ms/step - loss: 0.0019 - precision_6: 0.8262 - recall_1: 0.5711 - average_n_class: 2.0820 - hit_at_one: 0.8132 - val_loss: 0.0019 - val_precision_6: 0.8289 - val_recall_1: 0.5774 - val_average_n_class: 2.0923 - val_hit_at_one: 0.8131\n",
      "Epoch 9/10\n",
      "3000/3000 [==============================] - 34s 11ms/step - loss: 0.0019 - precision_6: 0.8276 - recall_1: 0.5763 - average_n_class: 2.0963 - hit_at_one: 0.8136 - val_loss: 0.0019 - val_precision_6: 0.8309 - val_recall_1: 0.5779 - val_average_n_class: 2.0888 - val_hit_at_one: 0.8146\n",
      "Epoch 10/10\n",
      "3000/3000 [==============================] - 34s 11ms/step - loss: 0.0018 - precision_6: 0.8253 - recall_1: 0.5794 - average_n_class: 2.1116 - hit_at_one: 0.8165 - val_loss: 0.0018 - val_precision_6: 0.8293 - val_recall_1: 0.5830 - val_average_n_class: 2.1112 - val_hit_at_one: 0.8149\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x13f03f5bb00>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# NOTE:\n",
    "# For Windows we need to use backslash to avoid a known bug in tensorboard.\n",
    "model_1 = create_baseline_model(\"entropy\", loss_fn=tf.keras.losses.binary_crossentropy)\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=\"models\\eval\\entropy\")\n",
    "model_1.fit(dataset, validation_data=valid_dataset,\n",
    "            epochs=10, steps_per_epoch=3000, validation_steps=100,\n",
    "            callbacks=[tensorboard_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "3000/3000 [==============================] - 36s 12ms/step - loss: 1074.2928 - precision_7: 0.0043 - recall_2: 0.5775 - average_n_class: 541.7737 - hit_at_one: 0.6016 - val_loss: 390.2760 - val_precision_7: 0.2560 - val_recall_2: 0.4645 - val_average_n_class: 5.4499 - val_hit_at_one: 0.7604\n",
      "Epoch 2/10\n",
      "3000/3000 [==============================] - 34s 11ms/step - loss: 208.5407 - precision_7: 0.4269 - recall_2: 0.4628 - average_n_class: 3.4964 - hit_at_one: 0.7637 - val_loss: 99.6230 - val_precision_7: 0.8523 - val_recall_2: 0.4764 - val_average_n_class: 1.6788 - val_hit_at_one: 0.7775\n",
      "Epoch 3/10\n",
      "3000/3000 [==============================] - 34s 11ms/step - loss: 58.4210 - precision_7: 0.8555 - recall_2: 0.4855 - average_n_class: 1.7081 - hit_at_one: 0.7842 - val_loss: 32.2811 - val_precision_7: 0.8489 - val_recall_2: 0.5097 - val_average_n_class: 1.8035 - val_hit_at_one: 0.7909\n",
      "Epoch 4/10\n",
      "3000/3000 [==============================] - 35s 12ms/step - loss: 21.2595 - precision_7: 0.8432 - recall_2: 0.5177 - average_n_class: 1.8461 - hit_at_one: 0.7948 - val_loss: 14.1416 - val_precision_7: 0.8382 - val_recall_2: 0.5399 - val_average_n_class: 1.9345 - val_hit_at_one: 0.8005\n",
      "Epoch 5/10\n",
      "3000/3000 [==============================] - 35s 12ms/step - loss: 11.1543 - precision_7: 0.8340 - recall_2: 0.5463 - average_n_class: 1.9718 - hit_at_one: 0.8049 - val_loss: 9.1868 - val_precision_7: 0.8288 - val_recall_2: 0.5607 - val_average_n_class: 2.0318 - val_hit_at_one: 0.8050\n",
      "Epoch 6/10\n",
      "3000/3000 [==============================] - 35s 12ms/step - loss: 8.3916 - precision_7: 0.8247 - recall_2: 0.5626 - average_n_class: 2.0583 - hit_at_one: 0.8081 - val_loss: 7.7715 - val_precision_7: 0.8197 - val_recall_2: 0.5714 - val_average_n_class: 2.0937 - val_hit_at_one: 0.8076\n",
      "Epoch 7/10\n",
      "3000/3000 [==============================] - 35s 12ms/step - loss: 7.4818 - precision_7: 0.8184 - recall_2: 0.5728 - average_n_class: 2.1071 - hit_at_one: 0.8123 - val_loss: 7.2170 - val_precision_7: 0.8173 - val_recall_2: 0.5818 - val_average_n_class: 2.1380 - val_hit_at_one: 0.8118\n",
      "Epoch 8/10\n",
      "3000/3000 [==============================] - 35s 12ms/step - loss: 7.0988 - precision_7: 0.8145 - recall_2: 0.5791 - average_n_class: 2.1413 - hit_at_one: 0.8139 - val_loss: 6.9291 - val_precision_7: 0.8161 - val_recall_2: 0.5858 - val_average_n_class: 2.1558 - val_hit_at_one: 0.8137\n",
      "Epoch 9/10\n",
      "3000/3000 [==============================] - 35s 12ms/step - loss: 6.8626 - precision_7: 0.8147 - recall_2: 0.5849 - average_n_class: 2.1615 - hit_at_one: 0.8141 - val_loss: 6.7639 - val_precision_7: 0.8171 - val_recall_2: 0.5873 - val_average_n_class: 2.1586 - val_hit_at_one: 0.8155\n",
      "Epoch 10/10\n",
      "3000/3000 [==============================] - 35s 12ms/step - loss: 6.7318 - precision_7: 0.8105 - recall_2: 0.5882 - average_n_class: 2.1827 - hit_at_one: 0.8168 - val_loss: 6.6384 - val_precision_7: 0.8144 - val_recall_2: 0.5928 - val_average_n_class: 2.1864 - val_hit_at_one: 0.8157\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x13f0ac38630>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_2 = create_baseline_model(\"hamming\", loss_fn=hamming_loss)\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=\"models\\eval\\hamming\")\n",
    "model_2.fit(dataset, validation_data=valid_dataset,\n",
    "            epochs=10, steps_per_epoch=3000, validation_steps=100,\n",
    "            callbacks=[tensorboard_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "3000/3000 [==============================] - 36s 12ms/step - loss: 111214.3579 - precision_8: 0.0043 - recall_3: 0.5747 - average_n_class: 541.9702 - hit_at_one: 0.5927 - val_loss: 40131.6802 - val_precision_8: 0.2559 - val_recall_3: 0.4639 - val_average_n_class: 5.4438 - val_hit_at_one: 0.7607\n",
      "Epoch 2/10\n",
      "3000/3000 [==============================] - 34s 11ms/step - loss: 21210.8709 - precision_8: 0.4269 - recall_3: 0.4621 - average_n_class: 3.4910 - hit_at_one: 0.7638 - val_loss: 9865.4552 - val_precision_8: 0.8533 - val_recall_3: 0.4757 - val_average_n_class: 1.6741 - val_hit_at_one: 0.7775\n",
      "Epoch 3/10\n",
      "3000/3000 [==============================] - 34s 11ms/step - loss: 5564.9983 - precision_8: 0.8565 - recall_3: 0.4843 - average_n_class: 1.7019 - hit_at_one: 0.7845 - val_loss: 2835.9604 - val_precision_8: 0.8503 - val_recall_3: 0.5087 - val_average_n_class: 1.7968 - val_hit_at_one: 0.7910\n",
      "Epoch 4/10\n",
      "3000/3000 [==============================] - 34s 11ms/step - loss: 1681.4377 - precision_8: 0.8444 - recall_3: 0.5163 - average_n_class: 1.8385 - hit_at_one: 0.7948 - val_loss: 936.2227 - val_precision_8: 0.8396 - val_recall_3: 0.5387 - val_average_n_class: 1.9272 - val_hit_at_one: 0.8005\n",
      "Epoch 5/10\n",
      "3000/3000 [==============================] - 34s 11ms/step - loss: 619.2896 - precision_8: 0.8350 - recall_3: 0.5451 - average_n_class: 1.9649 - hit_at_one: 0.8049 - val_loss: 416.2811 - val_precision_8: 0.8298 - val_recall_3: 0.5593 - val_average_n_class: 2.0242 - val_hit_at_one: 0.8047\n",
      "Epoch 6/10\n",
      "3000/3000 [==============================] - 34s 11ms/step - loss: 332.9585 - precision_8: 0.8256 - recall_3: 0.5614 - average_n_class: 2.0514 - hit_at_one: 0.8080 - val_loss: 277.6604 - val_precision_8: 0.8209 - val_recall_3: 0.5699 - val_average_n_class: 2.0851 - val_hit_at_one: 0.8076\n",
      "Epoch 7/10\n",
      "3000/3000 [==============================] - 34s 11ms/step - loss: 250.4427 - precision_8: 0.8195 - recall_3: 0.5714 - average_n_class: 2.0993 - hit_at_one: 0.8124 - val_loss: 231.7478 - val_precision_8: 0.8185 - val_recall_3: 0.5803 - val_average_n_class: 2.1294 - val_hit_at_one: 0.8115\n",
      "Epoch 8/10\n",
      "3000/3000 [==============================] - 34s 11ms/step - loss: 218.3221 - precision_8: 0.8159 - recall_3: 0.5776 - average_n_class: 2.1321 - hit_at_one: 0.8136 - val_loss: 210.8218 - val_precision_8: 0.8173 - val_recall_3: 0.5843 - val_average_n_class: 2.1471 - val_hit_at_one: 0.8134\n",
      "Epoch 9/10\n",
      "3000/3000 [==============================] - 35s 12ms/step - loss: 204.6644 - precision_8: 0.8156 - recall_3: 0.5836 - average_n_class: 2.1544 - hit_at_one: 0.8146 - val_loss: 199.5716 - val_precision_8: 0.8183 - val_recall_3: 0.5858 - val_average_n_class: 2.1501 - val_hit_at_one: 0.8149\n",
      "Epoch 10/10\n",
      "3000/3000 [==============================] - 35s 12ms/step - loss: 194.9307 - precision_8: 0.8116 - recall_3: 0.5868 - average_n_class: 2.1745 - hit_at_one: 0.8168 - val_loss: 192.0874 - val_precision_8: 0.8157 - val_recall_3: 0.5914 - val_average_n_class: 2.1776 - val_hit_at_one: 0.8155\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x13f0b408588>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_3 = create_baseline_model(\"weighted_hamming\", loss_fn=weighted_hamming_loss)\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=\"models\\eval\\weighted_hamming\")\n",
    "model_3.fit(dataset, validation_data=valid_dataset, epochs=10,\n",
    "            steps_per_epoch=3000, validation_steps=100,\n",
    "            callbacks=[tensorboard_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Deployment and Servicing\n",
    "\n",
    "For model deployment we will use `gcloud ai-platform`.\n",
    "To use the service, we need to convert our `keras` model to TensorFlow custom estimator.\n",
    "\n",
    "### Video Feature Extraction\n",
    "\n",
    "Before we implement the model servicing, we need to also imeplement a video processor to extract features from a raw `.mp4` file.\n",
    "This is for us to do model inference on unseen examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Google Cloud AI Platform \n",
    "\n",
    "We can package our model as a python module in a way compatible with `gcloud ai-platform`.\n",
    "By using `gcloud ai-platform` as the interface we can easily launch the model training job on Google Cloud for large scale distributed training without any change of our code base.\n",
    "\n",
    "The following code cells will write necessary python scripts as module, wrapping up all the codes in previous sections together.\n",
    "Specifically, we need a `task.py` as the entrypoint for `gcloud ai-platform` to interface with our module.\n",
    "If we'd like to use the hyper-parameter tuning service in Google Cloud, we need to make sure the hyper-parameters are part of the `task.py` command line arguments.\n",
    "For this project for simplicity we are not going to do that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Create python module directory.\n",
    "mkdir -p src\n",
    "touch src/__init__.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/eval_metrics.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/eval_metrics.py\n",
    "\"\"\"Custom eval metrics for YouTube-8M model.\"\"\"\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.ops import init_ops\n",
    "\n",
    "\n",
    "class AverageNClass(tf.keras.metrics.Metric):\n",
    "\n",
    "    def __init__(self, name=\"average_n_class\", **kwargs):\n",
    "        super(tf.keras.metrics.Metric, self).__init__(name=name, **kwargs)\n",
    "        self.n_example = self.add_weight(\n",
    "            \"n_example\",\n",
    "            shape=(),\n",
    "            dtype=tf.float32,\n",
    "            initializer=init_ops.zeros_initializer)\n",
    "        self.n_predicted_class = self.add_weight(\n",
    "            \"n_predicted_class\",\n",
    "            shape=(),\n",
    "            dtype=tf.float32,\n",
    "            initializer=init_ops.zeros_initializer)\n",
    "\n",
    "    def update_state(self, y_true, y_pred, **kwargs):\n",
    "        # Accumulate sample size.\n",
    "        batch_size = tf.cast(len(y_true), tf.float32)\n",
    "        self.n_example.assign_add(batch_size)\n",
    "        # Accumulate number of predicted classes.\n",
    "        batch_n_class = tf.reduce_sum(tf.cast(y_pred > .5, tf.float32))\n",
    "        self.n_predicted_class.assign_add(batch_n_class)\n",
    "\n",
    "    def result(self):\n",
    "        return self.n_predicted_class / self.n_example\n",
    "\n",
    "\n",
    "class HitAtOne(tf.keras.metrics.Metric):\n",
    "\n",
    "    def __init__(self, name=\"hit_at_one\", **kwargs):\n",
    "        super(tf.keras.metrics.Metric, self).__init__(name=name, **kwargs)\n",
    "        self.n_example = self.add_weight(\n",
    "            \"n_example\",\n",
    "            shape=(),\n",
    "            dtype=tf.float32,\n",
    "            initializer=init_ops.zeros_initializer)\n",
    "        self.hit_at_one = self.add_weight(\n",
    "            \"hit_at_one\",\n",
    "            shape=(),\n",
    "            dtype=tf.float32,\n",
    "            initializer=init_ops.zeros_initializer)\n",
    "\n",
    "    def update_state(self, y_true, y_pred, **kwargs):\n",
    "        # Accumulate sample size.\n",
    "        batch_size = tf.cast(len(y_true), tf.float32)\n",
    "        self.n_example.assign_add(batch_size)\n",
    "        # Count number of hit@one.\n",
    "        tops = tf.math.argmax(y_pred, axis=1, output_type=tf.int32)\n",
    "        top_idx = tf.stack([tf.range(len(y_true)), tops], axis=1)\n",
    "        hits = tf.gather_nd(y_true, indices=top_idx)\n",
    "        self.hit_at_one.assign_add(tf.reduce_sum(hits))\n",
    "\n",
    "    def result(self):\n",
    "        return self.hit_at_one / self.n_example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/model.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/model.py\n",
    "\"\"\"Video classification model on YouTube-8M dataset.\"\"\"\n",
    "\n",
    "import logging\n",
    "from functools import partial\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import feature_column as fc\n",
    "from tensorflow.python.ops import init_ops\n",
    "from .eval_metrics import AverageNClass, HitAtOne\n",
    "\n",
    "\n",
    "N_CLASS = 3862\n",
    "BATCH_SIZE = 1024\n",
    "VOCAB_FILE = \"data/vocabulary.csv\"\n",
    "FEAT_COL_VIDEO = [\n",
    "    fc.numeric_column(key=\"mean_rgb\", shape=(1024,), dtype=tf.float32),\n",
    "    #fc.numeric_column(key=\"mean_audio\", shape=(128,), dtype=tf.float32),\n",
    "    fc.indicator_column(fc.categorical_column_with_identity(key=\"labels\", num_buckets=N_CLASS))\n",
    "]\n",
    "FEAT_X = [\"mean_rgb\"]\n",
    "FEAT_SPEC_VIDEO = fc.make_parse_example_spec(FEAT_COL_VIDEO)\n",
    "MULTI_HOT_ENCODER = tf.keras.layers.DenseFeatures(FEAT_COL_VIDEO[-1])\n",
    "\n",
    "\n",
    "def calc_class_weight(infile, scale=1):\n",
    "    \"\"\"Calculate class weight to re-balance label distribution.\n",
    "    The class weight for class i (w_i) is determined by:\n",
    "    w_i = total no. samples / (n_class * count(class i))\n",
    "    \"\"\"\n",
    "    vocab = pd.read_csv(infile).sort_values(\"Index\")\n",
    "    cnt = vocab[\"TrainVideoCount\"]\n",
    "    w = cnt.sum() / (len(vocab) * cnt)\n",
    "    w = w.values.astype(np.float32)\n",
    "    return pow(w, scale)\n",
    "\n",
    "\n",
    "def _parse(examples, spec, batch_size, n_class):\n",
    "    features = tf.io.parse_example(examples, features=spec)\n",
    "    labels = features.pop(\"labels\")\n",
    "    labels = MULTI_HOT_ENCODER({\"labels\": labels})\n",
    "    # Keras to estimator bug workaround.\n",
    "    features[\"input_1\"] = features.pop(\"mean_rgb\")\n",
    "    return features, labels\n",
    "\n",
    "\n",
    "def input_fn(infiles, spec, mode=tf.estimator.ModeKeys.TRAIN):\n",
    "    dataset = tf.data.TFRecordDataset(tf.io.gfile.glob(infiles))\n",
    "    if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "        dataset = dataset.shuffle(buffer_size=1000).repeat(count=None).batch(BATCH_SIZE, drop_remainder=True)\n",
    "    else:\n",
    "        dataset = dataset.batch(BATCH_SIZE, drop_remainder=False)\n",
    "    dataset = dataset.map(partial(_parse, spec=spec, batch_size=BATCH_SIZE, n_class=N_CLASS))\n",
    "    dataset = dataset.prefetch(BATCH_SIZE)\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def serving_input_receiver_fn():\n",
    "    \"\"\"Parse seralized tfrecord string for online inference.\"\"\"\n",
    "    # Accept a list of serialized tfrecord string.\n",
    "    example_bytestring = tf.compat.v1.placeholder(shape=[None], dtype=tf.string)\n",
    "    # Parse them into feature tensors.\n",
    "    features = tf.io.parse_example(example_bytestring, FEAT_SPEC_VIDEO)\n",
    "    features.pop(\"labels\")  # Dummy label. Not important at all.\n",
    "    # Keras to estimator bug workaround.\n",
    "    features[\"input_1\"] = features.pop(\"mean_rgb\")\n",
    "    return tf.estimator.export.ServingInputReceiver(features, {\"examples_bytes\": example_bytestring})\n",
    "\n",
    "\n",
    "class BaseModel:\n",
    "\n",
    "    def __init__(self, params):\n",
    "        self.params = params\n",
    "        config = tf.estimator.RunConfig(\n",
    "            tf_random_seed=777,\n",
    "            save_checkpoints_steps=max(1000, params[\"train_steps\"] // 10),\n",
    "            model_dir=params[\"model_dir\"]\n",
    "        )\n",
    "        self.class_weights = calc_class_weight(VOCAB_FILE, scale=1)\n",
    "        self.serving_input_receiver_fn = serving_input_receiver_fn\n",
    "        self.estimator = tf.keras.estimator.model_to_estimator(keras_model=self.model_fn(), config=config)\n",
    "\n",
    "    def model_fn(self):\n",
    "\n",
    "        def hamming_loss(y_true, y_pred):\n",
    "            loss = tf.keras.backend.binary_crossentropy(y_true, y_pred, from_logits=False)\n",
    "            if self.params[\"weighted_loss\"]:\n",
    "                loss *= self.class_weights\n",
    "            return tf.reduce_mean(tf.reduce_sum(loss, axis=1))\n",
    "\n",
    "        FEAT_COL_X = [col for col in FEAT_COL_VIDEO if col.name in FEAT_X]\n",
    "        l2_reg = tf.keras.regularizers.l2(1e-8)\n",
    "        fix = True  # Keras model to estimator bug workaround.\n",
    "        if fix:\n",
    "            inputs = tf.keras.layers.Input(shape=(1024,))\n",
    "            predictions = tf.keras.layers.Dense(N_CLASS, acivation=\"sigmoid\", kernel_regularizer=l2_reg)(inputs)\n",
    "            model = tf.keras.Model(inputs=inputs, outputs=predictions)\n",
    "        else :\n",
    "            model = tf.keras.models.Sequential(name=\"baseline\")\n",
    "            model.add(tf.keras.layers.DenseFeatures(FEAT_COL_X))\n",
    "            model.add(tf.keras.layers.Dense(N_CLASS, activation=\"sigmoid\", kernel_regularizer=l2_reg))\n",
    "        model.compile(\n",
    "            optimizer=\"adam\",\n",
    "            loss=hamming_loss,\n",
    "            metrics=[\n",
    "                tf.keras.metrics.Precision(),\n",
    "                tf.keras.metrics.Recall(),\n",
    "                AverageNClass(),\n",
    "                HitAtOne()\n",
    "            ]\n",
    "        )\n",
    "        return model\n",
    "\n",
    "    def train_and_evaluate(self, params):\n",
    "        train_spec = tf.estimator.TrainSpec(\n",
    "            input_fn=lambda: input_fn(params[\"train_data_path\"], spec=FEAT_SPEC_VIDEO),\n",
    "            max_steps=params[\"train_steps\"]\n",
    "        )\n",
    "        exporter = tf.estimator.FinalExporter(\n",
    "            name=\"exporter\", serving_input_receiver_fn=serving_input_receiver_fn)\n",
    "        eval_spec = tf.estimator.EvalSpec(\n",
    "            input_fn=lambda: input_fn(params[\"eval_data_path\"], spec=FEAT_SPEC_VIDEO,\n",
    "                                      mode=tf.estimator.ModeKeys.EVAL),\n",
    "            steps=100,\n",
    "            start_delay_secs=60,\n",
    "            throttle_secs=1,\n",
    "            exporters=exporter\n",
    "        )\n",
    "        logging.getLogger(\"tensorflow\").setLevel(logging.INFO)\n",
    "        tf.estimator.train_and_evaluate(\n",
    "            estimator=self.estimator,\n",
    "            train_spec=train_spec,\n",
    "            eval_spec=eval_spec\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/task.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/task.py\n",
    "\"\"\"Model interface for gcloud ai-platform.\"\"\"\n",
    "\n",
    "import argparse\n",
    "import json\n",
    "import os\n",
    "\n",
    "from . import model\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\n",
    "        \"--train_data_path\",\n",
    "        help=\"GCS or local path to training data\",\n",
    "        required = True\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--train_steps\",\n",
    "        help=\"Steps to run the training job for (default: 1000)\",\n",
    "        type=int,\n",
    "        default=1000\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--eval_data_path\",\n",
    "        help=\"GCS or local path to evaluation data\",\n",
    "        required= True\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--model_dir\",\n",
    "        help=\"GCS location to write checkpoints and export models\",\n",
    "        required=True\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--weighted_loss\",\n",
    "        help = \"Use class weights in loss?\",\n",
    "        required=False,\n",
    "        default=True\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--job-dir\",\n",
    "        help=\"This is not used by our model, but it is required by gcloud\",\n",
    "    )\n",
    "    args = parser.parse_args().__dict__\n",
    "\n",
    "    # Append trial_id to path so trials don\"t overwrite each other\n",
    "    args[\"model_dir\"] = os.path.join(\n",
    "        args[\"model_dir\"],\n",
    "        json.loads(\n",
    "            os.environ.get(\"TF_CONFIG\", \"{}\")\n",
    "        ).get(\"task\", {}).get(\"trial\", \"\")\n",
    "    )\n",
    "\n",
    "    # Run the training job\n",
    "    yt8m_model = model.BaseModel(args)\n",
    "    yt8m_model.train_and_evaluate(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we modualize our model, a local trainning job can be triggered by using `gcloud ai-platform` command line:\n",
    "For example:\n",
    "\n",
    "```bash\n",
    "gcloud ai-platform local train \\\n",
    "    --package-path=src \\\n",
    "    --module-name=src.task \\\n",
    "    -- \\\n",
    "    --train_data_path=data/video/train* \\\n",
    "    --eval_data_path=data/video/valid* \\\n",
    "    --train_steps=30000 \\\n",
    "    --model_dir=local_debug_model\n",
    "```\n",
    "\n",
    "One thing to note is that the training speed is much slower than directly using Keras model API.\n",
    "There seems to be a huge overhead after converting a Keras model to TF estimator.\n",
    "\n",
    "To train the model on cloud, we can upload the training data to GCS then run instead something like:\n",
    "\n",
    "```bash\n",
    "gcloud ai-platform jobs submit training yt8m_$(date -u +%y%m%d_%H%M%S) \\\n",
    "    --package-path=src \\\n",
    "    --module-name=src.task \\\n",
    "    --job-dir=gs://${BUCKET}/yt8m \\\n",
    "    --python-version=3.5 \\\n",
    "    --runtime-version=${TFVERSION} \\\n",
    "    --region=us-central1 \\\n",
    "    -- \\\n",
    "    --train_data_path=gs://${BUCKET}/yt8m/data/video/train* \\\n",
    "    --eval_data_path=gs://${BUCKET}/yt8m/data/video/valid*  \\\n",
    "    --train_steps=30000 \\\n",
    "    --output_dir=gs://${BUCKET}/yt8m/models\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AI Platform Prediction Service\n",
    "\n",
    "To deploy a pre-trained model we first need to upload the model directory to GCS.\n",
    "This can be done by for example:\n",
    "\n",
    "```bash\n",
    "MODEL_DIR=\"local_debug_model\"\n",
    "\n",
    "gsutil -m rm -rf gs://${BUCKET}/yt8m/models\n",
    "gsutil -m cp -r ${MODEL_DIR} gs://${BUCKET}/yt8m/models\n",
    "```\n",
    "\n",
    "Then to create a model for online prediction:\n",
    "\n",
    "```bash\n",
    "VERSION=\"v1\"\n",
    "TFVERSION=\"1.14\"\n",
    "MODEL_NAME=\"yt8m_video\"\n",
    "MODEL_DIR=local_debug_model\n",
    "\n",
    "# Create ai-platform cloud model.\n",
    "gcloud ai-platform models create ${MODEL_NAME} --regions us-central1\n",
    "\n",
    "# Remove previous version.\n",
    "gcloud ai-platform versions delete ${VERSION} --model ${MODEL_NAME} --quiet\n",
    "\n",
    "# Deploy.\n",
    "gcloud ai-platform versions create ${VERSION} --model ${MODEL_NAME} \\\n",
    "    --origin $(gsutil ls gs://${BUCKET}/yt8m/models/export/exporter | tail -1) \\\n",
    "    --python-version=3.5 \\\n",
    "    --runtime-version ${TFVERSION}\n",
    "```\n",
    "\n",
    "Note that as of now (July 2019) AI Platform runtime doesn't support TF 2.0 yet.\n",
    "But our code is backward-compatible with 1.14."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Online Prediction with a Wep App\n",
    "\n",
    "We develop a web service using [Flask](https://palletsprojects.com/p/flask/) to receive user request about a YouTube video share link for online label(s) prediction.\n",
    "Also we will use [socket.io](https://socket.io/) to implement asynchronous processing to improve user experience. (It takes time to process a raw video without using a dedicated server.)\n",
    "Also we will use [youtube-dl](https://github.com/ytdl-org/youtube-dl) as the YouTube video downloader to download the requested video in `.mp4` format.\n",
    "For video feature extraction we directly utilize the open source [starter code](https://github.com/google/youtube-8m/tree/master/feature_extractor) from Google Research.\n",
    "\n",
    "The overall architecture of our application will look like this:\n",
    "\n",
    "<img src=\"assets/app_arch_demo.svg\">\n",
    "\n",
    "To simplify things (in a one-week window) the web server is also doing video feature extraction which is a heavy task since it involves in loading a pre-trained Inception model and making inference through the graph.\n",
    "\n",
    "The ideal architecture should be something like:\n",
    "\n",
    "<img src=\"assets/app_arch_ideal.svg\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Create directories for our flask app.\n",
    "mkdir -p app\n",
    "mkdir -p app/templates\n",
    "mkdir -p app/static\n",
    "mkdir -p app/assets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Server-Side Coding\n",
    "\n",
    "The web server needs to do a series of task given a user request:\n",
    "\n",
    "1. Download the requested video\n",
    "2. Process the video to extract features in tfrecord\n",
    "3. Parse the tfrecord back and package it in base64, send it to the online predictor\n",
    "4. Receive prediction result and tidy it up, then send it to the front-end\n",
    "\n",
    "Ideally we should by-pass the writing of tfrecord and direectly package the serialized string in-memory.\n",
    "But since we utilize the starter code in feature extractor without changing anything, this is the extra effort needed to be done.\n",
    "\n",
    "Here is the main script for the server:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing app/main.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile app/main.py\n",
    "\"\"\"Run Flask app for YouTube-8M model demo.\"\"\"\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import shutil\n",
    "import tempfile\n",
    "import subprocess\n",
    "import base64\n",
    "\n",
    "from flask import Flask, render_template, request\n",
    "from flask_socketio import SocketIO, emit\n",
    "import googleapiclient.discovery\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "app = Flask(__name__)\n",
    "socketio = SocketIO(app)\n",
    "\n",
    "PROJECT = os.environ[\"PROJECT\"]  # Name of gcloud project.\n",
    "MODEL = \"yt8m_video\"  # Name of the deployed ai-platform model.\n",
    "LABEL_VOCAB_FILE = \"../data/vocabulary.csv\"\n",
    "VIDEO_DIR = \"test_videos\"\n",
    "TFREC_DIR = \"test_tfrecords\"\n",
    "YT_DL = \"bin/youtube-dl\"\n",
    "FT_EXTRACTOR = \"feature_extractor/extract_tfrecords_main.py\"\n",
    "\n",
    "\n",
    "def read_label_vocab(infile=LABEL_VOCAB_FILE):\n",
    "    with open(infile, \"rt\") as f:\n",
    "        raw_vocab = [l.strip(\"\\n\") for l in f.readlines()]\n",
    "    header = raw_vocab[0].split(\",\")\n",
    "    index_pos = header.index(\"Index\")\n",
    "    label_pos = header.index(\"Name\")\n",
    "    vocab = {}\n",
    "    for line in raw_vocab[1:]:\n",
    "        line = line.split(\",\")\n",
    "        vocab[int(line[index_pos])] = line[label_pos]\n",
    "    return vocab\n",
    "\n",
    "\n",
    "def predict_json(instances, project=PROJECT, model=MODEL, version=None):\n",
    "    # To authenticate set the environment variable\n",
    "    # GOOGLE_APPLICATION_CREDENTIALS=<path_to_service_account_file>\n",
    "    service = googleapiclient.discovery.build(\"ml\", \"v1\")\n",
    "    name = \"projects/{}/models/{}\".format(project, model)\n",
    "    if version is not None:\n",
    "        name += \"/versions/{}\".format(version)\n",
    "    response = service.projects().predict(\n",
    "        name=name,\n",
    "        body={\"instances\": instances}\n",
    "    ).execute()\n",
    "    if \"error\" in response:\n",
    "        raise RuntimeError(response[\"error\"])\n",
    "    return response[\"predictions\"][0]\n",
    "\n",
    "\n",
    "def parse_tfrecord(tfrec_file):\n",
    "    \"\"\"Encode tfrecord serialized string in base64.\"\"\"\n",
    "    rec_iter = tf.io.tf_record_iterator(tfrec_file)\n",
    "    body = {\"b64\": base64.b64encode(next(rec_iter)).decode(\"utf-8\")}\n",
    "    return body\n",
    "\n",
    "\n",
    "def video_to_tfrecord(video_file):\n",
    "    video_tag = os.path.basename(video_file).split(\".\")[0]\n",
    "    tmpcsv, tmpcsv_name = tempfile.mkstemp()\n",
    "    tmprec, tmprec_name = tempfile.mkstemp()\n",
    "    with open(tmpcsv_name, \"wt\") as f:\n",
    "        f.write(\"{},0\\n\".format(video_file))\n",
    "    p = subprocess.Popen([\n",
    "        \"python\", FT_EXTRACTOR,\n",
    "        \"--input_videos_csv\", tmpcsv_name,\n",
    "        \"--output_tfrecords_file\", tmprec_name,\n",
    "        \"--skip_frame_level_features\", \"false\"\n",
    "        ], stdout=sys.stdout)    \n",
    "    out, err = p.communicate()\n",
    "    return tmprec_name\n",
    "\n",
    "\n",
    "def download_yt(video_link, outdir=VIDEO_DIR):\n",
    "    \"\"\"Use youtube-dl to download a youtube video.\n",
    "    https://github.com/ytdl-org/youtube-dl\n",
    "    \"\"\"\n",
    "    video_tag = os.path.basename(video_link)\n",
    "    outfile = os.path.join(outdir, \"{}.mp4\".format(video_tag))\n",
    "    p = subprocess.Popen([\n",
    "        YT_DL, video_link,\n",
    "        \"-o\", outfile,\n",
    "        \"-k\",\n",
    "        \"-f\", \"mp4\"\n",
    "        ], stdout=sys.stdout)    \n",
    "    out, err = p.communicate()\n",
    "    return outfile\n",
    "\n",
    "\n",
    "def inspect_tfrec(tfrec_file, is_sequence=False):\n",
    "    \"\"\"Print a tfrecord file content.\"\"\"\n",
    "    record_iter = tf.io.tf_record_iterator(tfrec_file)\n",
    "    if is_sequence:\n",
    "        example = tf.train.SequenceExample()\n",
    "        example.ParseFromString(next(record_iter))\n",
    "    else:\n",
    "        example = tf.train.Example()\n",
    "        example.ParseFromString(next(record_iter))\n",
    "    return example\n",
    "\n",
    "\n",
    "vocab = read_label_vocab()\n",
    "\n",
    "\n",
    "@socketio.on(\"predict_request\", namespace=\"\")\n",
    "def start_predict_pipeline(message):\n",
    "    # Form iframe to autoplay the requested youtube video.\n",
    "    video_link = message[\"link\"]\n",
    "    video_tag = os.path.basename(video_link)\n",
    "    emit(\"video_response\", {\"tag\": video_tag})\n",
    "\n",
    "    # Do prediction.\n",
    "    # Check if the video is already processed before.\n",
    "    tfrec_file = os.path.join(TFREC_DIR, \"{}.tfrecord\".format(video_tag))\n",
    "    if not os.path.exists(tfrec_file):\n",
    "        # Download the youtube video as mp4.\n",
    "        emit(\"status_update\", {\"status\": \"Start Downloading video...\"})\n",
    "        video_file = download_yt(video_link)\n",
    "        if os.path.exists(video_file):\n",
    "            emit(\"status_update\", {\"status\": \"Download completed.\"})\n",
    "        else:\n",
    "            emit(\"status_update\", {\"status\": \"Invalid link!\"})\n",
    "            return\n",
    "        # Convert mp4 to tfrecord.\n",
    "        emit(\"status_update\", {\"status\": \"Extracting video embeddings...\"})\n",
    "        tmp_tfrec_file = video_to_tfrecord(video_file)\n",
    "        shutil.move(tmp_tfrec_file, tfrec_file)\n",
    "        emit(\"status_update\", {\"status\": \"Feature extraction completed.\"})\n",
    "    # Request online prediction service.\n",
    "    emit(\"status_update\", {\"status\": \"Request online predictions...\"})\n",
    "    request_data = parse_tfrecord(tfrec_file)\n",
    "    responses = predict_json(request_data)\n",
    "    emit(\"status_update\", {\"status\": \"All done!\"})\n",
    "    # Tidy predictions.\n",
    "    predictions = {}\n",
    "    proba = np.array(responses[\"activation\"])\n",
    "    top_k_pos = proba.argsort()[-10:][::-1]\n",
    "    predictions[\"top_k\"] = [\"{}: {:.2%}\".format(vocab[c], p) for c, p in \n",
    "        zip(top_k_pos, proba[top_k_pos])]\n",
    "    predictions[\"n_class\"] = str((proba > .5).sum())\n",
    "    emit(\"predict_response\", predictions)\n",
    "\n",
    "\n",
    "@app.route(\"/\", methods=[\"GET\", \"POST\"])\n",
    "def root():\n",
    "    return render_template(\"index.html\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # This is used when running locally only. When deploying to Google App\n",
    "    # Engine, a webserver process such as Gunicorn will serve the app. This\n",
    "    # can be configured by adding an `entrypoint` to app.yaml.\n",
    "    socketio.run(app,host=\"0.0.0.0\", port=8080, debug=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Client-Side Coding\n",
    "\n",
    "For the front-end we need to write some simple `jquery` to handle the asynchronous calls made by `socket.io`.\n",
    "We will update the page right-hand sidebar on each background task to inform the user what's going on.\n",
    "For a new video the processing time can easily exceed 1 minute which is definitely too long.\n",
    "If there is no response at the page a user may feel frustrated and just go away.\n",
    "\n",
    "The file `app/templates/index.html` is created as the following:\n",
    "\n",
    "```html\n",
    "<!doctype html>\n",
    "<head>\n",
    "    <meta charset=\"utf-8\">\n",
    "    <meta http-equiv=\"X-UA-Compatible\" content=\"IE=edge\">\n",
    "    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1\">\n",
    "    \n",
    "    <title>YouTube-8M-Demo</title>\n",
    "\n",
    "    <link rel=\"shortcut icon\" href=\"{{ url_for('static', filename='favicon.ico') }}\">\n",
    "    <link rel=\"stylesheet\" href=\"{{ url_for('static', filename='style.css') }}\">\n",
    "\n",
    "    <script src=\"{{ url_for('static', filename='jquery-3.4.1.min.js') }}\"></script>\n",
    "    <script src=\"{{ url_for('static', filename='socket.io-2.2.0.min.js') }}\"></script>\n",
    "    <script type=\"text/javascript\" charset=\"utf-8\">\n",
    "        // Handle async web socket communication.\n",
    "        $(document).ready(function() {\n",
    "            namespace = \"\";\n",
    "            var socket = io(namespace);\n",
    "            // Send async request to server.\n",
    "            $(\"form#yt_link\").submit(function(event) {\n",
    "                //socket.emit(\"predict_request\", {data: $(\"#video_link\").val()});\n",
    "                socket.emit(\"predict_request\",  {link: $(\"#video_link\").val()});\n",
    "                return false;\n",
    "            });\n",
    "            // Receive async response from server.\n",
    "            socket.on(\"video_response\", function(msg, cb) {\n",
    "                var iframe = $(\"<iframe>\", {\n",
    "                   src: \"https://www.youtube.com/embed/\" + msg.tag + \"?autoplay=1\",\n",
    "                   frameborder: 0,\n",
    "                   style: \"position:absolute;top:0;left:0;width:100%;height:100%;\"\n",
    "                });\n",
    "                $(\"#video-frame\").html(iframe);\n",
    "                // Clear previous predictions, if any.\n",
    "                $(\"#predictions\").html(\"\");\n",
    "                if (cb)\n",
    "                    cb();\n",
    "            });\n",
    "            socket.on(\"predict_response\", function(msg, cb) {\n",
    "                var output = $(\"<ol>\");\n",
    "                output.append(\"<h2>Predictions</h2>\")\n",
    "                var cnt = 0;\n",
    "                $(msg.top_k).each(function(index, item) {\n",
    "                    cnt += 1;\n",
    "                    output.append(\n",
    "                        $(document.createElement(\"li\")).text(item)\n",
    "                    );\n",
    "                    if ( cnt == msg.n_class ) {\n",
    "                        output.append(\"---<br>\")\n",
    "                    }\n",
    "                });\n",
    "                output.append(\"---<br>\")\n",
    "                output.append(\"Total No. of Predicted Classes: \")\n",
    "                output.append(\"<h2>\" + msg.n_class + \"</h2>\")\n",
    "                $(\"#predictions\").html(output);\n",
    "                if (cb)\n",
    "                    cb();\n",
    "            });\n",
    "            socket.on(\"status_update\", function(msg, cb) {\n",
    "                $(\"#predictions\").append(\"<p>\" + msg.status + \"</p>\");\n",
    "                if (cb)\n",
    "                    cb();\n",
    "            });\n",
    "        });\n",
    "    </script>\n",
    "</head>\n",
    "\n",
    "<body>\n",
    "    <h1>Hello YouTube-8M!</h1>\n",
    "\n",
    "    <form id=\"yt_link\" method=\"post\" action=\"#\">\n",
    "        <input placeholder=\"Put a youtube video link here...\" \n",
    "               type=\"text\" name=\"video_link\" id=\"video_link\"\n",
    "               style=\"width:45%;margin:auto;\">\n",
    "    </form>\n",
    "\n",
    "    <section class=\"container\">\n",
    "        <div class=\"left-half\">\n",
    "            <div style=\"position:relative;padding-top:56.25%;\">\n",
    "                <div id=\"video-frame\"></div>\n",
    "            </div>\n",
    "        </div>\n",
    "        <div class=\"right-half\">\n",
    "            <div id=\"predictions\"></div>\n",
    "        </div>\n",
    "    </section>\n",
    "</body>\n",
    "```\n",
    "\n",
    "Finally, this is how the app looks like after a prediction:\n",
    "\n",
    "<img src=\"assets/demo.png\">\n",
    "\n",
    "It can handle virtually *any* public YouTube video.\n",
    "Its a pity that the label *Samurai Shodown* didn't come at the top, since the game in play is indeed *Samurai Shodown*. But it is a brand-new released series title just a month ago as this writeup, so definitely we don't have any video about it in our training dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Thoughts\n",
    "\n",
    "TensorFlow is evolving very fast.\n",
    "There are a variety of ways in both training and deploying a machine learning model using TF.\n",
    "Currently there is really lacking a sort of developing best-practice in this field, due to the constant changing in its API design.\n",
    "\n",
    "Some definitely directions to move forward, particularly for Keras integration:\n",
    "\n",
    "+ Better inter-operation between Keras model and TF estimator\n",
    "    + Right now the process is really buggy\n",
    "+ Better cloud prediction service deployment with Keras model"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "386px",
    "width": "421px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "340.355px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
