---
title: "High Performance Computing in R using `Rcpp`"
subtitle: ""
author:
- name: Kyle Chung
  affiliation:
date: "`r format(Sys.time(), '%d %B %Y')` Last Updated"
output:
  html_notebook:
    highlight: tango
    number_sections: yes
    theme: paper
    toc: yes
    toc_depth: 4
    toc_float: yes
    includes:
      in_header: /tmp/meta_header.html
  code_download: true
bibliography: rcpp.bib
nocite: |
  @R.utils
  @data.table
  @ggplot2
  @wordcloud2
  @rcpp1
  @rcpp2
  @rcpp3
  @quanteda
  @text2vec
  @ngram
  @microbenchmark
abstract: |
  TBC.
---

```{r meta, include=FALSE}
dir.create("/tmp", showWarnings=FALSE)
meta_header_file <- file("/tmp/meta_header.html")

# Add open graph meta.
meta <- c(
    '<meta name="author" content="Kyle Chung">',
    '<meta property="og:title" content="High Performance Computing in R using Rcpp">',
    '<meta property="og:url" content="https://everdark.github.io/k9/programming/rcpp/rcpp.nb.html">',
    '<meta property="og:image" content="https://everdark.github.io/k9/assets/avatar.jpg">',
    '<meta property="og:description" content="A Showcase of high performance computing in R using Rcpp.">'
)
writeLines(meta, meta_header_file)

# Add Github corner.
github_corner_svg <- "../../../assets/github_corner.html"
github_corner_conf <- list(github_link="https://github.com/everdark/k9/tree/master/programming/r/rcpp")
writeLines(stringr::str_interp(readLines(github_corner_svg), github_corner_conf), meta_header_file)

close(meta_header_file)
```

The notebook is a re-writing of its [original version](http://everdark.github.io/rcpp_lightning_dsc2015/) for a lightning talk at Data Science Conference 2015 at Taipei.

```{r import, results="hide"}
# Import all required packages.

library(data.table)
library(ggplot2)
library(wordcloud2)
library(Rcpp)
library(R.utils)
```

# Decompose the R Source Code

```{r download_r_source, results="hide"}
# Download the source code of R lang.
# This may take a while.

TEMPDIR <- tempdir()
target_file <- "https://cran.r-project.org/src/base/R-3/R-3.6.1.tar.gz"
downloaded_file <- file.path(TEMPDIR, basename(target_file))
download.file(target_file, downloaded_file)
untar(downloaded_file, exdir=TEMPDIR)  # This may generate warnings under Windows.
```

```{r read_r_source}
# List all files in the source and count the extensions.

src_dir <- file.path(tools::file_path_sans_ext(downloaded_file, compression=TRUE), "src")
src_files <- list.files(src_dir, recursive=TRUE, full.names=TRUE)
src_exts <- tools::file_ext(src_files)
src_exts <- src_exts[src_exts != ""]

ext_counts <- as.data.table(sort(table(src_exts), decreasing=TRUE), keep.rownames=TRUE)
setnames(ext_counts, c("ext", "count"))
ext_counts[, ext:=factor(ext, levels=rev(ext))]
```

```{r ext_wordcloud}
# Scale down to avoid over-sizing.
wordcloud2(sqrt(table(src_exts)))
```

```{r}
top_n <- 10
ggplot(ext_counts[1:top_n], aes(x=ext, y=count)) +
  geom_bar(stat="identity") +
  coord_flip()
```

+ `.Rd`: Document file
+ `.R`: R source file
+ `.c`: C source file
+ `.mo`: Binary data file
+ `.po`: `gettext` file, about programming language translation
+ `.h`: Header file for C
+ `.afm`: Font file
+ `.in`: Template config file for some macro preprocessor
+ `.win`: Same above, but specifically for Windows
+ `.f`: Fortran source

```{r}
lang_ext <- c("c", "h", "R", "f")
lang_ext_counts <- ext_counts[ext %in% lang_ext]
lang_ext_counts[, pct:=count / sum(count)]

ggplot(lang_ext_counts, aes(x=ext, y=count)) +
  geom_bar(stat="identity") +
  geom_text(aes(label=scales::percent(pct), y=count),
            vjust=1.25, size=5, color="white")
```

The fact is that,
*R is heavily written in C.*
`Primitive` functions are writtin in C.
Most of them are vectorized so it is very fast to apply the function directly to a vector.
For example the power function is a `primitive` function

```{r}
`^`  # A primitive.

(1:10)^2  # A vectorized primitive function call.
```

```{r}
lang_src_files <- src_files[tools::file_ext(src_files) %in% lang_ext]
lang_src_file_lens <- sapply(lang_src_files, R.utils::countLines)

lang_len <- data.table(lang=gsub("^.*\\.", "", names(lang_src_file_lens)), len=lang_src_file_lens)
lang_len_counts <- lang_len[, .(tot_lines=sum(len)), by="lang"]
lang_len_counts[, pct:=tot_lines / sum(tot_lines)]

ggplot(lang_len_counts, aes(x=lang, y=tot_lines)) +
  geom_bar(stat="identity") +
  geom_text(aes(label=scales::percent(pct), y=tot_lines),
            vjust=1.25, size=5, color="white")
```

# Tips for Performant R Programming

[speed up discussion]
vectorization
avoid memory copy (matrix pre-allocation >> matrix combine)


# Working Examples for `Rcpp`

## N-Gram Generation

N-gram generation is a common task for natural language processing and understanding.
Despite its simplicity,
surprisingly,
it is not easy for native R code to fulfill the task in an efficient manner.

Let's assume we'd like to count the bigrams of the [hex dump](https://en.wikipedia.org/wiki/Hex_dump) of a (potentially binary) file.
The byte code information could be useful for a downstream machine learning task such as file category classification.

```{r ngram_read_text}
# Prepare hex dump of a file.
infile <- "README.md"
hex <- readBin(file(infile, "rb"), what="raw", n=file.info(infile)$size)
hex_c <- as.character(hex)

# cat(rawToChar(hex)) # One can verify the data if it is a text file.
```

### Native R {-}

Now we implement the minimum ngram counter given a character vector,
assuming each element is a token:

```{r ngram_r}
ngram_r <- function(x, n) {
    len <- length(x) - (n - 1)
    out <- character(len)
    for ( i in 1:len ) {
        out[i] <- x[i]
        if ( n > 1 )
            for ( j in 1:(n-1) )
                out[i] <- paste0(out[i], x[i+j])
    }
    sort(table(out), decreasing=TRUE)
}

out_r <- ngram_r(hex_c, 2)

print(length(out_r))  # Total ngrams.

print(out_r[1]) # The most frequent ngram.
```

The above function,
though implenmentation is straightforward,
is terribly slow because it didn't utilize vectorization.
This is the usually frustrated experience when people familiar with other general purpose languages first come to use R.

### Rcpp {-}

For a function requiring arbitraily large explicit loop and there is no obvious way to vectorization,
we have `Rcpp` come to the rescue.^[And also surprisingly, we don't really have many such cases in R programming where we really need to resort to its C interface. The languange is doing a very good job in vectorizing the most utilized functions for the majority of programming operations, especially in the data analytical field.]

For exactly the same minimum implementation we can re-write it in C++ using the `Rcpp` API:

```{r ngram_rcpp}
cppFunction("
  CharacterVector ngramRcpp (CharacterVector hexvector, int ngram) {
    int len = hexvector.size() - (ngram - 1);
    CharacterVector out(len);
    for (int i = 0; i < len; i++) {
      out(i) = hexvector[i];
      for (int j = 1; j < ngram; j++) {
        out(i) += hexvector[i+j];
      }
    }
    return out;
}")

ngram_rcpp <- function(x, n) {
  ngrams <- ngramRcpp(x, 2)
  sort(table(ngrams), decreasing=TRUE)
}

out_rcpp <- ngram_rcpp(hex_c, 2)

all.equal(out_rcpp, out_r, check.attributes=FALSE)
```

Notice that we only implement the ngram extraction loop but leave the frequency count for native R.
This is because the `table` function is not a bottleneck of our task.

In this example we use the `CharacterVector` class implemented in `Rcpp` which mimics the `character` vector we usually use in native R.
There are many such high-level classes we can find in the `Rcpp` library.
These classes will save our development time when writing the C++ code since we won't limit ourselves to only C++ standard library.

```{r ngram_benchmark_1}
library(microbenchmark)

microbenchmark(
  ngram_r(hex_c, 2),
  ngram_rcpp(hex_c, 2),
  times=10
)
```

### Other Packages {-}

For completeness,
in this section we include the benchmark for some other well-developed packages for text processing.
These packages are usually using R's native C interface or `Rcpp` to implement the critical parts of the computation for their intended tasks.
We will expect them to be also much more efficient than a naive implementation in native R code.

The first package we explore is [`quanteda`](https://github.com/quanteda/quanteda):

```{r ngram_quanteda}
library(quanteda)

# For quanteda we need to convert text to its `tokens` API.
# This may introduce some overhead so we will create two functions for fair comparison latter.

hex_s <- paste(hex_c, collapse=" ")
hex_t <- tokens(hex_s)

ngram_quanteda_1 <- function(x, n) {
  # Tokenization included.
  ngrams <- tokens_ngrams(tokens(x), n=n)[[1]]
  sort(table(ngrams), decreasing=TRUE)
}

ngram_quanteda_2 <- function(tok, n) {
  # Assume pre-tokenized.
  ngrams <- tokens_ngrams(tok, n=n)[[1]]
  sort(table(ngrams), decreasing=TRUE)
}

out_quanteda <- ngram_quanteda_1(hex_s, n=2)

all.equal(out_quanteda, out_r, check.attributes=FALSE)
```

[`text2vec`](https://github.com/dselivanov/text2vec) is also a high-performance package implemented with `Rcpp`:

```{r ngram_text2vec}
library(text2vec)

# Again to use text2vec we need to follow its API:
# Converting text into `itoken` iterator.
# Similar to the case of quanteda, we use two functions where one excludes the conversion overhead.
hex_i <- itoken(hex_s, progressbar=FALSE)

ngram_text2vec_1 <- function(x, n=n) {
  it <- itoken(x, progressbar=FALSE)
  create_vocabulary(it, ngram=c(n, n))
}

ngram_text2vec_2 <- function(it, n=n) {
  create_vocabulary(it, ngram=c(n, n))
}

out_text2vec <- ngram_text2vec_1(hex_s, n=2)

# Tidy the result to align with our previous functions.
out_text2vec <- setNames(out_text2vec$term_count, out_text2vec$term)
all.equal(as.table(sort(out_text2vec, decreasing=TRUE)), out_r, check.attributes=FALSE)
```

Lastly,
there is a package called [`ngram`](https://github.com/wrathematics/ngram) dedicating only at ngram generation task,
written mainly in C:

```{r ngram_ngram}
library(ngram)

ngram_ngram <- function(x, n) {
  ngram(x, n=n)
}

out_ngram <- ngram(hex_s)

# Tidy the result.
out_ngram <- get.phrasetable(out_ngram)
out_ngram <- setNames(out_ngram$freq, out_ngram$ngrams)
all.equal(as.table(out_ngram), out_r, check.attributes=FALSE)
```

Now we benchmark all the above implementations:

```{r ngram_benchmark_all}
microbenchmark(
  ngram_r(hex_c, 2),
  ngram_rcpp(hex_c, 2),
  ngram_quanteda_1(hex_s, 2),
  ngram_quanteda_2(hex_t, 2),
  ngram_text2vec_1(hex_s, 2),
  ngram_text2vec_2(hex_i, 2),
  ngram_ngram(hex_s, 2),
  times=10
)
```

# References

```{r session_info}
sessionInfo()
```