---
title: "High Performance Computing in R using `Rcpp`"
subtitle: ""
author:
- name: Kyle Chung
  affiliation:
date: "`r format(Sys.time(), '%d %B %Y')` Last Updated"
output:
  html_notebook:
    highlight: tango
    number_sections: yes
    theme: paper
    toc: yes
    toc_depth: 4
    toc_float: yes
    includes:
      in_header: /tmp/meta_header.html
  code_download: true
bibliography: rcpp.bib
nocite: |
  @R.utils
  @data.table
  @ggplot2
  @wordcloud2
  @rcpp1
  @rcpp2
  @rcpp3
  @quanteda
  @text2vec
  @ngram
  @microbenchmark
abstract: |
  TBC.
---

```{r meta, include=FALSE}
dir.create("/tmp", showWarnings=FALSE)
meta_header_file <- file("/tmp/meta_header.html")

# Add open graph meta.
meta <- c(
    '<meta name="author" content="Kyle Chung">',
    '<meta property="og:title" content="High Performance Computing in R using Rcpp">',
    '<meta property="og:url" content="https://everdark.github.io/k9/programming/rcpp/rcpp.nb.html">',
    '<meta property="og:image" content="https://everdark.github.io/k9/assets/avatar.jpg">',
    '<meta property="og:description" content="A Showcase of high performance computing in R using Rcpp.">'
)
writeLines(meta, meta_header_file)

# Add Github corner.
github_corner_svg <- "../../../assets/github_corner.html"
github_corner_conf <- list(github_link="https://github.com/everdark/k9/tree/master/programming/r/rcpp")
writeLines(stringr::str_interp(readLines(github_corner_svg), github_corner_conf), meta_header_file)

close(meta_header_file)
```

The notebook is a re-writing of its [original version](http://everdark.github.io/rcpp_lightning_dsc2015/) for a lightning talk at Data Science Conference 2015 at Taipei.

---

# Motivation

The performance of a R program can vary a lot,
depending on whether it is written by an experienced R programmer or a newbie who don't know how to properly write R.

Unfortunately,
unlike other general purpose langauges,
it is much more likely to write improper R codes which run embarrassingly slow.
Two important and common factors that affect the performance of R code:

1. Vectorization
2. Memory Copy

In general,
we should use vectorization as much as possible and *avoid* memory copy as much as possible.
For example the following code is TERRIBLE:

```{r terrible}
numbers <- 1:1e5

pow_all_1 <- function(numbers) {
  out <- c()
  for ( n in numbers ) {
    out <- c(out, n^2)  # Memory copy is happening here.
  }
  out
}

system.time(out_1 <- pow_all_1(numbers))
```

We are just trying to calculate the base-2 power of a sequence of numbers.
It will be very straghtforward to just loop over the numbers and compute the result for a general purpose language.

But NOT for R.

The above code is just WRONG because it violates both our two principles:
it didn't vectorize when it could, and it didn't avoid memory copy.

Let's first fix the second issue about the memory copy:

```{r still_terrible}
pow_all_2 <- function(numbers) {
  out <- numeric(length(numbers))  # Pre-allocate memory.
  for ( i in 1:length(numbers) ) {
    out[i] <- numbers[i]^2
  }
  out
}

system.time(out_2 <- pow_all_2(numbers))
stopifnot(all.equal(out_1, out_2))
```

The output vector size is pre-determined for our second attempt,
which avoid copies happening again and again during the unnecessary concatenation in the loop.

Still the code is terrible,
because it didn't *vectorize* when it actually could:

```{r good}
pow_all_3 <- function(numbers) {
  numbers^2
}

system.time(out_3 <- pow_all_3(numbers))
stopifnot(all.equal(out_1, out_3))
```

The difference may not be obvious at the first glance because the task is too trivial.
Now for a serious benchmarking at microsecond-level:

```{r benchmark_vectorization, paged.print=FALSE}
library(microbenchmark)

numbers <- 1:1e4  # Make it smaller for faster benchmarking.

ben <- microbenchmark(
  pow_all_1(numbers),
  pow_all_2(numbers),
  pow_all_3(numbers),
  times=10
)

print(ben)
```

A properly written R program can be of several orders of magnitude faster.

Even if we already carefully write our R code,
it may still occur that we want even faster speed.
This is especially true when there is no trivial way of vectorization for the implementation desired.
This is the main topic of this notebook:
we can still be faster,
by using R's C interface.

`Rcpp` is the most dominant way of achieving that.
But before our deep dive,
let's understand *what is R* a bit more.

# Decompose the R Source Code

Let's examine the source code of the R language.

```{r download_r_source, results="hide", warning=FALSE}
# Download the source code of R lang.
# This may take a while.

TEMPDIR <- tempdir()
target_file <- "https://cran.r-project.org/src/base/R-3/R-3.6.1.tar.gz"
downloaded_file <- file.path(TEMPDIR, basename(target_file))
download.file(target_file, downloaded_file)
untar(downloaded_file, exdir=TEMPDIR)  # This may generate warnings under Windows.
```

We'd like to know the file type distribution in the source code.

```{r read_r_source}
# List all files in the source and count the extensions.

library(data.table)

src_dir <- file.path(tools::file_path_sans_ext(downloaded_file, compression=TRUE), "src")
src_files <- list.files(src_dir, recursive=TRUE, full.names=TRUE)
src_exts <- tools::file_ext(src_files)
src_exts <- src_exts[src_exts != ""]

ext_counts <- as.data.table(sort(table(src_exts), decreasing=TRUE), keep.rownames=TRUE)
setnames(ext_counts, c("ext", "count"))
ext_counts[, ext:=factor(ext, levels=rev(ext))]
```

```{r ext_wordcloud}
library(wordcloud2)

# Scale down to avoid over-sizing.
wordcloud2(sqrt(table(src_exts)))
```

```{r top_n_ext}
top_n <- 10
ggplot(ext_counts[1:top_n], aes(x=ext, y=count)) +
  geom_bar(stat="identity") +
  labs(title=sprintf("Top %s File Extensions in R Source Code", top_n)) +
  coord_flip()
```

Here are the most common file types found in the source code of R.

+ `.Rd`: Document file
+ `.R`: R source file
+ `.c`: C source file
+ `.mo`: Binary data file
+ `.po`: Language translation file
+ `.h`: Header file for C
+ `.afm`: Font file
+ `.in`: Template configuration file for some macro preprocessor
+ `.win`: Same above but specifically for Windows
+ `.f`: Fortran source

If we further limit to only programming language related files:

```{r lang_ext_count}
lang_ext <- c("c", "h", "f", "R")
lang_ext_counts <- ext_counts[ext %in% lang_ext]
lang_ext_counts[, pct:=count / sum(count)]
lang_ext_counts[, ext:=factor(ext, levels=lang_ext)]

ggplot(lang_ext_counts, aes(x=ext, y=count)) +
  geom_bar(stat="identity") +
  labs(y="Number of Files", x="File Extension",
       title="Language File Distribution in R Source Code") +
  geom_text(aes(label=scales::percent(pct), y=count),
            vjust=1, size=5, color="white")
```

The fact is that,
*R is heavily written in C.*
`Primitive` functions are writtin in C.
Most of them are vectorized so it is very fast to apply the function directly to a vector.
For example the power function we just examined previously is a `primitive` function:

```{r primitive}
`^`  # A primitive.

(1:10)^2  # A vectorized primitive function call.
```

Instead of counting files,
we can further count the number of lines in those language files to arrive at the distribution at file line level:

```{r lang_line_count}
lang_src_files <- src_files[tools::file_ext(src_files) %in% lang_ext]
lang_src_file_lens <- sapply(lang_src_files, R.utils::countLines)

lang_len <- data.table(lang=gsub("^.*\\.", "", names(lang_src_file_lens)), len=lang_src_file_lens)
lang_len_counts <- lang_len[, .(tot_lines=sum(len)), by="lang"]
lang_len_counts[, pct:=tot_lines / sum(tot_lines)]
lang_len_counts[, ext:=factor(lang, levels=lang_ext)]

ggplot(lang_len_counts, aes(x=lang, y=tot_lines)) +
  geom_bar(stat="identity") +
  labs(y="Number of Lines", x="File Extension",
       title="Language File Line Distribution in R Source Code") +
  geom_text(aes(label=scales::percent(pct), y=tot_lines),
            vjust=1.25, size=5, color="white")
```

The take-away is that,
R simply *cannot* be slow for most of the fundamentally demanding computating tasks.
Its core is written in C and C is super fast.

# Working Examples of Using `Rcpp`

There will be cases where the native R code is not suitable for the desired implementation.
This is usually because the built-in `primitive` functions are not available for a particular kind of algorithm.
In such scenario we can use `Rcpp` to re-write only the most demanding part of our implementation in C++ and port it to our R program seamlessly.
In this section we are going to illustrate several such use cases.

## N-Gram Generation

N-gram generation is a common task for natural language processing and understanding.
Despite its simplicity,
surprisingly,
it is not easy for native R code to fulfill the task in an efficient manner.

Let's assume we'd like to count the bigrams of the [hex dump](https://en.wikipedia.org/wiki/Hex_dump) of a (potentially binary) file.
The byte code information could be useful for a downstream machine learning task such as file category classification.
But that is not our concern for now.
We just want to implement a function that can extract ngrams given a long string,
and count their frequencies.

```{r ngram_read_text}
# Prepare hex dump of a file.
# We will use the Rmd file itself as an example.
infile <- "rcpp.Rmd"
con <- file(infile, "rb")
hex <- readBin(con, what="raw", n=file.info(infile)$size)
hex_c <- as.character(hex)
close(con)

str(hex_c)  # A character vector of hexadecimals.
```

### Native R {-}

Now we implement the minimum ngram counter given a character vector,
assuming each element is a token:

```{r ngram_r}
ngram_r <- function(x, n) {
    len <- length(x) - (n - 1)
    out <- character(len)  # Pre-allocate size.
    for ( i in 1:len ) {
        out[i] <- x[i]
        if ( n > 1 )
            for ( j in 1:(n-1) )
                out[i] <- paste0(out[i], x[i+j])
    }
    sort(table(out), decreasing=TRUE)
}

out_r <- ngram_r(hex_c, 2)

print(length(out_r))  # Total ngrams.

print(out_r[1]) # The most frequent ngram.
```

The above function,
though implementation is straightforward,
is terribly slow because it didn't utilize vectorization.
And there doesn't seem to have a readily available `primitive` function for this very purpose.
This is the usually frustrated experience when people familiar with other general purpose languages first come to use R.

### Rcpp {-}

For a function requiring arbitraily large explicit loop and there is no obvious way to vectorization,
we have `Rcpp` come to the rescue.^[And also surprisingly, we don't really have many such cases in R programming where we really need to resort to its C interface. The languange is doing a very good job in vectorizing the most utilized functions for the majority of programming operations, especially in the data analytical field.]

For exactly the same minimum implementation we can re-write it in C++ using the `Rcpp` API:

```{r import_rcpp, message=FALSE}
library(Rcpp)
print(installed.packages()["Rcpp", "Version"])
```

```{r ngram_rcpp}
cppFunction("
  CharacterVector ngramRcpp (CharacterVector hexvector, int ngram) {
    int len = hexvector.size() - (ngram - 1);
    CharacterVector out(len);
    for (int i = 0; i < len; i++) {
      out(i) = hexvector[i];
      for (int j = 1; j < ngram; j++) {
        out(i) += hexvector[i+j];
      }
    }
    return out;
}")

ngramRcpp  # A primitive function.

ngram_rcpp <- function(x, n) {
  ngrams <- ngramRcpp(x, 2)
  sort(table(ngrams), decreasing=TRUE)
}

out_rcpp <- ngram_rcpp(hex_c, 2)

all.equal(out_rcpp, out_r, check.attributes=FALSE)
```

Notice that we only implement the ngram extraction loop but leave the frequency count for native R.
This is because the `table` function is not a bottleneck of our task.

In this example we use the `CharacterVector` class implemented in `Rcpp` which mimics the `character` vector we usually use in native R code.
There are many such high-level classes we can find in the `Rcpp` library.
These classes will save our development time when writing the C++ code since we won't limit ourselves to only C++ standard library.

```{r ngram_benchmark, paged.print=FALSE}
microbenchmark(
  ngram_r(hex_c, 2),
  ngram_rcpp(hex_c, 2),
  times=10
)
```

The `Rcpp` is much faster than the native R implementation.
And it will be even faster if the data is getting larger.

```{r read_big_file}
# Now take the entire R source code as one big file.
con <- file(downloaded_file, "rb")
big_hex <- readBin(con, what="raw", n=file.info(downloaded_file)$size)
big_hex_c <- as.character(big_hex)
close(con)

str(big_hex_c)
```

This time we benchmark with different file size,
we'll see that theoretically `Rcpp` can be hundreds of times faster than native R.

```{r ngram_benchmark_size}
# Benchmark the native R code with varying file sizes.
# Since it may take too long for the redundant tasks we'll use parallel computing.
# Another more clever way is to time on-the-fly when we parse the file and hence parse only once.
# In that case we need to modify the original function.
# Here we go for the brutal but simple way.
library(parallel)

cl <- makeCluster(detectCores() / 2)
clusterExport(cl, c("big_hex_c"))

n_size <- 10
sizes <- seq(1e3, length(big_hex_c), length.out=n_size)

time_ngram <- function(func, s, n) {
  st <- proc.time()
  func(big_hex_c[1:s], n)
  et <- proc.time()
  (et - st)["elapsed"]
}

# This could take a while.
time_r <- parLapplyLB(cl, sizes, time_ngram, func=ngram_r, n=2)

stopCluster(cl)


# Benchmark the Rcpp code with varying file sizes.
time_rcpp <- numeric(n_size)
for ( i in 1:length(sizes) ) {
  time_rcpp[i] <- time_ngram(ngram_rcpp, sizes[i], 2)
}

out_time <- data.table(r=unlist(time_r), rcpp=unlist(time_rcpp), size=sizes)
out_time <- melt(out_time, id.vars="size", variable.name="impl", value.name="time")
ggplot(out_time, aes(x=size, y=time, group=impl, color=impl)) +
  geom_line() +
  geom_point() +
  labs(x="File Size (Bytes)", y="Seconds",
       title="Speed on Bigram Generation of Varying Input Length")
```

### Other Packages {-}

For completeness,
in this section we include the benchmark for some other well-developed packages for text processing.
These packages are usually using R's native C interface or `Rcpp` to implement the critical parts of the computation for their intended tasks.
We will expect them to be also much more efficient than a naive implementation in native R code.

The first package we explore is [`quanteda`](https://github.com/quanteda/quanteda):

```{r import_quanteda, message=FALSE}
library(quanteda)
print(installed.packages()["quanteda", "Version"])
```

```{r ngram_quanteda}
# For quanteda we need to convert text to its `tokens` API.
# This may introduce some overhead so we will create two functions for fair comparison latter.
hex_s <- paste(hex_c, collapse=" ")
hex_t <- tokens(hex_s)

ngram_quanteda_1 <- function(x, n) {
  # Tokenization included.
  ngrams <- tokens_ngrams(tokens(x), n=n)[[1]]
  sort(table(ngrams), decreasing=TRUE)
}

ngram_quanteda_2 <- function(tok, n) {
  # Assume pre-tokenized.
  ngrams <- tokens_ngrams(tok, n=n)[[1]]
  sort(table(ngrams), decreasing=TRUE)
}

out_quanteda <- ngram_quanteda_1(hex_s, n=2)

all.equal(out_quanteda, out_r, check.attributes=FALSE)
```

[`text2vec`](https://github.com/dselivanov/text2vec) is also a high-performance package implemented with `Rcpp`:

```{r import_text2vec, message=FALSE}
library(text2vec)
print(installed.packages()["text2vec", "Version"])
```

```{r ngram_text2vec}
# Again to use text2vec we need to follow its API:
# Converting text into `itoken` iterator.
# Similar to the case of quanteda, we use two functions where one excludes the conversion overhead.
hex_i <- itoken(hex_s, progressbar=FALSE)

ngram_text2vec_1 <- function(x, n=n) {
  it <- itoken(x, progressbar=FALSE)
  create_vocabulary(it, ngram=c(n, n))
}

ngram_text2vec_2 <- function(it, n=n) {
  create_vocabulary(it, ngram=c(n, n))
}

out_text2vec <- ngram_text2vec_1(hex_s, n=2)

# Tidy the result to align with our previous functions.
out_text2vec <- setNames(out_text2vec$term_count, out_text2vec$term)

all.equal(as.table(sort(out_text2vec, decreasing=TRUE)), out_r, check.attributes=FALSE)
```

Lastly,
unlike the previous two packages both aim at a larger scope of natural language processing tasks,
the package [`ngram`](https://github.com/wrathematics/ngram) dedicates only at ngram generation task,
written mainly in C:

```{r import_ngram, message=FALSE}
library(ngram)
print(installed.packages()["ngram", "Version"])
```

```{r ngram_ngram}
ngram_ngram <- function(x, n) {
  ngram(x, n=n)
}

out_ngram <- ngram(hex_s)

# Tidy the result.
out_ngram <- get.phrasetable(out_ngram)
out_ngram <- setNames(out_ngram$freq, out_ngram$ngrams)

all.equal(as.table(out_ngram), out_r, check.attributes=FALSE)
```

Now we benchmark all the above implementations using the large file:

```{r ngram_benchmark_all, paged.print=FALSE}
benchmark_all <- function(x) {
  
  x_s <- paste(x, collapse=" ")
  x_t <- tokens(x_s)
  x_i <- itoken(x_s, progressbar=FALSE)
  
  microbenchmark(
    ngram_r(x, 2),
    ngram_rcpp(x, 2),
    ngram_quanteda_1(x_s, 2),
    ngram_quanteda_2(x_t, 2),
    ngram_text2vec_1(x_s, 2),
    ngram_text2vec_2(x_i, 2),
    ngram_ngram(x_s, 2),
    times=3
  )
}

x <- big_hex_c[1:as.integer(length(big_hex_c) / 10)]
system.time(
ben_all <- benchmark_all(x)
)

print(ben_all)
```

# References

```{r session_info}
sessionInfo()
```