---
title: "On Model Explainability"
subtitle: "From LIME, SHAP, to Interpretable Boosting"
author:
- name: Kyle Chung
  affiliation:
date: "`r format(Sys.time(), '%d %b %Y')` Last Updated (03 Dec 2019 First Uploaded)"
output:
  html_notebook:
    highlight: tango
    number_sections: yes
    theme: paper
    toc: yes
    toc_depth: 3
    toc_float: yes
    includes:
      in_header: /tmp/meta_header.html
  code_download: true
bibliography: model_explain.bib
nocite: |
  @reticulate
  @scikit-learn
  @tensorflow2015-whitepaper
abstract: |
  TBC.
---
<!--For controling code folding by chunk.-->
<script src="../site_libs/utils/hide_output.js"></script>

<!--For equation reference in Rmd.-->
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: { equationNumbers: { autoNumber: "AMS" } }
});
</script>

```{r meta, include=FALSE}
meta_header_file <- file("/tmp/meta_header.html")

# Add open graph meta.
meta <- c(
    '<meta name="author" content="Kyle Chung">',
    '<meta property="og:title" content="On Model Explainability: From Shap, Lime, to Interpretable Boosting">',
    '<meta property="og:type" content="article">',
    '<meta property="og:url" content="https://everdark.github.io/k9/model_explain/model_explain.nb.html">',
    '<meta property="og:image" content="https://everdark.github.io/k9/assets/androidify.jpg">',
    '<meta property="og:description" content="A data science notebook about machine learning model explainability.">'
)
contents <- meta

# Add Github corner.
github_corner_svg <- "../assets/github_corner.html"
github_corner_conf <- list(github_link="https://github.com/everdark/k9/tree/master/model_explain")
contents <- c(contents, stringr::str_interp(readLines(github_corner_svg), github_corner_conf))
writeLines(contents, meta_header_file)

close(meta_header_file)
```

```{r setup, include=FALSE}
library(reticulate)
r <- try(use_python(Sys.getenv("PYTHON_PATH"), required=TRUE), silent=TRUE)
if ( is(r, "try-error") ) {
  r <- try(use_virtualenv(Sys.getenv("PYTHON_PATH"), required=TRUE), silent=TRUE)
  if ( is(r, "try-error") ) use_condaenv(Sys.getenv("PYTHON_PATH"), required=TRUE)
}
```

---

This notebook is written with [`reticulate`](https://github.com/rstudio/reticulate),
a package that allows inter-operation between R and Python.

---

# Motivation

Why do we need to explain a machine learning model?
The benefit of an explanable model against a black-box model is for the model to be *trusted*.
Trust can be important in many real applications where the successful deployment of a machine learning model requires the trust from end users.
Sometimes trust plays a even bigger role than model accuracy.

Other than trust,
model explainability (or interpretability, interchangeably used hereafter) may also guide us in the correct direction to further improve the model.

In general,
linear model is more interpretable than non-linear model.
But the former also suffers from lower accuracy.
More advanced and hence complicated model usually has worse interpretability.

One should not confuse model explainability with the actual causality.
Being able to explain a model doesn't mean that we can identify any ground-truth causal relation behind the model.
Model explainability is for and only for the model,
but not for the facts we'd like to model.
Nevertheless,
understand how we can reason the model definitely will help us better model the actual pattern behind the scence.

In this notebook we will walk through 3 popular approaches of model prediction explanation,
each of them comes with a dedicated Python package:

1. [`shap`](https://github.com/slundberg/shap)
2. [`lime`](https://github.com/marcotcr/lime)
3. [`interpret`](https://github.com/interpretml/interpret)

# Explanation Models

An explanation model $g(x)$ is an *interpretable approximation* of the original model $f(x)$.
Its sole purpose is to give extra explainability the original model fails to provide,
due to its own complexity.

The general idea is to use a simplified input $x\prime$ such that $x = h_x(x\prime)$,
where $h_x(\cdot)$ is a mapping function for any given raw input $x$.
Then the interpretable approximation can be written as:

$$
g(x\prime) \approx f(h_x(x\prime)).
$$

The *additive feature attribution methods* specify the explanation model of the following form:

$$
g(x\prime) = \phi_0 + \sum_{i = 1}^m \phi_i x_i\prime,
$$

where $m$ is total number of simplified features,
$x\prime \in \{0, 1\}$ simply an indicator.^[In many such methods, the simplified input is the indicator of feature presence. One example: Shapley regression values.]
Apparently,
the choice of an additive model is for (linear) intrepretability.
The simplified features are an *interpretable representation* of the original model features.

# LIME

One very popular such above additive model is LIME (@ribeiro2016should).
LIME stands for Local Interpretable Model-Agnostic Explanations.
As its full name suggests,
LIME can be applied to *any* machine learning model.
LIME achieves prediction-level interpretability by approxmiating the original model with an explanation model locally around that prediction.

## On Text Classifiers

For text classification problem,
the most straightforward interpretable representation of the model features will be a binary indicator vector of bag of words.
So the explanation model will try to reason which word or token is driving the prediction in what direction.
And this is true no matter the form of the original model feature.
May it be a word count matrix,
a term frequency-inverse document frequency (TF-IDF) matrix,
or numerical embeddings.

In the following we experiment LIME with two binary classifiers on a small text dataset,
one using random forest with TF-IDF representation,
another using a neural network with word embeddings.

### Random Forest with TF-IDF Vectorization

This section is based on the [official demo script](https://marcotcr.github.io/lime/tutorials/Lime%20-%20basic%20usage%2C%20two%20class%20case.html),
with only minor modifications.
Particularly,
we restrict the vocabulary size to avoid potential overfitting simply due to extremely rare word.

```{python lime_rf_tfidf}
import os

from sklearn.datasets import fetch_20newsgroups
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report

home = os.path.expanduser("~")
data_home = os.path.join(home, "scikit_learn_data")
categories = ["alt.atheism", "soc.religion.christian"]
newsgroups_train = fetch_20newsgroups(subset="train", categories=categories, data_home=data_home)
newsgroups_test = fetch_20newsgroups(subset="test", categories=categories, data_home=data_home)

vectorizer = TfidfVectorizer(lowercase=True, min_df=10)
train_vectors = vectorizer.fit_transform(newsgroups_train.data)
test_vectors = vectorizer.transform(newsgroups_test.data)
print(len(vectorizer.vocabulary_))  # Without OOV token.

rf = RandomForestClassifier(n_estimators=300)
_ = rf.fit(train_vectors, newsgroups_train.target)
rf_pred = rf.predict(test_vectors)

print(classification_report(newsgroups_test.target, rf_pred))
```

For the tiny dataset RF seems to work well.
Now move on to model explanation with LIME:

```{python lime_on_rf}
from lime.lime_text import LimeTextExplainer
from sklearn.pipeline import make_pipeline

# We need a pipeline since LimeTextExplainer.explain_instance expects raw text input.
rf_pipe = make_pipeline(vectorizer, rf)
explainer = LimeTextExplainer(class_names=categories)

idx = 83
exp = explainer.explain_instance(
  newsgroups_test.data[idx], rf_pipe.predict_proba, num_features=6)

exp.save_to_file("/tmp/explain_text_rf.html")
```

```{r, echo=FALSE}
htmltools::includeHTML("/tmp/explain_text_rf.html")
```

### Neural Nets with Word Embeddings

Now let's try a neural network model with word embeddings trained from scratch.
We use `tensorflow.keras` API to quickly build and train a neural net:

```{python import_tf}
import logging
logging.getLogger("tensorflow").setLevel(logging.ERROR)
import warnings
warnings.simplefilter(action="ignore", category=FutureWarning)

import tensorflow as tf
print(tf.__version__)
if tf.test.is_gpu_available():
  print(tf.test.gpu_device_name())
```

```{python lime_nn_embeddings}
import numpy as np
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences

# Build vocabulary.
# Since we will use zero padding, 0 cannot be used as OOV index.
# Keras tokenizer by default reserves 0 already. OOV token, if used, will be indexed at 1.
# And len(tokenizer.index_word) will be all vocabulary instead of `num_words`.
vocab_size = 3001  # +1 for 0 index used for padding.
oov_token = "<unk>"
tokenizer = Tokenizer(lower=True, oov_token=oov_token, num_words=vocab_size - 1)
tokenizer.fit_on_texts(newsgroups_train.data)

# Encode text with padding to ensure fixed-length input.
seq_train = tokenizer.texts_to_sequences(newsgroups_train.data)
seq_train_padded = pad_sequences(seq_train, padding="post")
maxlen = seq_train_padded.shape[1]
seq_test = tokenizer.texts_to_sequences(newsgroups_test.data)
seq_test_padded = pad_sequences(seq_test, padding="post", maxlen=maxlen)

# Wrap Keras Sequential model with scikit-learn API.
# This is because LimeTextExplainer seems buggy with native Keras model.
def model_fn():
  embedding_size = 32
  model = tf.keras.Sequential([
    tf.keras.layers.Input(shape=(maxlen,), name="sequence"),
    tf.keras.layers.Embedding(vocab_size, embedding_size, mask_zero=True, name="word_embedding"),
    tf.keras.layers.GlobalAveragePooling1D(name="doc_embedding"),
    tf.keras.layers.Dense(embedding_size / 2, activation="relu", name="relu"),
    tf.keras.layers.Dense(1, activation="sigmoid", name="sigmoid")
  ], name="nn_classifier")
  model.compile(optimizer="adam",
                loss="binary_crossentropy",
                metrics=["accuracy"])
  return model

print(model_fn().summary(line_length=90))

model = tf.keras.wrappers.scikit_learn.KerasClassifier(model_fn)

losses = model.fit(
  x=seq_train_padded, y=newsgroups_train.target,
  batch_size=32, epochs=10,
  validation_data=(seq_test_padded, newsgroups_test.target),
  validation_steps=20,
  verbose=0)

nn_pred = (np.squeeze(model.predict(seq_test_padded)) > .5).astype(int)
print(classification_report(newsgroups_test.target, nn_pred))
```

```{python lime_on_nn_embeddings}
def nn_predict_fn(text):
  # This is for sklearn wrapper only.
  seq = tokenizer.texts_to_sequences(text)
  seq = pad_sequences(seq, padding="post", maxlen=maxlen)
  return model.predict_proba(seq)

explainer = LimeTextExplainer(class_names=categories)
model_exp = explainer.explain_instance(newsgroups_test.data[idx], nn_predict_fn, num_features=6)
model_exp.save_to_file("/tmp/explain_text_nn.html")
```

```{r, echo=FALSE}
htmltools::includeHTML("/tmp/explain_text_nn.html")
```

### Transfer Learning

One step further,
let's use pre-trained word embeddings for the neural nets and build another explanation model.
We will use [GloVe](https://nlp.stanford.edu/projects/glove/) (@pennington2014glove).
We use just the smaller GloVe model since our dataset is quite small.
In building the GloVe embeddings we need to take special care about out-of-vocabulary token AND padding index since we will be using the Keras API.

```{python lime_transfer_learning_vocab}
import pandas as pd

# Download GloVe pre-trained embeddings.
cache_dir = os.path.join(home, ".keras")
glove6b_path = "http://nlp.stanford.edu/data/glove.6B.zip"
glove6b_50_model_path = os.path.join(cache_dir, "datasets", "glove.6B.50d.txt")

if not os.path.exists(glove6b_50_model_path):
  _ = tf.keras.utils.get_file(fname="glove.6B.zip", origin=glove6b_path,
                              extract=True, cache_dir=cache_dir)

glove_all = pd.read_csv(glove6b_50_model_path, sep=" ", header=None, index_col=0, quoting=3)

# Map vocabulary to pre-trained embeddings.
matched_toks = []
for i, w in tokenizer.index_word.items():
  if i < vocab_size:
    if w in glove_all.index:
      matched_toks.append(w)
    else:
      matched_toks.append(oov_token)

# How many OOVs?
print(len([t for t in matched_toks if t == oov_token]))

# Note that GloVe pre-trained embeddings does not include its own OOV token.
# We will use a global average embedding to represent OOV token.
glove_all.loc[oov_token] = glove_all.values.mean(axis=0)
glove = glove_all.loc[matched_toks].values

# Append dummy 0-index vector to support padding.
glove = np.vstack([np.zeros((1, glove.shape[1])), glove])
print(glove.shape)
```

Now let's build the neural network.
Most of the code will be the same as before,
only the `Embedding` layer now we will use a constant matrix for initialization.
We will freeze the GloVe embeddings and only train the subsequent layers.
We expect the accuracy will be lower than our previous models.
But will the model be more reasonable?

```{python lime_transfer_learning_model}
def tr_model_fn():
  embedding_size = glove.shape[1]
  model = tf.keras.Sequential([
    tf.keras.layers.Input(shape=(maxlen,), name="sequence"),
    tf.keras.layers.Embedding(
      vocab_size, embedding_size,
      embeddings_initializer=tf.keras.initializers.Constant(glove),
      trainable=False, mask_zero=True, name="glove_embedding"),
    tf.keras.layers.GlobalAveragePooling1D(name="doc_embedding"),
    tf.keras.layers.Dense(embedding_size / 2, activation="relu", name="relu"),
    tf.keras.layers.Dense(1, activation="sigmoid", name="sigmoid")
  ], name="tr_classifier")
  model.compile(optimizer="adam",
                loss="binary_crossentropy",
                metrics=["accuracy"])
  return model

print(tr_model_fn().summary(line_length=90))

tr_model = tf.keras.wrappers.scikit_learn.KerasClassifier(tr_model_fn)

losses = tr_model.fit(
  x=seq_train_padded, y=newsgroups_train.target,
  batch_size=32, epochs=50,
  validation_data=(seq_test_padded, newsgroups_test.target),
  validation_steps=20,
  verbose=0)

tr_pred = (np.squeeze(tr_model.predict(seq_test_padded)) > .5).astype(int)
print(classification_report(newsgroups_test.target, tr_pred))
```

```{python lime_on_transfer_learning}
def tr_predict_fn(text):
  # This is for sklearn wrapper only.
  seq = tokenizer.texts_to_sequences(text)
  seq = pad_sequences(seq, padding="post", maxlen=maxlen)
  return tr_model.predict_proba(seq)

explainer = LimeTextExplainer(class_names=categories)
model_exp = explainer.explain_instance(newsgroups_test.data[idx], tr_predict_fn, num_features=6)
model_exp.save_to_file("/tmp/explain_text_tr.html")
```

```{r, echo=FALSE}
htmltools::includeHTML("/tmp/explain_text_tr.html")
```

# Shapley Regression Values

# SHAP

@NIPS2017_7062 propose SHAP (SHapley Additive exPlanations),
yet another additive feature attribution method for model explainability.
It can be applied to *any* machine learning model,
but comes with a customized fast implementation particularly for gradient boosting trees (GBT).
It supports APIs of well-known GBT libraries such as
[`xgboost`](https://github.com/dmlc/xgboost),
[`lightgbm`](https://github.com/microsoft/LightGBM),
and [`catboost`](https://github.com/catboost/catboost).

The interpretability provided by SHAP is *local*.
It assigns each feature an importance value *for a particular prediction.*
Hence it provides for any given model prediction what may be the driving force for the model to make such prediction.


# Explainable Boosting Machine

As of 2019-12-03,
`interpret` is still in its alpha release.

@caruana2015intelligible

generalized additive models with pairwise interactions

# References
