---
title: "On Model Explainability"
subtitle: "From LIME, SHAP, to Interpretable Boosting"
author:
- name: Kyle Chung
  affiliation:
date: "`r format(Sys.time(), '%d %b %Y')` Last Updated (04 Dec 2019 First Uploaded)"
output:
  html_notebook:
    highlight: tango
    number_sections: yes
    theme: paper
    toc: yes
    toc_depth: 3
    toc_float: yes
    includes:
      in_header: /tmp/meta_header.html
  code_download: true
bibliography: model_explain.bib
nocite: |
  @reticulate
  @scikit-learn
  @tensorflow2015-whitepaper
abstract: |
  Model explainability has gained more and more attention recently among machine learning practitioners. Especially with the popularization of deep learning frameworks, which further promotes the use of increasingly complicated models to improve accuracy. In the reality, however, model with the highest accuracy may not be the one that can be deployed. Trust is one important factor affecting the adoption of complicated models. In this notebook we give a brief introduction to several popular methods on model explainability. And we focus more on the hands-on which demonstrates how we can actually explain a model, under a variety of use cases.
---
<!--For equation reference in Rmd.-->
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: { equationNumbers: { autoNumber: "AMS" } }
});
</script>

```{r meta, include=FALSE}
meta_header_file <- file("/tmp/meta_header.html")

# Add open graph meta.
meta <- c(
  '<meta name="author" content="Kyle Chung">',
  '<meta property="og:title" content="On Model Explainability: From Shap, Lime, to Interpretable Boosting">',
  '<meta property="og:type" content="article">',
  '<meta property="og:url" content="https://everdark.github.io/k9/model_explain/model_explain.nb.html">',
  '<meta property="og:image" content="https://everdark.github.io/k9/assets/androidify.jpg">',
  '<meta property="og:description" content="A data science notebook about machine learning model explainability.">'
)
contents <- meta

# Add Github corner.
github_corner_svg <- "../assets/github_corner.html"
github_corner_conf <- list(github_link="https://github.com/everdark/k9/tree/master/model_explain")
contents <- c(contents, stringr::str_interp(readLines(github_corner_svg), github_corner_conf))
writeLines(contents, meta_header_file)

close(meta_header_file)
```

```{r setup, include=FALSE}
library(reticulate)
r <- try(use_python(Sys.getenv("PYTHON_PATH"), required=TRUE), silent=TRUE)
if ( is(r, "try-error") ) {
  r <- try(use_virtualenv(Sys.getenv("PYTHON_PATH"), required=TRUE), silent=TRUE)
  if ( is(r, "try-error") ) use_condaenv(Sys.getenv("PYTHON_PATH"), required=TRUE)
}

# Auto show matplotlib plots.
matplotlib <- import("matplotlib")
matplotlib$use("Agg", force=TRUE)
```

---

This notebook is written with [`reticulate`](https://github.com/rstudio/reticulate),
a package that allows inter-operation between R and Python.

---

# Motivation

Why do we need to explain a machine learning model?
The benefit of an explanable model against a black-box model is for the model to be *trusted*.
Trust can be important in many real applications where the successful deployment of a machine learning model requires the trust from end users.
Sometimes trust plays a even bigger role than model accuracy.

Other than trust,
model explainability (or interpretability, interchangeably used hereafter) may also guide us in the correct direction to further improve the model.

In general,
linear model is more interpretable than non-linear model.
But the former also suffers from lower accuracy.
More advanced and hence complicated model usually has worse interpretability.

One should not confuse model explainability with the actual causality.
Being able to explain a model doesn't mean that we can identify any ground-truth causal relation behind the model.
Model explainability is for and only for the model,
but not for the facts we'd like to model.
Nevertheless,
understand how we can reason the model definitely will help us better model the actual pattern behind the scence.

In this notebook we will walk through 3 popular approaches of model prediction explanation,
each of them comes with a dedicated Python package:

1. [`shap`](https://github.com/slundberg/shap)
2. [`lime`](https://github.com/marcotcr/lime)
3. [`interpret`](https://github.com/interpretml/interpret)

# Explanation Models

An explanation model $g(x)$ is an *interpretable approximation* of the original model $f(x)$.
Its sole purpose is to give extra explainability the original model fails to provide,
due to its own complexity.

The general idea is to use a simplified input $x\prime$ such that $x = h_x(x\prime)$,
where $h_x(\cdot)$ is a mapping function for any given raw input $x$.
Then the interpretable approximation can be written as:

$$
g(x\prime) \approx f(h_x(x\prime)).
$$

The *additive feature attribution methods* specify the explanation model of the following form:

$$
g(x\prime) = \phi_0 + \sum_{i = 1}^m \phi_i x_i\prime,
$$

where $m$ is total number of simplified features,
$x\prime \in \{0, 1\}$ simply an indicator.^[In many such methods, the simplified input is the indicator of feature presence. One example: Shapley regression values.]
Apparently,
the choice of an additive model is for (linear) intrepretability.
The simplified features are an *interpretable representation* of the original model features.

# LIME

One very popular such above additive model is LIME (@ribeiro2016should).
LIME stands for **Local Interpretable Model-Agnostic Explanations.**
As its full name suggests,
LIME can be applied to *any* machine learning model.
LIME achieves prediction-level interpretability by approxmiating the original model with an explanation model locally around that prediction.

## On Text Classifiers

For text classification problem,
the most straightforward interpretable representation of the model features will be a binary indicator vector of bag of words.
So the explanation model will try to reason which word or token is driving the prediction in what direction.
And this is true no matter the form of the original model feature.
May it be a word count matrix,
a term frequency-inverse document frequency (TF-IDF) matrix,
or numerical embeddings.

In the following we experiment LIME with two binary classifiers on a small text dataset,
one using random forest with TF-IDF representation,
another using a neural network with word embeddings.

### Random Forest with TF-IDF Vectorization

**TODO: Switch to a more meaningful dataset for demo throughout the notebook.**

This section is based on the [official demo script](https://marcotcr.github.io/lime/tutorials/Lime%20-%20basic%20usage%2C%20two%20class%20case.html),
with only minor modifications.
Particularly,
we restrict the vocabulary size to avoid potential overfitting simply due to extremely rare word.

```{python lime_rf_tfidf}
import os

from sklearn.datasets import fetch_20newsgroups
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report

home = os.path.expanduser("~")
data_home = os.path.join(home, "scikit_learn_data")
categories = ["alt.atheism", "soc.religion.christian"]
newsgroups_train = fetch_20newsgroups(subset="train", categories=categories, data_home=data_home)
newsgroups_test = fetch_20newsgroups(subset="test", categories=categories, data_home=data_home)

vectorizer = TfidfVectorizer(lowercase=True, min_df=10)
train_vectors = vectorizer.fit_transform(newsgroups_train.data)
test_vectors = vectorizer.transform(newsgroups_test.data)
print(len(vectorizer.vocabulary_))  # Without OOV token.

rf = RandomForestClassifier(n_estimators=300)
_ = rf.fit(train_vectors, newsgroups_train.target)
rf_pred = rf.predict(test_vectors)

print(classification_report(newsgroups_test.target, rf_pred))
```

For the tiny dataset RF seems to work well.
Now move on to model explanation with LIME:

```{python lime_on_rf}
from lime.lime_text import LimeTextExplainer
from sklearn.pipeline import make_pipeline

# We need a pipeline since LimeTextExplainer.explain_instance expects raw text input.
rf_pipe = make_pipeline(vectorizer, rf)
explainer = LimeTextExplainer(class_names=categories)

idx = 83
exp = explainer.explain_instance(
  newsgroups_test.data[idx], rf_pipe.predict_proba, num_features=6)
# For ipynb, one can simply call exp.show_in_notebook(text=True) to embed the html output.
exp.save_to_file("/tmp/explain_text_rf.html")
```

```{r, echo=FALSE}
htmltools::includeHTML("/tmp/explain_text_rf.html")
```

### Neural Nets with Word Embeddings

Now let's try a neural network model with word embeddings trained from scratch.
We use `tensorflow.keras` API to quickly build and train a neural net:

```{python import_tf}
import logging
logging.getLogger("tensorflow").setLevel(logging.ERROR)
import warnings
warnings.simplefilter(action="ignore", category=FutureWarning)

import tensorflow as tf
print(tf.__version__)
if tf.test.is_gpu_available():
  print(tf.test.gpu_device_name())
```

```{python lime_nn_embeddings}
import numpy as np
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences

# Build vocabulary.
# Since we will use zero padding, 0 cannot be used as OOV index.
# Keras tokenizer by default reserves 0 already. OOV token, if used, will be indexed at 1.
# And len(tokenizer.index_word) will be all vocabulary instead of `num_words`.
vocab_size = 3001  # +1 for 0 index used for padding.
oov_token = "<unk>"
tokenizer = Tokenizer(lower=True, oov_token=oov_token, num_words=vocab_size - 1)
tokenizer.fit_on_texts(newsgroups_train.data)

# Encode text with padding to ensure fixed-length input.
seq_train = tokenizer.texts_to_sequences(newsgroups_train.data)
seq_train_padded = pad_sequences(seq_train, padding="post")
maxlen = seq_train_padded.shape[1]
seq_test = tokenizer.texts_to_sequences(newsgroups_test.data)
seq_test_padded = pad_sequences(seq_test, padding="post", maxlen=maxlen)

# Wrap Keras Sequential model with scikit-learn API.
# This is because LimeTextExplainer seems buggy with native Keras model.
def model_fn():
  embedding_size = 32
  model = tf.keras.Sequential([
    tf.keras.layers.Input(shape=(maxlen,), name="sequence"),
    tf.keras.layers.Embedding(vocab_size, embedding_size, mask_zero=True, name="word_embedding"),
    tf.keras.layers.GlobalAveragePooling1D(name="doc_embedding"),
    tf.keras.layers.Dense(embedding_size / 2, activation="relu", name="relu"),
    tf.keras.layers.Dense(1, activation="sigmoid", name="sigmoid")
  ], name="nn_classifier")
  model.compile(optimizer="adam",
                loss="binary_crossentropy",
                metrics=["accuracy"])
  return model

print(model_fn().summary(line_length=90))

model = tf.keras.wrappers.scikit_learn.KerasClassifier(model_fn)

metrics = model.fit(
  x=seq_train_padded, y=newsgroups_train.target,
  batch_size=32, epochs=10,
  validation_data=(seq_test_padded, newsgroups_test.target),
  validation_steps=20,
  verbose=2)

nn_pred = (np.squeeze(model.predict(seq_test_padded)) > .5).astype(int)
print(classification_report(newsgroups_test.target, nn_pred))
```

```{python lime_on_nn_embeddings}
def nn_predict_fn(text):
  # This is for sklearn wrapper only.
  seq = tokenizer.texts_to_sequences(text)
  seq = pad_sequences(seq, padding="post", maxlen=maxlen)
  return model.predict_proba(seq)

explainer = LimeTextExplainer(class_names=categories)
model_exp = explainer.explain_instance(newsgroups_test.data[idx], nn_predict_fn, num_features=6)
model_exp.save_to_file("/tmp/explain_text_nn.html")
```

```{r, echo=FALSE}
htmltools::includeHTML("/tmp/explain_text_nn.html")
```

### Transfer Learning

One step further,
let's use pre-trained word embeddings for the neural nets and build another explanation model.
We will use [GloVe](https://nlp.stanford.edu/projects/glove/) (@pennington2014glove).
We use just the smaller GloVe model since our dataset is quite small.
In building the GloVe embeddings we need to take special care about out-of-vocabulary token AND padding index since we will be using the Keras API.

```{python lime_transfer_learning_vocab}
import pandas as pd

# Download GloVe pre-trained embeddings.
cache_dir = os.path.join(home, ".keras")
glove6b_path = "http://nlp.stanford.edu/data/glove.6B.zip"
glove6b_50_model_path = os.path.join(cache_dir, "datasets", "glove.6B.50d.txt")

if not os.path.exists(glove6b_50_model_path):
  _ = tf.keras.utils.get_file(fname="glove.6B.zip", origin=glove6b_path,
                              extract=True, cache_dir=cache_dir)

glove_all = pd.read_csv(glove6b_50_model_path, sep=" ", header=None, index_col=0, quoting=3)

# Map vocabulary to pre-trained embeddings.
matched_toks = []
for i, w in tokenizer.index_word.items():
  if i < vocab_size:
    if w in glove_all.index:
      matched_toks.append(w)
    else:
      matched_toks.append(oov_token)

# Note that GloVe pre-trained embeddings does not include its own OOV token.
# We will use a global average embedding to represent OOV token.
print(len([t for t in matched_toks if t == oov_token]))  # How many OOVs?

glove_all.loc[oov_token] = glove_all.values.mean(axis=0)
glove = glove_all.loc[matched_toks].values

# Append dummy 0-index vector to support padding.
glove = np.vstack([np.zeros((1, glove.shape[1])), glove])
print(glove.shape)
```

Now let's build the neural network.
Most of the code will be the same as before,
only the `Embedding` layer now we will use a constant matrix for initialization.
We will freeze the GloVe embeddings and only train the subsequent layers.
We expect the accuracy will be lower than our previous models.
But will the model be more reasonable?

```{python lime_transfer_learning_model}
def tr_model_fn():
  embedding_size = glove.shape[1]
  model = tf.keras.Sequential([
    tf.keras.layers.Input(shape=(maxlen,), name="sequence"),
    tf.keras.layers.Embedding(
      vocab_size, embedding_size,
      embeddings_initializer=tf.keras.initializers.Constant(glove),
      trainable=False, mask_zero=True, name="glove_embedding"),
    tf.keras.layers.GlobalAveragePooling1D(name="doc_embedding"),
    tf.keras.layers.Dense(embedding_size / 2, activation="relu", name="relu"),
    tf.keras.layers.Dense(1, activation="sigmoid", name="sigmoid")
  ], name="tr_classifier")
  model.compile(optimizer="adam",
                loss="binary_crossentropy",
                metrics=["accuracy"])
  return model

print(tr_model_fn().summary(line_length=90))

tr_model = tf.keras.wrappers.scikit_learn.KerasClassifier(tr_model_fn)

metrics = tr_model.fit(
  x=seq_train_padded, y=newsgroups_train.target,
  batch_size=32, epochs=50,
  validation_data=(seq_test_padded, newsgroups_test.target),
  validation_steps=20,
  verbose=2)

tr_pred = (np.squeeze(tr_model.predict(seq_test_padded)) > .5).astype(int)
print(classification_report(newsgroups_test.target, tr_pred))
```

```{python lime_on_transfer_learning}
def tr_predict_fn(text):
  # This is for sklearn wrapper only.
  seq = tokenizer.texts_to_sequences(text)
  seq = pad_sequences(seq, padding="post", maxlen=maxlen)
  return tr_model.predict_proba(seq)

explainer = LimeTextExplainer(class_names=categories)
model_exp = explainer.explain_instance(newsgroups_test.data[idx], tr_predict_fn, num_features=6)
model_exp.save_to_file("/tmp/explain_text_tr.html")
```

```{r, echo=FALSE}
htmltools::includeHTML("/tmp/explain_text_tr.html")
```

## On Tabular Data Classifier

**TODD: Find am interesting tabular dataset for demo.**

```{python}
from lime.lime_tabular import LimeTabularExplainer
```

# Shapley Regression Values

# SHAP

@NIPS2017_7062 propose SHAP (**SHapley Additive exPlanations**),
yet another additive feature attribution method for model explainability.
It is a more general approach where LIME is indeed only a special case of it.
Just like LIME,
in theory it can be applied to *any* machine learning model,
but comes with a customized fast implementation particularly for gradient boosting trees (GBT).
It supports APIs of well-known GBT libraries such as
[`xgboost`](https://github.com/dmlc/xgboost),
[`lightgbm`](https://github.com/microsoft/LightGBM),
and [`catboost`](https://github.com/catboost/catboost).

The interpretability provided by SHAP is again *local*.
It assigns each feature an importance value *for a particular prediction.*
Hence it provides for any given model prediction what may be the driving force for the model to make such prediction.

`shap` also comes with more visualization methods for feature investigation,
especially for feature interaction exploration.

## On Text Classifiers

### Random Forest with TF-IDF Vectorization

For exactly the same model we build in the previous section about `lime`,
this time we use `shap` to build the explanation model.

```{python shap_on_rf}
import shap

sorted_vocab = sorted(vectorizer.vocabulary_.items(), key=lambda kv: kv[1])
sorted_vocab = [w for w, i in sorted_vocab]

# shap doesn't support sparse matrix input for scikit-learn model.
test_vectors_d = test_vectors.toarray()

# Run explanation approximation for all testing examples at once.
rf_shape_explainer = shap.TreeExplainer(rf)
rf_shap_values = rf_shape_explainer.shap_values(test_vectors_d)

rf_shap_p = shap.force_plot(
  rf_shape_explainer.expected_value[1],
  rf_shap_values[1][idx,:],
  test_vectors_d[idx,:],
  feature_names=sorted_vocab
)
shap.save_html("/tmp/rf_shap_val.html", rf_shap_p)
```

```{r, echo=FALSE}
htmltools::includeHTML("/tmp/rf_shap_val.html")
```

```{python shap_on_rf_summary_plot}
top_k = 20
shap.summary_plot(rf_shap_values, test_vectors_d, feature_names=sorted_vocab, max_display=top_k, plot_size=.25)
```

```{python rf_feat_imp}
rf_feat_imp = pd.Series(rf.feature_importances_, index=sorted_vocab).sort_values()
rf_feat_imp.tail(top_k).plot(kind="barh")
# TODO:
# How to show this plot in rmd?
```

### Gradient Boosting Trees

Gradient boosting trees (GBT) is a powerful model family proven to work exceptionally well in many different applications.
Yet due to its ensembling nature,
GBT is also hard to intrepret in general.
Like random forest,
after building a model we will have access to the overall feature importance.
But at per-instance prediction level there is nothing much to say about why the outcome is what it is.

Let's quickly build a GBT using `lightgbm`:

```{python shap_lgb}
import lightgbm as lgb

d_train = lgb.Dataset(train_vectors, label=newsgroups_train.target,
                      feature_name=sorted_vocab)
d_test = lgb.Dataset(test_vectors, label=newsgroups_test.target,
                     feature_name=sorted_vocab)

lgb_params = {
  "learning_rate": .05,
  "boosting_type": "gbdt",
  "objective": "binary",
  "metric": ["binary_logloss", "auc"],
  "first_metric_only": True,
  "num_leaves": 6,
  "max_depth": 3,
  "min_data_per_leaf": 5,
  "verbose": -1
}

lgbooster = lgb.train(
  params=lgb_params,
  num_boost_round=300, early_stopping_rounds=20,
  train_set=d_train, valid_sets=[d_test],
  verbose_eval=50)

lgb_pred = (lgbooster.predict(test_vectors) > .5).astype(int)
print(classification_report(newsgroups_test.target, lgb_pred))
```

For the overall feature importance derived by GBT:

```{python shap_lgb_feat_imp}
lgb.plot_importance(lgbooster, max_num_features=20)
```

```{python shap_on_lgb}
# Sparse matrix is supported for lightgbm model.
lgb_explainer = shap.TreeExplainer(lgbooster)
lgb_shap_values = lgb_explainer.shap_values(test_vectors)

lgb_shap_p = shap.force_plot(
  lgb_explainer.expected_value[1],
  lgb_shap_values[1][idx,:],
  test_vectors[idx,:].toarray(),  # We still need a dense matrix here.
  feature_names=sorted_vocab
)
shap.save_html("/tmp/lgb_shap_val.html", lgb_shap_p)
```

```{r, echo=FALSE}
htmltools::includeHTML("/tmp/lgb_shap_val.html")
```

```{python verify_log_odds}
# Note that by default shap for lightgbm reports log-odds rather than probability.
# To verify this:
p = lgbooster.predict(test_vectors[idx,:].toarray())
print(np.log(p / (1 - p)))
```

### Neural Nets with Word Embeddings

Encountered a bug for TF 2.0:
+ https://github.com/slundberg/shap/issues/850
+ https://github.com/slundberg/shap/issues/885

```python
# shap does not support keras model in scikit-learn wrapper.
# Let's re-build the model and retain its Sequental class.

dl_model = model_fn()
metrics = dl_model.fit(
  x=seq_train_padded, y=newsgroups_train.target,
  batch_size=32, epochs=10,
  validation_data=(seq_test_padded, newsgroups_test.target),
  validation_steps=20,
  verbose=0)

dl_shap_explainer = shap.DeepExplainer(dl_model, seq_train_padded)
```

## On Tabular Data Classifier


# Explainable Boosting Machine

As of 2019-12-03,
`interpret` is still in its alpha release.

@caruana2015intelligible

generalized additive models with pairwise interactions

```{python}
from interpret.glassbox import ExplainableBoostingClassifier

#ebm = ExplainableBoostingClassifier()
#ebm.fit(X_train, y_train)

# EBM supports pandas dataframes, numpy arrays, and handles "string" data natively.
```

```{python}
from interpret import show

#ebm_global = ebm.explain_global()
#show(ebm_global)

#ebm_local = ebm.explain_local(X_test, y_test)
#show(ebm_local)
```

# For cross comparison.
# https://nbviewer.jupyter.org/github/interpretml/interpret/blob/master/examples/python/notebooks/Explaining%20Blackbox%20Classifiers.ipynb

# References
