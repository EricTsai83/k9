---
title: "On Model Explainability"
subtitle: "From LIME, SHAP, to Interpretable Boosting"
author:
- name: Kyle Chung
  affiliation:
date: "`r format(Sys.time(), '%d %b %Y')` Last Updated (03 Dec 2019 First Uploaded)"
output:
  html_notebook:
    highlight: pygments
    number_sections: yes
    theme: paper
    toc: yes
    toc_depth: 3
    toc_float: yes
    includes:
      in_header: /tmp/meta_header.html
  code_download: true
bibliography: model_explain.bib
nocite: |
  @reticulate
  @scikit-learn
  @tensorflow2015-whitepaper
abstract: |
  TBC.
---
<!--For controling code folding by chunk.-->
<script src="../site_libs/utils/hide_output.js"></script>

<!--For equation reference in Rmd.-->
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: { equationNumbers: { autoNumber: "AMS" } }
});
</script>

```{r meta, include=FALSE}
meta_header_file <- file("/tmp/meta_header.html")

# Add open graph meta.
meta <- c(
    '<meta name="author" content="Kyle Chung">',
    '<meta property="og:title" content="On Model Explainability: From Shap, Lime, to Interpretable Boosting">',
    '<meta property="og:type" content="article">',
    '<meta property="og:url" content="https://everdark.github.io/k9/model_explain/model_explain.nb.html">',
    '<meta property="og:image" content="https://everdark.github.io/k9/assets/androidify.jpg">',
    '<meta property="og:description" content="A data science notebook about machine learning model explainability.">'
)
contents <- meta

# Add Github corner.
github_corner_svg <- "../assets/github_corner.html"
github_corner_conf <- list(github_link="https://github.com/everdark/k9/tree/master/model_explain")
contents <- c(contents, stringr::str_interp(readLines(github_corner_svg), github_corner_conf))
writeLines(contents, meta_header_file)

close(meta_header_file)
```

```{r setup, include=FALSE}
library(reticulate)
r <- try(use_python(Sys.getenv("PYTHON_PATH"), required=TRUE), silent=TRUE)
if ( is(r, "try-error") ) {
  r <- try(use_virtualenv(Sys.getenv("PYTHON_PATH"), required=TRUE), silent=TRUE)
  if ( is(r, "try-error") ) use_condaenv(Sys.getenv("PYTHON_PATH"), required=TRUE)
}
```

---

This notebook is written with [`reticulate`](https://github.com/rstudio/reticulate),
a package that allows inter-operation between R and Python.

---

# Motivation

Why do we need to explain a machine learning model?
The benefit of an explanable model against a black-box model is for the model to be *trusted*.
Trust can be important in many real applications where the successful deployment of a machine learning model requires the trust from end users.
Sometimes trust plays a even bigger role than model accuracy.

Other than trust,
model explainability (or interpretability, interchangeably used hereafter) may also guide us in the correct direction to further improve the model.

In general,
linear model is more interpretable than non-linear model.
But the former also suffers from lower accuracy.
More advanced and hence complicated model usually has worse interpretability.

One should not confuse model explainability with the actual causality.
Being able to explain a model doesn't mean that we can identify any ground-truth causal relation behind the model.
Model explainability is for and only for the model,
but not for the facts we'd like to model.
Nevertheless,
understand how we can reason the model definitely will help us better model the actual pattern behind the scence.

In this notebook we will walk through 3 popular approaches of model prediction explanation,
each of them comes with a dedicated Python package:

1. [`shap`](https://github.com/slundberg/shap)
2. [`lime`](https://github.com/marcotcr/lime)
3. [`interpret`](https://github.com/interpretml/interpret)

# Explanation Models

An explanation model $g(x)$ is an *interpretable approximation* of the original model $f(x)$.
Its sole purpose is to give extra explainability the original model fails to provide,
due to its own complexity.

The general idea is to use a simplified input $x\prime$ such that $x = h_x(x\prime)$,
where $h_x(\cdot)$ is a mapping function for any given raw input $x$.
Then the interpretable approximation can be written as:

$$
g(x\prime) \approx f(h_x(x\prime)).
$$

The *additive feature attribution methods* specify the explanation model of the following form:

$$
g(x\prime) = \phi_0 + \sum_{i = 1}^m \phi_i x_i\prime,
$$

where $m$ is total number of simplified features,
$x\prime \in \{0, 1\}$ simply an indicator.^[In many such methods, the simplified input is the indicator of feature presence. One example: Shapley regression values.]
Apparently,
the choice of an additive model is for (linear) intrepretability.
The simplified features are an *interpretable representation* of the original model features.

# LIME

One very popular such above additive model is LIME (@ribeiro2016should).
LIME stands for Local Interpretable Model-Agnostic Explanations.
As its full name suggests,
LIME can be applied to *any* machine learning model.
LIME achieves prediction-level interpretability by approxmiating the original model with an explanation model locally around that prediction.

## On Text Classifiers

For text classification problem,
the most straightforward interpretable representation of the model features will be a binary indicator vector of bag of words.
So the explanation model will try to reason which word or token is driving the prediction in what direction.
And this is true no matter the form of the original model feature.
May it be a word count matrix,
a term frequency-inverse document frequency (TF-IDF) matrix,
or numerical embeddings.

In the following we experiment LIME with two binary classifiers on a small text dataset,
one using random forest with TF-IDF representation,
another using a neural network with word embeddings.

### Random Forest with TF-IDF Vectorization

This section is based on the [official demo script](https://marcotcr.github.io/lime/tutorials/Lime%20-%20basic%20usage%2C%20two%20class%20case.html),
with only minor modifications.

```{python lime_rf_tfidf}
import os

from sklearn.datasets import fetch_20newsgroups
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report

home = os.path.expanduser("~")
data_home = os.path.join(home, "scikit_learn_data")
categories = ["alt.atheism", "soc.religion.christian"]
newsgroups_train = fetch_20newsgroups(subset="train", categories=categories, data_home=data_home)
newsgroups_test = fetch_20newsgroups(subset="test", categories=categories, data_home=data_home)

vectorizer = TfidfVectorizer(lowercase=True)
train_vectors = vectorizer.fit_transform(newsgroups_train.data)
test_vectors = vectorizer.transform(newsgroups_test.data)

rf = RandomForestClassifier(n_estimators=300)
rf.fit(train_vectors, newsgroups_train.target)
rf_pred = rf.predict(test_vectors)

print(classification_report(newsgroups_test.target, rf_pred))
```

For the tiny dataset RF seems to work well.
Now move on to model explanation with LIME:

```{python lime_on_rf}
from lime.lime_text import LimeTextExplainer
from sklearn.pipeline import make_pipeline

# We need a pipeline since LimeTextExplainer.explain_instance expects raw text input.
rf_pipe = make_pipeline(vectorizer, rf)
explainer = LimeTextExplainer(class_names=categories)

idx = 83
exp = explainer.explain_instance(
  newsgroups_test.data[idx], rf_pipe.predict_proba, num_features=6)

exp.save_to_file("explain_text_rf.html")
```

```{r, echo=FALSE}
htmltools::includeHTML("explain_text_rf.html")
```

### Neural Nets with Word Embeddings

Now let's try a neural network model with word embeddings trained from scratch.
We use `tensorflow.keras` API to quickly build and train a neural net:

```{python import_tf}
import logging
logging.getLogger("tensorflow").setLevel(logging.ERROR)
import warnings
warnings.simplefilter(action="ignore", category=FutureWarning)

import tensorflow as tf
print(tf.__version__)
if tf.test.is_gpu_available():
  print(tf.test.gpu_device_name())
```

```{python lime_nn_embeddings}
import numpy as np
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences

# Build vocabulary.
tokenizer = Tokenizer(lower=True)
tokenizer.fit_on_texts(newsgroups_train.data)
vocab_size = len(tokenizer.index_word) + 1

# Encode text with Padding.
seq_train = tokenizer.texts_to_sequences(newsgroups_train.data)
seq_train_padded = pad_sequences(seq_train, padding="post")
maxlen = seq_train_padded.shape[1]
seq_test = tokenizer.texts_to_sequences(newsgroups_test.data)
seq_test_padded = pad_sequences(seq_test, padding="post", maxlen=maxlen)

# Wrap Keras Sequential model with scikit-learn API.
# This is because LimeTextExplainer seems buggy with native Keras model.
def model_fn():
  embedding_size = 32
  model = tf.keras.Sequential([
    tf.keras.layers.Input(shape=(maxlen,)),
    tf.keras.layers.Embedding(vocab_size, embedding_size, mask_zero=True),
    tf.keras.layers.GlobalAveragePooling1D(),
    tf.keras.layers.Dense(embedding_size / 2, activation="relu"),
    tf.keras.layers.Dense(1, activation="sigmoid")
  ])
  model.compile(optimizer="adam",
                loss="binary_crossentropy",
                metrics=["accuracy"])
  return model

model = tf.keras.wrappers.scikit_learn.KerasClassifier(model_fn)

model.fit(
  x=seq_train_padded, y=newsgroups_train.target,
  batch_size=32, epochs=10,
  validation_data=(seq_test_padded, newsgroups_test.target),
  validation_steps=20,
  verbose=0)

nn_pred = (np.squeeze(model.predict(seq_test_padded)) > .5).astype(int)
print(classification_report(newsgroups_test.target, nn_pred))
```

```{python lime_on_nn}
def nn_predict_fn(text):
  # This is for sklearn wrapper only.
  seq = tokenizer.texts_to_sequences(text)
  seq = pad_sequences(seq, padding="post", maxlen=maxlen)
  return model.predict_proba(seq)

explainer = LimeTextExplainer(class_names=categories)
model_exp = explainer.explain_instance(newsgroups_test.data[idx], nn_predict_fn, num_features=6)
model_exp.save_to_file("explain_text_nn.html")
```

```{r, echo=FALSE}
htmltools::includeHTML("explain_text_nn.html")
```

# Shapley Regression Values

# SHAP

@NIPS2017_7062 propose SHAP (SHapley Additive exPlanations),
yet another additive feature attribution method for model explainability.
It can be applied to *any* machine learning model,
but comes with a customized fast implementation particularly for gradient boosting trees (GBT).
It supports APIs of well-known GBT libraries such as
[`xgboost`](https://github.com/dmlc/xgboost),
[`lightgbm`](https://github.com/microsoft/LightGBM),
and [`catboost`](https://github.com/catboost/catboost).

The interpretability provided by SHAP is *local*.
It assigns each feature an importance value *for a particular prediction.*
Hence it provides for any given model prediction what may be the driving force for the model to make such prediction.


# Explainable Boosting Machine

As of 2019-12-03,
`interpret` is still in its alpha release.

@caruana2015intelligible

generalized additive models with pairwise interactions

# References
