---
title: "On Subword Units"
subtitle: "Segmentation for Natural Language Modeling"
author:
- name: Kyle Chung
  affiliation:
date: "`r format(Sys.time(), '%d %B %Y')` Last Updated"
output:
  html_notebook: 
    highlight: pygments
    number_sections: yes
    theme: paper
    toc: yes
    toc_depth: 3
    toc_float: yes
  code_download: true
bibliography: subword_units.bib
abstract: |
  Langauge segmentation or tokenization is the very first step toward a natural language understanding (NLU) task. The state-of-the-art NLU model usually involves neural networks with word embeddings as the workhorse to encode raw text onto vector space. A fixed vocabulary is pre-determined in order to facilitate such setup. With the challenge of out-of-vocabulary issue and non-whitespace-delimited language, we use subword units to further decompose raw texts into substrings. In this notebook we summarize the technique of subword segmentation in details with Python coding examples. We also provide a general usage walk-through for Google's open source library *SentencePiece*, a powerful language-agnostic unsupervised subword segmentation tool.
---
<!--For equation reference in Rmd.-->
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: { equationNumbers: { autoNumber: "AMS" } }
});
</script>

```{r setup, include=FALSE}
library(reticulate)
r <- try(use_python(Sys.getenv("PYTHON_PATH"), required=TRUE), silent=TRUE)
if ( is(r, "try-error") ) {
  r <- try(use_virtualenv(Sys.getenv("PYTHON_PATH"), required=TRUE), silent=TRUE)
  if ( is(r, "try-error") ) use_condaenv(Sys.getenv("PYTHON_PATH"), required=TRUE)
}
```

# Subword Units

Neural network models use a fixed size vocabulary to handle natrual language.
When the potentail vocabulary space is huge,
especially for a neural machine translation task,
there will be too many unknown words to a model.

To deal with such challenge,
(@sennrich2015neural) propose the idea to break up rare words into subword units for neural network modeling.
The segmentation approach is inspired by the [*byte pair encoding*](https://en.wikipedia.org/wiki/Byte_pair_encoding) compression algorithm,
or BPE in short.

## Byte Pair Encoding

BPE is a data compression algorithm that iteratively replaces the most frequent pair of bytes in a sequence with single unused byte.
By maintaining a mapping table of the new byte and the replaced old bytes,
we can recover the original message from a compressed representation by reversing the encoding process.

The same idea can be applied at charater-level instead of byte-level on a given corpus.

The procedure to apply BPE to obtain subwords can be summarized as the followings:

1. Extract word-level vocabulary as the initial vocabulary
2. Represent the initial vocabulary at character-level for each word, served as the working vocabulary
3. [Pairing] For each word in the working vocabulary, do character-level BPE and extract the most frequent pair concatenated as a new subword added into the subword vocabulary
4. [Merging] Replace the most frequent pair with its single concatenated version in the working vocabulary
5. Repeat step 3-4 for a given number of times
6. The final vocabulary is the initial fullword vocabulary + the subword vocabulary

Based on the procedure,
essentially we do not consider paris across word boundaries to be a potential subword.
Number of merge operations is the only hyperparameter for the algorithm,
usually determined by the desired final vocabulary size given a modeling problem.

Since the most frequent subwords will be merged early,
common words will remain as one unique symbol in the vocabulary,
leaving out rare words splitted into smaller units (subwords).

Here is a toy implementation of BPE applied on a given corpus:

```{python bpe}
import re
from collections import defaultdict


class BPE:

  def __init__(self, sents, min_cnt=3, verbose=False):
    self.verbose = verbose
    init_vocab = defaultdict(int)
    for sent in sents:
      words = re.split(r"\W+", sent)
      for w in words:
        if w != "":
          init_vocab[w] += 1
    # Create fullword vocabulary.
    self.word_vocab = {k: v for k, v in init_vocab.items() if v >= min_cnt}
    # Insert space between each char in a word for latter ease of merge operation.
    # We directly borrow the idea from https://www.aclweb.org/anthology/P16-1162.
    self.working_vocab = {" ".join(k): v for k, v in self.word_vocab.items()}
    self.subword_vocab = defaultdict(int)
    # Also build a character-level vocabulary as the base subwords.
    self.char_vocab = defaultdict(int)
    for sent in sents:
      for char in list(sent):
        self.char_vocab[char] += 1

  def _find_top_subword(self):
    subword_pairs = defaultdict(int)
    for w, cnt in self.working_vocab.items():
      subw = w.split()
      for i in range(len(subw) - 1):
        # Count bigrams.
        subword_pairs[subw[i], subw[i+1]] += cnt
    top_subw_pair = max(subword_pairs, key=subword_pairs.get)
    top_subw = "".join(top_subw_pair)
    self.subword_vocab[top_subw] = subword_pairs[top_subw_pair]
    if self.verbose:
      print("New subword added: {}".format(top_subw))
    return top_subw_pair

  def _merge(self, subw_pair):
    bigram = re.escape(" ".join(subw_pair))
    p = re.compile(r"(?<!\S)" + bigram + r"(?!\S)")
    self.working_vocab = {p.sub("".join(subw_pair), w): cnt for w, cnt in self.working_vocab.items()}
  
  def update_subword(self, n_merge=1):
    for n in range(n_merge):
      top_subw_pair = self._find_top_subword()
      self._merge(top_subw_pair)
```

Let's use Shakespeare as the input text example to run the BPE algorithm.
First we download the data from Google's public hosting service:

```{bash mkdir}
# bash
mkdir -p data
```

```{python bpe_dl_shakespeare}
import os
import shutil
import tensorflow as tf

shakes_file = "data/shakespeare.txt"
if not os.path.exists(shakes_file):
  shakes_dl_path = tf.keras.utils.get_file(
    "shakespeare.txt",
    "https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt")
  shutil.move(shakes_dl_path, shakes_file)
shakespeare = open(shakes_file, "rb").read().decode(encoding="utf-8")
shakespeare = shakespeare.lower().split("\n")

for sent in shakespeare[:20]:
  print(sent)
```

```{python bpe_shakespeare}
bpe = BPE(shakespeare, min_cnt=10)
print(len(bpe.word_vocab))

print(list(bpe.word_vocab.items())[:5])  # Print some from fullword vocabulary.

print(list(bpe.working_vocab.items())[:5])  # (For debug) Print some from the working vocab that we are going to perform the merge.

bpe.update_subword(n_merge=100)  # Do merge update.
print(len(bpe.subword_vocab))

print(list(bpe.working_vocab.items())[:5])  # Check the working vocabulary after merge.

print(list(bpe.subword_vocab.items())[:5])  # Print some subwords generated by the first 100 merge operations.
```

Each learned subword itself doesn't need to have explicit meaning.
It is their potential combinations with other subwords that reveal the actual context,
which can be hopefully learned by neural network embeddings.

A modification of the above BPE segmentation is to expand subword vocabulary by likelihood instead of the most frequent pair.
That is,
in each merge operation we choose the new subword that increases the likelihood of the training data the most.

##  Probablistic Subword Segmentation

Note that BPE is *deterministic* by a greedy replacement of adjacent symbols,
while the actual subword segmentation is potentially ambiguous.
A sequence can be represented in multiple subword sequences since even the same word can be segmented differently by different subwords.
@kudo2018subword propose an approach called "subword regularization" to handle this issue in a probablistic fashion.
A new subword segmentation under this approach uses a *unigram language model* for generating subword segmentation with attached probability.

Denote $X$ as a subword sequence of length $n$

$$
X = \begin{pmatrix} x_1 & x_2 & \dots & x_n\end{pmatrix},
$$

with *independent* subword probability $P(x_i)$.
The probability of the sequence hence is

$$
P(X) = \prod_{i=1}^n P(x_i),
$$

with

$$
\sum_{x \in V}P(x) = 1,
$$

where $V$ is a pre-determined vocabulary set.

The most probable sequence (from all possible segmentation candidates) can be found by applying [expectation-maximization](https://en.wikipedia.org/wiki/Expectation%E2%80%93maximization_algorithm) with the [Viterbi algorithm](https://en.wikipedia.org/wiki/Viterbi_algorithm),
which is designed for finding the most likely sequence of hidden states.
Here the hidden states are subwords used in a sequence.

The vocabulary set $V$ is indeed undertermined at the very beginning and cannot be solved simultaneously with the maximization task.
A workaround is to provide a seed vocabulary as the initial (reasonably big enough) vocabulary,
and shrink the seed vocabulary during training until a desired vocabulary size is reached.

### Expectation-Maximization

For completeness let's briefly review the EM algorithm.

#### Working Example: A Gaussian Mixture {-}

The Hello-World example of EM is usually a Gaussian Mixture model.
In its simplest illustration,
assume we observe a set of data resulting from either one of two different Gaussians,
how can we estimate parameters of these two Gaussians without knowing which Gaussian they came from?^[We borrow the example from [this stack-overflow thread](https://stackoverflow.com/questions/11808074/what-is-an-intuitive-explanation-of-the-expectation-maximization-technique/43561339#43561339), with modification to use a general initial guess by randomizing the hidden state weights.]

```{python em_toy_data}
import numpy as np

np.random.seed(777)
size = 100

g1_mean = 3
g2_mean = 7
g1_std = 1
g2_std = 2

g1_data = np.random.normal(g1_mean, g1_std, size=size)
g2_data = np.random.normal(g2_mean, g2_std, size=size)
mixture_data = np.concatenate([g1_data, g2_data])
```

```{r em_toy_data_hist, results="hold"}
# R
x <- py$mixture_data
hist(x, xlab="Data", main="Sample Data from a Gaussian Mixture",
     probability=TRUE, ylim=c(0, .45))
curve(dnorm(x, mean=py$g1_mean, sd=py$g1_std),
      col="blue", lty="dotted", add=TRUE, yaxt="n")
curve(dnorm(x, mean=py$g2_mean, sd=py$g2_std),
      col="red", lty="dotted", add=TRUE, yaxt="n")
```

```{python em}
from scipy import stats


class GaussianMixtureEM:

  def __init__(self, data):
    self.data = data
    # Initialize hidden state weight.
    # Note that there is no guarantee that g1 will correspond to the first half of the data.
    # The actual estimated assignment is determined by the final weights.
    self.g1_weights = np.random.uniform(size=len(data))
    self.g2_weights = 1 - self.g1_weights
    # Initialize parameter guessings.
    self.update_estimates()

  def update_estimates(self):
    self.g1_mean = self.estimate_mean(self.g1_weights)
    self.g2_mean = self.estimate_mean(self.g2_weights)
    self.g1_std = self.estimate_std(self.g1_mean, self.g1_weights)
    self.g2_std = self.estimate_std(self.g2_mean, self.g2_weights)

  def estimate_mean(self, weights):
      return np.sum(self.data * weights) / np.sum(weights)

  def estimate_std(self, mean, weights):
      variance = np.sum(weights * (self.data - mean)**2) / np.sum(weights)
      return np.sqrt(variance)

  def em(self, n_iter=10):
    for n in range(n_iter):
      g1_lik = stats.norm(self.g1_mean, self.g1_std).pdf(self.data)
      g2_lik = stats.norm(self.g2_mean, self.g2_std).pdf(self.data)
      self.g1_weights = g1_lik / (g1_lik + g2_lik)
      self.g2_weights = g2_lik / (g1_lik + g2_lik)
      self.update_estimates()


mix_model = GaussianMixtureEM(mixture_data)
mix_model.em(30)
```

```{python em_result}
true_params = (g1_mean, g2_mean, g1_std, g2_std)
mle_params = (
  np.mean(mixture_data[:size]),
  np.mean(mixture_data[size:]),
  np.std(mixture_data[:size]),
  np.std(mixture_data[size:])
)
# To ensure the ordering so we can have a nicely printed comparison.
em_params = np.concatenate([
  np.sort([mix_model.g1_mean, mix_model.g2_mean]),
  np.sort([mix_model.g1_std, mix_model.g2_std])
])


for i, (t, m, e) in enumerate(zip(true_params, mle_params, em_params)):
  if i == 0:
    print("True Value | MLE (Known Hidden States) | EM (Unknown Hidden States)")
  print("{:^11}|{:^27}|{:^27}".format(t, np.round(m, 4), np.round(e, 4)))
```

### Viterbi Algorithm

Let's use the Shakespeare to illustrate the Viterbi algorithm.

For a given text line we'd like to calculate the most probable subwords that can recover the text line.
In order to achieve that we need to have a vocabulary for subwords that can attribute each subword to a probability.
We can use BPE to build such vocabulary.
For complete coverage we will also include character-level subwords into the vocabulary.

```{python viterbi_seed}
# Build a seeding subword vocabulary using BPE.
bpe = BPE(shakespeare, min_cnt=10)
bpe.update_subword(n_merge=1000)
subword_cnt = {**bpe.char_vocab, **bpe.subword_vocab}
tot_cnt = np.sum(list(subword_cnt.values()))
subword_logp = {k: np.log(v / tot_cnt) for k, v in subword_cnt.items()}
```

#### Forward Step {-}

Now we can implement the *forward* step of Viterbi:

```{python viterbi_forward}
def viterbi_forward(word, subword_logp):
  """Forward step of Viterbi given a single word."""
  # Create storage array for best substring recorded at each character position.
  best_subw_slices =  [None] * (len(word) + 1)
  neg_loglik = np.zeros(len(word) + 1)
  # Search the best substring given every possible end of position along the word.
  for eow in range(1, len(word) + 1):
    # For every end-of-word position:
    neg_loglik[eow] = np.inf
    for bow in range(eow):
      # For every possible beginning-of-word position given the end-of-word position:
      subw = line[bow:eow]
      if subw in subword_logp:
        logp = subword_logp[subw]
        # Compute subword probability:
        # P(current segment) + P(the best segment just before the current segment).
        s = neg_loglik[bow] - logp
        if s < neg_loglik[eow]:
          neg_loglik[eow] = s
          best_subw_slices[eow] = (bow, eow)
  return neg_loglik, best_subw_slices
```

```{python viterbi_forward_demo}
# Take a text line from Shakespeare.
line = shakespeare[213]
words = line.split()

w = words[0]
print(w)  # Word for segmentation demo.

subw_scores, subw_slices = viterbi_forward(w, subword_logp)
print(subw_slices)  # Best segment indices at each subword end.

subw = [w[idx[0]:idx[1]] for idx in subw_slices if idx is not None]
print(subw)  # Best segmented subword at each subword end.
```

Let's visualize the segmentation.
Numbers beneath the subword are the corresponding negative log-likelihood for that subword.
(The lower the better.)

```{r viterbi_segment_plot, results="hold"}
# R
library(ggplot2)
library(data.table)

subw_demo <- rbindlist(py$subw_slices)
setnames(subw_demo, c("start", "end"))
subw_demo[, subw:=py$subw]
subw_demo <- rbind(list(0, 0, ""), subw_demo)  # To plot one more block at the top.
subw_demo <- rbind(subw_demo, list(0, 0, "_"))  # To plot one more block at the bottom.
subw_demo[, subw:=factor(subw, levels=rev(subw))]
subw_demo[, score:=c(py$subw_scores, 0)]

# To also plot the entire word by character.
w_df <- data.table(x=rep("", nrow(subw_demo) - 2), y=0:(nrow(subw_demo)-3) + .5,
                   label=unlist(strsplit(py$w, NULL)))

viterbi_demo_p <- ggplot(subw_demo, aes(x=subw, y=(start + end) / 2, label=subw)) +
  geom_linerange(aes(x=subw, ymin=start, ymax=end), size=1.1) +
  geom_text(vjust=1, size=7) +
  geom_text(data=subw_demo[2:(nrow(subw_demo) - 1)],
            aes(label=sprintf("(%.2f)", score)), vjust=3.5, size=3, color="darkgrey") +
  geom_text(data=w_df, aes(x=x, y=y, label=label), size=14) +
  geom_point(aes(x=subw, y=end), data=subw_demo[2:(nrow(subw_demo) - 1)], size=3) +
  scale_y_continuous("", breaks=0:(nrow(subw_demo)-2), sec.axis=dup_axis()) +
  theme(axis.title.y=element_blank(),
        axis.text.y=element_blank(),
        axis.ticks.y=element_blank(),
        panel.grid.minor=element_blank()) +
  coord_flip()

viterbi_demo_p
```

We iterate over every position in a given word.
At each position,
we determine the best segment by finding the one with highest likelihood given the vocabulary.
For example,
when we are at position 2,
we compare the log-liklihood of `(wh)` and `(w, h)`,
the former is more likely so we record its negative log-likelihood along with the segment indices and then move onto the next position.
Now at position 3 we compare `(whe)` with `(w, he)` and `(wh, e)` and again return the best segment.
Notice that we don't compare `(w, h, e)` since in the previous position we already rule `(w, h)` out due to its being inferior to `(wh)`.

#### Backward Step {-}

After the forward step is finished,
we can track backward the path of segments that generates the highest likelihood.

```{python viterbi_backward}
def viterbi_backward(word, subw_slices):
  """Backward step of Viterbi to return the best path."""
  subwords = []
  subword_slices = []
  next_slices = subw_slices[-1]
  while next_slices is not None:
    subw = word[next_slices[0]:next_slices[1]]
    subwords.append(subw)
    subword_slices.append(next_slices)
    next_slices = subw_slices[next_slices[0]]
  subwords.reverse()
  return subwords, subword_slices


# Test with a single word.
best_subw, best_slices = viterbi_backward(w, subw_slices)
print(best_subw)
```

This is easily understandable if we also visualize the backward step:

```{r vitewrbi_backward_plot}
# R
seg <- rbindlist(py$best_slices)
setnames(seg, c("yend", "y"))
seg[, x:=match(rev(py$best_subw), rev(subw_demo$subw))]
seg[, xend:=x]
seg[, subw:=""]  # Required to bypass ggplot data checking.

viterbi_demo_p +
  geom_segment(aes(x=x, xend=xend, y=y, yend=yend), data=seg,
               col="red", arrow=arrow(length=unit(0.03, "npc")), size=1.5)
```

Now combining forward and backward for a complete segmentation of a text line:

```{python viterbi}
def viterbi(line):
  out = []
  words = line.split()
  for w in words:
    subw_scores, subw_slices = viterbi_forward(w, subword_logp)
    subw, subw_idx = viterbi_backward(w, subw_slices)
    out.extend(subw)
  return out

print(line)  # Input sentence.
print(viterbi(line))  # Output subword sequence.
```

### EM with Viterbi

Now we know the idea of EM,
and we know how to find the optimal path by Viterbi,
we can put them together.

Here is the complete procedure to the probabilistic subword segmentation:

1. Initialize a large seeding subword vocabulary from the training corpus.
2. Estimate each subword probability by the corresponding frequency counts in the vocabulary. This is the *expectation* step.
3. Use Viterbi to segment the corpus, returning optimal segments. This is the *maximization* step.
4. Compute the loss of each new subword from optimal segments. The loss of a subword is the reduction in sequence likelihood if that subword is removed from the vocabulary.
5. Shrink the vocabulary size by dropping the subwords with top X% smallest losses.
6. Repeat step 2 to 5 until the vocabulary size reaches a desired number.

#### On the Seed Vocabulary {-}

The natural choice is to use the union of all characters and the most frequent substrings in the corpus.
This can be done efficiently by the [suffix array](https://en.wikipedia.org/wiki/Suffix_array) algorithm.

Another approach is to run BPE with a sufficient number of merges to gather a large enough number of initial subwords,
as what we did in our previous toy illustration.

# SentencePiece: Unsupervised Segmentation

@kudo2018sentencepiece propose a language-agnostic subword segmentation algorithm called [SentencePiece](https://github.com/google/sentencepiece).
SentencePiece is a powerful and efficient open-sourced unsupervised langugae tokenizer that can help build vocabulary for neural networks.
In this section we will deep-dive into this model from both its theoretical stands and practical usage.

## Language Agnostic

SentencePiece is langauge independent becuase under the hood it simply treats a given sentence as a sequence of *Unicode* characters.
And most langauge in the world we speak can be encoded in Unicode.
This is particualrly useful for non-whitespace-segmented languages such as Chinese,
Korean and Japanese.

Technically speaking,
whitespace is also handled as a normal symbol in SentencePiece.
To achieve this,
a meta symbol "▁" (U+2581) is used to replace the whitespace.

The entire vocabulary is built from subwords so there is also no need to do pre-processing to provide a fullword initial vocabulary.
This bypasses the need for any language-specific preprocessing logic,
resulting in a unified framework for natural langauge segmentation.

```{python spm_unigram, results="hide"}
import sentencepiece as spm
spm_args = "--input=data/shakespeare.txt"
spm_args += " --model_prefix=m"
spm_args += " --vocab_size=1000"
spm_args += " --model_type=unigram"
spm.SentencePieceTrainer.Train(spm_args)
```

```{r}
library(data.table)
vv <- fread("m.vocab", header=FALSE, encoding="UTF-8")
setnames(vv, c("subword", "logp"))
vv[, p:=exp(logp)]
head(vv, 10)
```


```{python}
sp = spm.SentencePieceProcessor()
sp.Load("m.model")
```

```{python}
subword_pieces = sp.EncodeAsPieces("This is a test.")
print(subword_pieces)
subword_ids = sp.EncodeAsIds("This is a test.")
print(subword_ids)

print(sp.DecodePieces(subword_pieces))
print(sp.DecodeIds(subword_ids))

# By default the first 3 ids are token for unknown, bos and eos.
for i in range(3):
  print(sp.IdToPiece(i))

# The word "test" is not common in Shakespeare.
print(sp.PieceToId("test"))

# Recover the whitespace.
print("".join(subword_pieces).replace("\u2581", " "))
```

Subword segmentation sampling:

```{python}
for seg in sp.NBestEncodeAsPieces("This is a test", 5):
  print(seg)
```


```{python}
# Special start of word symbol.
print(u"\u2581".encode("utf-8"))
```

We can also use SentencePiece with BPE segmentation:

```{python spm_bpe, results="hide"}
import sentencepiece as spm
spm_args = "--input=data/shakespeare.txt"
spm_args += " --model_prefix=m"
spm_args += " --vocab_size=100"
spm_args += " --model_type=bpe"
spm.SentencePieceTrainer.Train(spm_args)
```


# References
