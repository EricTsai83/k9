---
title: "Understand Subword Units for Natural Language Modeling"
subtitle: ""
author:
- name: Kyle Chung
  affiliation:
date: "`r format(Sys.time(), '%d %B %Y')` Last Updated"
output:
  html_notebook: 
    highlight: pygments
    number_sections: yes
    theme: paper
    toc: yes
    toc_depth: 4
    toc_float: yes
  code_download: true
bibliography: subword_units.bib
abstract: |
  TBC.
---
<!--For equation reference in Rmd.-->
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: { equationNumbers: { autoNumber: "AMS" } }
});
</script>

```{r setup, include=FALSE}
library(reticulate)
r <- try(use_python(Sys.getenv("PYTHON_PATH"), required=TRUE), silent=TRUE)
if ( is(r, "try-error") ) {
  r <- try(use_virtualenv(Sys.getenv("PYTHON_PATH"), required=TRUE), silent=TRUE)
  if ( is(r, "try-error") ) use_condaenv(Sys.getenv("PYTHON_PATH"), required=TRUE)
}
```

# Subword Units

Neural network models use a fixed size vocabulary to handle natrual language.
When the potentail vocabulary space is huge,
especially for a neural machine translation task,
there will be too many unknown words to a model.

To deal with such challenge,
(@sennrich2015neural) propose the idea to break up rare words into subword units for neural network modeling.
The segmentation approach is inspired by the [*byte pair encoding*](https://en.wikipedia.org/wiki/Byte_pair_encoding) compression algorithm,
or BPE in short.

## Byte Pair Encoding

BPE is a data compression algorithm that iteratively replaces the most frequent pair of bytes in a sequence with single unused byte.
By maintaining a mapping table of the new byte and the replaced old bytes,
we can recover the original message from a compressed representation by reversing the encoding process.

The same idea can be applied at charater-level instead of byte-level on a given corpus.

The procedure to apply BPE to obtain subwords can be summarized as the followings:

1. Extract word-level vocabulary as the initial vocabulary
2. Represent the initial vocabulary at character-level for each word, served as the working vocabulary
3. [Pairing] For each word in the working vocabulary, do character-level BPE and extract the most frequent pair concatenated as a new subword added into the subword vocabulary
4. [Merging] Replace the most frequent pair with its single concatenated version in the working vocabulary
5. Repeat step 3-4 for a given number of times
6. The final vocabulary is the initial fullword vocabulary + the subword vocabulary

Based on the procedure,
essentially we do not consider paris across word boundaries to be a potential subword.
Number of merge operations is the only hyperparameter for the algorithm,
usually determined by the desired final vocabulary size given a modeling problem.

Since the most frequent subwords will be merged early,
common words will remain as one unique symbol in the vocabulary,
leaving out rare words splitted into smaller units (subwords).

Here is a toy implementation of BPE applied on a given corpus:

```{python bpe}
import re
from collections import defaultdict

class BPE:
  def __init__(self, sents, min_cnt=3):
    init_vocab = defaultdict(int)
    for sent in sents:
      words = re.split(r"\W+", sent)
      for w in words:
        if w != "":
          init_vocab[w] += 1
    # Create fullword vocabulary.
    self.word_vocab = {k: v for k, v in init_vocab.items() if v >= min_cnt}
    # Insert space between each char in a word for latter ease of merge operation.
    # We directly borrow the idea from https://www.aclweb.org/anthology/P16-1162.
    self.init_vocab = {" ".join(k): v for k, v in self.word_vocab.items()}
    self.subword_vocab = defaultdict(int)
    self.subword_pairs = defaultdict(int)
    
  def _find_top_subword(self):
    for w, cnt in self.init_vocab.items():
      subw = w.split()
      for i in range(len(subw) - 1):
        self.subword_pairs[subw[i], subw[i+1]] += cnt
    top_subw_pair = max(bpe.subword_pairs, key=bpe.subword_pairs.get)
    top_subw = "".join(top_subw_pair)
    self.subword_vocab[top_subw] = bpe.subword_pairs[top_subw_pair]
    return top_subw_pair

  def _merge(self, subw_pair):
    bigram = re.escape(" ".join(subw_pair))
    p = re.compile(r"(?<!\S)" + bigram + r"(?!\S)")
    self.init_vocab = {p.sub("".join(subw_pair), w): cnt for w, cnt in self.init_vocab.items()}
  
  def update_subword(self, n_merge=1):
    for n in range(n_merge):
      top_subw_pair = self._find_top_subword()
      self._merge(top_subw_pair)
```

Let's use Shakespeare as the input text example to run the BPE algorithm.
First we download the data from Google's public hosting service:

```{python bpe_dl_shakespeare}
import tensorflow as tf

shakes_file = tf.keras.utils.get_file(
  "shakespeare.txt",
  "https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt")
shakespeare = open(shakes_file, "rb").read().decode(encoding="utf-8")
shakespeare = shakespeare.lower().split("\n")

for sent in shakespeare[:20]:
  print(sent)
```

```{python bpe_shakespeare}
bpe =  BPE(shakespeare, min_cnt=50)
print(len(bpe.word_vocab))

# Print the first 10 subwords generated by the first 10 merge operations.
bpe.update_subword(n_merge=10)
for subw in bpe.subword_vocab:
  print(subw)
```

##  Probablistic Subword Segmentation

Note that BPE is *deterministic*,
while the actual subword segmentation is potentially ambiguous.
A sequence can be represented in multiple subword sequences since even the same word can be segmented differently by subwords.
@kudo2018subword propose an approach called "subword regularization" to handle this issue in a probablistic fashion.
A new subword segmentation under this approach uses a unigram language model for generating subword segmentation with attached probability.

Denote $X$ as a subword sequence of length $n$

$$
X = \begin{pmatrix} x_1 & x_2 & \dots & x_n\end{pmatrix},
$$

with *independent* subword probability $P(x_i)$.
The probability of the sequence hence is

$$
P(X) = \prod_{i=1}^n P(x_i),
$$

with

$$
\sum_{x \in V}P(x) = 1,
$$

where $V$ is a pre-determined vocabulary set.

The most probable sequence (from all possible segmentation candidates) can be found by applying the [Viterbi algorithm](https://en.wikipedia.org/wiki/Viterbi_algorithm) which is a dynamic programming algorithm designed for finding the most likely sequence of hidden states.

The vocabulary set $V$ is indeed undertermined at the very beginning and cannot be solved simultaneously with the maximization task.
A workaround is to provide a seed vocabulary as the initial (reasonably big enough) vocabulary,
and shrink the seed vocabulary during training until a desired vocabulary size is reached.

#### Expectation-Maximization {-}

[A brief review on EM algorithm here?]

#### Seed Vocabulary {-}

The natural choice is to use the union of all characters and the most frequent substrings in the corpus.
Another approach is to run BPE with a sufficient number of merges.



```{python}

```

# SentencePiece: Unsupervised Segmentation

@kudo2018sentencepiece propose a language-agnostic subword segmentation model called [SentencePiece](https://github.com/google/sentencepiece).
SentencePiece is a powerful and efficient open-sourced unsupervised langugae tokenizer that can help build vocabulary for neural networks.
In this section we will deep-dive into this model from both its theoretical stands and practical usage.

SentencePiece is langauge independent becuase under the hood it simply treats a given sentence as a sequence of Unicode characters.
And most langauge in the world we speak can be encoded in Unicode.
This is particualrly useful for non-whitespace-segmented languages such as Chinese, Korean and Japanese.

The entire vocabulary is built from subwords so there is also no need to do pre-processing to provide a fullword initial vocabulary.
This bypasses the need for any language-specific preprocessing logic,
resulting in a unified framework for natural langauge segmentation.

```{python spm}
import sentencepiece as spm
spm.SentencePieceTrainer.Train('--input=shakespeare.txt --model_prefix=m --vocab_size=1000')
```


```{python}
sp = spm.SentencePieceProcessor()
sp.Load("m.model")
```

```{python}
subword_pieces = sp.EncodeAsPieces("This is a test.")
print(subword_pieces)
subword_ids = sp.EncodeAsIds("This is a test.")
print(subword_ids)

print(sp.DecodePieces(subword_pieces))
print(sp.DecodeIds(subword_ids))

for i in range(3):
  print(sp.IdToPiece(i))
print(sp.PieceToId('</s>'))
```

```{python}
for seg in sp.NBestEncodeAsPieces("This is a test", 5):
  print(seg)
```


```{python}
# Special start of word symbol.
print(u"\u2581".encode("utf-8"))
```

```{python}
# Segment sampling.
print(sp.SampleEncodeAsPieces("This is a test", nbest_size=-1, alpha=0.1))
```




# References


