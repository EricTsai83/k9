---
title: "Understand Subword Units for Natural Language Modeling"
subtitle: ""
author:
- name: Kyle Chung
  affiliation:
date: "`r format(Sys.time(), '%d %B %Y')` Last Updated"
output:
  html_notebook: 
    highlight: pygments
    number_sections: yes
    theme: paper
    toc: yes
    toc_depth: 4
    toc_float: yes
  code_download: true
bibliography: subword_units.bib
abstract: |
  TBC.
---
<!--For equation reference in Rmd.-->
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: { equationNumbers: { autoNumber: "AMS" } }
});
</script>

```{r setup, include=FALSE}
library(reticulate)
r <- try(use_python(Sys.getenv("PYTHON_PATH"), required=TRUE), silent=TRUE)
if ( is(r, "try-error") ) {
  r <- try(use_virtualenv(Sys.getenv("PYTHON_PATH"), required=TRUE), silent=TRUE)
  if ( is(r, "try-error") ) use_condaenv(Sys.getenv("PYTHON_PATH"), required=TRUE)
}
```

# Subword Units

Neural network models use a fixed size vocabulary to handle natrual language.
When the potentail vocabulary space is huge,
especially for a neural machine translation task,
there will be too many unknown words to a model.

To deal with such challenge,
(@sennrich2015neural) propose the idea to break up rare words into subword units for neural network modeling.
The segmentation approach is inspired by the [*byte pair encoding*](https://en.wikipedia.org/wiki/Byte_pair_encoding) compression algorithm,
or BPE in short.

## Byte Pair Encoding

BPE is a data compression algorithm that iteratively replaces the most frequent pair of bytes in a sequence with single unused byte.
By maintaining a mapping table of the new byte and the replaced old bytes,
we can recover the original message from a compressed representation by reversing the encoding process.

The same idea can be applied on charater-level instead of byte-level.

Do not consider paris across word boundaries.
Number of merge operations is the only hyperparameter for the algorithm.

The final symbol vocabulary size is equal to the size of the initial vocabulary,
plus the number of merge operations.


```{python bpe}
import re
from collections import defaultdict

class BPE:
  def __init__(self, sents, min_cnt=3):
    init_vocab = defaultdict(int)
    for sent in sents:
      words = re.split(r"\W+", sent)
      for w in words:
        if w != "":
          init_vocab[w] += 1
    # Create fullword vocabulary.
    self.word_vocab = {k: v for k, v in init_vocab.items() if v >= min_cnt}
    # Insert space between each char in a word for latter ease of merge operation.
    # We directly borrow the idea from https://www.aclweb.org/anthology/P16-1162.
    self.init_vocab = {" ".join(k): v for k, v in self.word_vocab.items()}
    self.subword_vocab = defaultdict(int)
    self.subword_pairs = defaultdict(int)
    
  def _find_top_subword(self):
    for w, cnt in self.init_vocab.items():
      subw = w.split()
      for i in range(len(subw) - 1):
        self.subword_pairs[subw[i], subw[i+1]] += cnt
    top_subw_pair = max(bpe.subword_pairs, key=bpe.subword_pairs.get)
    top_subw = "".join(top_subw_pair)
    self.subword_vocab[top_subw] = bpe.subword_pairs[top_subw_pair]
    return top_subw_pair

  def _merge(self, subw_pair):
    bigram = re.escape(" ".join(subw_pair))
    p = re.compile(r"(?<!\S)" + bigram + r"(?!\S)")
    self.init_vocab = {p.sub("".join(subw_pair), w): cnt for w, cnt in self.init_vocab.items()}
  
  def update_subword(self, n_merge=1):
    for n in range(n_merge):
      top_subw_pair = self._find_top_subword()
      self._merge(top_subw_pair)
```

Let's use Shakespeare as the input text example to run the BPE algorithm.
First we download the data from Google's public hosting service:

```{python}
import tensorflow as tf

shakes_file = tf.keras.utils.get_file(
  "shakespeare.txt",
  "https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt")
shakespeare = open(shakes_file, "rb").read().decode(encoding="utf-8")
shakespeare = text.lower().split("\n")

for sent in shakespeare[:20]:
  print(sent)
```

```{python bpe_shakespeare}
bpe =  BPE(shakespeare, min_cnt=50)
print(len(bpe.word_vocab))

# Print the first 10 subwords generated by the first 10 merge operations.
bpe.update_subword(n_merge=10)
for subw in bpe.subword_vocab:
  print(subw)
```



```{python}

```


# Subword Regularization

@kudo2018subword proposes a method called subword regularization.


# SentencePiece

@kudo2018sentencepiece

BPE encodes a sentence into a unique subword sequences.
A sequence can be represented in multiple subword sequences since even the same word can be segmented differently by subwords.

# References
