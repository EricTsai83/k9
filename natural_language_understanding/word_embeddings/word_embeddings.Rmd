---
title: "On Word Embeddings"
subtitle: "Vector Representation for Language Modeling"
author:
- name: Kyle Chung
  affiliation:
date: "`r format(Sys.time(), '%d %B %Y')` Last Updated"
output:
  html_notebook: 
    highlight: pygments
    number_sections: yes
    theme: paper
    toc: yes
    toc_depth: 3
    toc_float: yes
    includes:
      in_header: /tmp/meta_header.html
  code_download: true
bibliography: word_embeddings.bib
abstract: |
  TBC.
---
<!--For equation reference in Rmd.-->
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: { equationNumbers: { autoNumber: "AMS" } }
});
</script>

```{r meta, include=FALSE}
meta_header_file <- file("/tmp/meta_header.html")
meta <- c(
    '<meta name="author" content="Kyle Chung">',
    '<meta property="og:title" content="On Word Embeddings: Vector Representation for Language Modeling">',
    '<meta property="og:url" content="https://everdark.github.io/k9/natural_language_understanding/word_embeddings/word_embeddings.nb.html">',
    '<meta property="og:image" content="https://everdark.github.io/k9/assets/avatar.jpg">',
    '<meta property="og:description" content="A data science notebook about word embeddings for natural language modeling.">'
)
writeLines(meta, meta_header_file)
close(meta_header_file)
```

```{r setup, include=FALSE}
library(reticulate)
r <- try(use_python(Sys.getenv("PYTHON_PATH"), required=TRUE), silent=TRUE)
if ( is(r, "try-error") ) {
  r <- try(use_virtualenv(Sys.getenv("PYTHON_PATH"), required=TRUE), silent=TRUE)
  if ( is(r, "try-error") ) use_condaenv(Sys.getenv("PYTHON_PATH"), required=TRUE)
}
```

# The Motivation for Embeddings

# Context-Free Word Embeddings

## Word2vec

@mikolov2013distributed propose the famous `word2vec` model,
followed by a subsequent massive researches on how word embeddings can be learned with a variety of different algorithms.

`Word2vec` is considered a context-free embedding model since the embedding for a word (after learned) is the same no matter where the word appears.
There is a universal embedding lookup tables for each word in a pre-determined vocabulary.

### The Learning Objective

The core idea of learning `word2vec` is extremely simple:
to *train a predictive model that can predict the surrounding context of a word.*
There are two ways of construct such a predictive learning task:
the continous bag-of-words model and the skip-gram model.

#### Continuous Bag-of-Words {-}

CBOW predicts the target word from its context words.
Take the following text line for example:

```
all models are wrong but some are useful
```

Given a window size of 4,
we may like to predict the target word `wrong` by its context words `all models are`,
and also predict `useful` by `but same are`.

Indeed,
a moving window of size 4 will give us the following training examples:

```
[context] -> [target]

all models are -> wrong
models are wrong -> but
are wrong but -> some
wrong but some -> are
but some are -> useful
```

CBOW is known to have better results for smaller training corpus.
For large corpus the skip-gram model performs better.

#### Skip-Gram {-}

A skip-gram model does exactly the opposite of a CBOW.
It learns to predict the surrounding context words from a given target word.
So for the same example above,
assuming instead a window size of 1,
we will have the following training examples:

```
(window)
[target] -> [context]

(all models are)
  models -> all
  models -> are
(models are wrong)
  are -> models
  are -> wrong
(are wrong but)
  wrong -> are
  wrong -> but
(wrong but some)
  but -> wrong
  but -> some
(but some are)
  some -> but
  some -> are
(some are useful)
  are -> some
  are -> useful
```

In mathametical notation,
the objectvie function to maximize is:

$$
\frac{1}{T}\sum_{t=1}^T\sum_{-c\le j \le c; j \ne 0}\ln P(w_{t+j} \vert w_t),
$$

where $c$ is the skip-gram window size,
$T$ is the total number of words in the training corpus,
and $w_t$ ius the $t$-th word in the corpus.

Assuming the word embedding for $w_t$ is $v_t$,
the natural choice of estimator for $P(w_{t+j} \vert w_t)$ is the softmax using embedding dot-product:

$$
P(w_{t+j} \vert w_t) = \frac{\exp({v_{t+j}^Tv_t})}{\sum_{w \in W}\exp(v_w^Tv_t)},
$$

where $W$ is the vocabulary set.

### Negative Sampling

Essentially the learner is a supervised multi-class estimator,
even though we don't need to explicitly label our data.
The labels come directly from the training corpus once the context window is determined,
which is considered a hyperparameter of the model.

The model is learned by maximizing the likelihood of all the surrounding words within a context window.
Theoretically,
the loss function will be a softmax with number of class equal to vocabulary size.
In practice this is prohibitively costly to compute due to usually the fairly large size of vocabulary.

To overcome the issue,
a technique called negative sampling is used to approximate the original softmax probability over the entire vocabulary.
The idea is to *replace one softmax training example with multiple logistic regression training examples.*
The log-likelihood of a training pair hence becomes:

$$
\ln P(w_{t+j} \vert w_t) \approx
\underbrace{\ln\sigma(v_{t+j}^Tv_t) \vphantom{\sum_{i=1;w_i \sim P_n(W)}}}
_{\log P(Y = 1 \vert w_{t+j}, w_t)} +
\underbrace{\sum_{i=1;w_i \sim P_n(W)}^k\ln\sigma(-v_i^Tv_t)}
_{\sum_i \log P(Y = 0 \vert w_i, w_t)},
$$

where $\sigma$ is a standard sigmoid function,
$P_n(W)$ is a noise distribution and $k$ is the number of negative samples drawn from that distribution,
$Y$ is simply a dummy indicator of the correctness of the prediction.^[We use the fact that $\sigma(t) = \frac{1}{1 + e^{-t}} = \frac{e^t}{1 + e^t}$ and hence $1 - \sigma(t) = \frac{1}{1 + e^t} = \sigma(-t)$.]

Let's use our toy example to illustrate the idea of negative sampling further.
Assuming the text line

```
all models are wrong but some are useful
```

is our entire corpus and also our vocabulary set.
Now consider the following one training example in a skip-gram model:

```
are -> wrong
```

In a softmax setup,
our label is a one-hot encoded vector of the length equal to vocabulary size:

| target / context | all | models | are | wrong | but | some | are | useful |
|:----------------:|:---:|:------:|:---:|:-----:|:---:|:----:|:---:|:------:|
| are              | 0   | 0      | 0   |  1    | 0   | 0    | 0   | 0      |

For this single training example the model needs to learn that `wrong` is the right word to predict.

Now if we use a negative sampling with sample size $k = 2$,
the single softmax training example effectively expands into multiple binary training examples:

| target | context | y  |
|:------:|:-------:|:--:|
| are    | wrong   | 1  |
| are    | but     | 0  |
| are    | some    | 0  |

And the model now is going to learn that given the word `are` the probasbility of `wrong` should be high and the probability of `but` and `some` should be low.
(We assume `but` and `some` are the random draws from a noise distribution.)

For natural language modeling a classical choice of the noise distribution is the unigram distribution which can be estimated simply by vocabulary frequency counts.

The negative sampling loss is indeed a simplifed version of a more general loss called the Noise-Contrastive Estimation (NCE) loss,
proposed by @mnih2013learning for also learning a word embedding model.
Since practically there is little difference between the two approaches we will skip the discussion on NCE loss for simplicity.^[In a nutshell, negative sampling approximates $\ln \frac{P(y \vert x)}{Q(y \vert x)}$ while NCE approximates $\ln P(y \vert x)$. So NCE is asymptotically softmax but negative sampling is not. For the sake of learning word embeddings this is not a required condition since we don't use the model prediction for the downstream task at all; instead we extract the fitted embeddings of the entire vocabulary.]

### Dropout for Frequent Words

To counter the effect of imbalanced distribution among popular (and usually less informative) words.
@mikolov2013distributed also propose the idea of randomly discard training example based on a heuristic dropout probability:

$$
P(w_i) = 1 - \sqrt{\frac{t}{f(w_i)}},
$$

where $f(w_i) \in [0, 1]$ is the normalized frequency of word $w_i$ and $t$ is a hyperparameter around $10^{-5}$.
The idea is to drop in higher probability when a word is more frequent.

We can plot the function to see its property:

```{r word2vec_dropout_func}
# R
p <- function(fw, t=1e-5) {
  1 - sqrt(t / fw)
}

curve(p, from=0, to=1, n=5000, ylab="P(w)", xlab="f(w)",
      main="Dropout Probability based on Word Frequency")
```

If a word is dominant it is highly possible to be discarded.
But this is unlikely to happen for a meaningful corpus with a fair size.
Since the normalized word frequency is more likely to be around zero,
we can zoom in a bit:

```{r word2vec_dropout_func_zoom}
# R
options(scipen=999)
curve(p, from=1e-6, to=3e-4, n=5000,
      ylab="P(w)", xlab="f(w)",
      main="Dropout Probability based on Word Frequency")
abline(v=1e-5, col="red")
text(1e-5, -1, pos=4, col="red", bquote(t == 10^-5))
```

Note that the function does not return a valid probability in smaller $f(w_i)$.
Indeed $\lim_{f(w_i) \to 0} P(w_i) = -\infty$.
In case of a negative value we simply replace it with a zero:
no dropout.

The hyperparameter $t$ is chosen such that for words with frequency larger than $t$ the dropout start to apply,
increasing in frequency.

### A Tensorflow Implementation

The original implementation of `word2vec` is written in C and can be found [here](https://github.com/tmikolov/word2vec).^[macOS won't compile. The source code can be compiled only under Linux.]

In this section we will try to use `tensorflow` (@tensorflow2015-whitepaper) to implement a `word2vec` model.

```{python import_tf}
import logging
logging.getLogger("tensorflow").setLevel(logging.ERROR)

import tensorflow as tf
print(tf.__version__)
```

```{python}

```

## GloVe

@pennington2014glove

collect word co-occurrence within a window, forming a matrix $X$ where $X_{ij}$ is the co-occurence of word $i$ and word $j$.

$$
W_i^TW_j + b_i + b_j = \ln X_{ij},
$$

where $W_i$ is the embedding for word $i$,
$b_i$ is a bias term.
Similarly for word $j$.


### Efficient Implementation

Beside GloVe's [original implementation](https://github.com/stanfordnlp/GloVe),
in this section we also discuss some other efficient implementation but with a higher-level API at our disposal.

#### `text2vec`

[`text2vec`](https://github.com/dselivanov/text2vec)

```{r, text2vec_import, results="hide"}
library(text2vec)
```


```{r text2vec_demo}
text8_file = "~/text8"
if (!file.exists(text8_file)) {
  download.file("http://mattmahoney.net/dc/text8.zip", "~/text8.zip")
  unzip ("~/text8.zip", files = "text8", exdir = "~")
}
wiki <- readLines(text8_file, n=1, warn=FALSE)

# Create iterator over tokens
tokens <- space_tokenizer(wiki)
# Create vocabulary. Terms will be unigrams (simple words).
it <- itoken(tokens, progressbar = FALSE)
vocab <- create_vocabulary(it)

vocab <- prune_vocabulary(vocab, term_count_min=5L)

# Use our filtered vocabulary
vectorizer <- vocab_vectorizer(vocab)
# use window of 5 for context words
tcm <- create_tcm(it, vectorizer, skip_grams_window=5L)
dim(tcm)

glove <- GlobalVectors$new(word_vectors_size=50, vocabulary=vocab, x_max=10)
wv_main <- glove$fit_transform(tcm, n_iter=5, convergence_tol=.01)

dim(wv_main)
```




## FastText

@bojanowski2017enriching

Using subword emgeddings to overcome out-of-vocabulary issue.


# Context-Aware Word Embeddings

## BERT

@devlin2018bert

https://github.com/google-research/bert

### Attention

### Positional Encoding

## XLNet

@yang2019xlnet

https://github.com/zihangdai/xlnet

# Transfer Learning

## Pre-Trained Embeddings

### TensorFlow Hub

[TensorFlow Hub](https://www.tensorflow.org/hub) is a reository for sharing modules of pre-trained neural network embeddings not limited to natural language.
Both `word2vec` and `BERT` pre-trained embeddings are available under the [text embedding module](https://tfhub.dev/s?module-type=text-embedding).

[coding example of using tensorhub]

### Gensim

[coding example here]

# References
