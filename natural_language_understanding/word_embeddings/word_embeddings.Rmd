---
title: "On Word Embeddings"
subtitle: "Vector Representation for Natural Language"
author:
- name: Kyle Chung
  affiliation:
date: "`r format(Sys.time(), '%d %B %Y')` Last Updated"
output:
  html_notebook:
    highlight: pygments
    number_sections: yes
    theme: paper
    toc: yes
    toc_depth: 4
    toc_float: yes
    includes:
      in_header: /tmp/meta_header.html
  code_download: true
bibliography: word_embeddings.bib
abstract: |
  Word embeddings are the building blocks for neural network models that solve natural language understanding (NLU) tasks. In this notebook we review several influential models that are designed to learn word embeddings for other downstream machine learning application, a.k.a. transfer learning. We will cover both context-free embeddings (word2vec, GloVe, and fastText) and context-aware embeddings (BERT and XLNet). Especially the latter recently has become the state-of-the-art methodology for several important NLU applications.
---
<!--For equation reference in Rmd.-->
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: { equationNumbers: { autoNumber: "AMS" } }
});
</script>

```{r meta, include=FALSE}
meta_header_file <- file("/tmp/meta_header.html")
meta <- c(
    '<meta name="author" content="Kyle Chung">',
    '<meta property="og:title" content="On Word Embeddings: Vector Representation for Language Modeling">',
    '<meta property="og:url" content="https://everdark.github.io/k9/natural_language_understanding/word_embeddings/word_embeddings.nb.html">',
    '<meta property="og:image" content="https://everdark.github.io/k9/assets/avatar.jpg">',
    '<meta property="og:description" content="A data science notebook about word embeddings for natural language modeling.">'
)
writeLines(meta, meta_header_file)
close(meta_header_file)
```

```{r setup, include=FALSE}
library(reticulate)
r <- try(use_python(Sys.getenv("PYTHON_PATH"), required=TRUE), silent=TRUE)
if ( is(r, "try-error") ) {
  r <- try(use_virtualenv(Sys.getenv("PYTHON_PATH"), required=TRUE), silent=TRUE)
  if ( is(r, "try-error") ) use_condaenv(Sys.getenv("PYTHON_PATH"), required=TRUE)
}
```

# The Motivation for Embeddings

Generally speaking,
machine learning models handle only numerics.
Every piece of information must be represented by numbers,
in one way or another.
This is especially true for neural network models.

But not all features have their numerical representation by nature.
One notable example is natural language.
Words are not numbers and do not have a numerical interpretation on their own.
A classical way of converting words into numbers is through one-hot encoding.
So a word can be represented by a numerical row from a term-document sparse matrix given a corpus.

One-hot encoding has its limitation though.
As a baseline approach it works surprisingly well in lots of traditional machine learning tasks.
But it fails to deliver usable results when it comes to more complicated task such as machine translation,
question answering,
or other advanced natural language understanding (NLU) tasks.

Embeddings are vector representation of features that are otherwise non-numerical.
All weights in the vector are trainable parameters given a machine learning task.
Depending on the task itself,
the resulting learned embeddings can be informative in the vector space.

For example,
in factorization model for recommender systems,
users and items are represented by embeddings and their similarity can be calculated by calculating the dot-product or the cosine distance of the underlying vectors.

The same idea can be applies to natural language,
where each word is represented by its embeddings.
Word embeddings are the building blocks for modern neural network models that can successfully handle a variety of NLU tasks.
In this notebook we are going to review thoroughly several most popular unsupervised approaches for learning word embeddings.
The learned embeddings can then be used by a downstream machine learning model to solve further problems at an application level.
This is referred to as *transfer learning*.

# Context-Free Word Embeddings

If the embedding for the same word remain the same no matter where the word appears,
it is a context-free word embedding.
There will be a universal embedding lookup tables for each word in a pre-determined vocabulary.

There are 3 popular models to learn context-free word embeddings:

+ Word2vec
+ GloVe
+ FastText

All three approaches are unsupervised in the sense that the labels come directly from the training corpus so there is no extra effort for data labeling.
But inherently they all apply supuervised learning algorithms,
which we will see clearly in the following sections one by one.

## Word2vec

@mikolov2013distributed propose the famous `word2vec` model,
followed by a subsequent massive researches on how word embeddings can be learned with a variety of different algorithms.

Though it is no longer the most popular choice of pre-trained word embeddings after years of evolution,
its concept remain solid and it is very helpful to understand what the model is trying to learn,
so we can lay down the foundation for natural language modeling.

### The Learning Objective

The core idea of learning `word2vec` is extremely simple:
to *train a predictive model that can predict the surrounding context of a word.*
There are two ways of construct such a predictive learning task:
the continous bag-of-words model and the skip-gram model.

#### Continuous Bag-of-Words {-}

CBOW predicts the target word from its context words.
Take the following text line for example:

```
all models are wrong but some are useful
```

Given a window size of 4,
we may like to predict the target word `wrong` by its context words `all models are`,
and also predict `useful` by `but same are`.

Indeed,
a moving window of size 4 will give us the following training examples:

```
[context] -> [target]

all models are -> wrong
models are wrong -> but
are wrong but -> some
wrong but some -> are
but some are -> useful
```

CBOW is known to have better results for smaller training corpus.
For large corpus the skip-gram model performs better.

#### Skip-Gram {-}

A skip-gram model does exactly the opposite of a CBOW.
It learns to predict the surrounding context words from a given target word.
So for the same example above,
assuming instead a window size of 1,
we will have the following training examples:

```
(window)
[target] -> [context]

(all models are)
  models -> all
  models -> are
(models are wrong)
  are -> models
  are -> wrong
(are wrong but)
  wrong -> are
  wrong -> but
(wrong but some)
  but -> wrong
  but -> some
(but some are)
  some -> but
  some -> are
(some are useful)
  are -> some
  are -> useful
```

In mathametical notation,
the objectvie function to maximize is:

$$
\frac{1}{T}\sum_{t=1}^T\sum_{-c\le j \le c; j \ne 0}\ln P(w_{t+j} \vert w_t),
$$

where $c$ is the skip-gram window size,
$T$ is the total number of words in the training corpus,
and $w_t$ ius the $t$-th word in the corpus.

Assuming the word embedding for $w_t$ is $v_t$,
the natural choice of estimator for $P(w_{t+j} \vert w_t)$ is the softmax using embedding dot-product:

$$
P(w_{t+j} \vert w_t) = \frac{\exp({v_{t+j}^Tv_t})}{\sum_{w \in W}\exp(v_w^Tv_t)},
$$

where $W$ is the vocabulary set.

### Negative Sampling

Essentially the learner is a supervised multi-class estimator,
even though we don't need to explicitly label our data.
The labels come directly from the training corpus once the context window is determined,
which is considered a hyperparameter of the model.

The model is learned by maximizing the likelihood of all the surrounding words within a context window.
Theoretically,
the loss function will be a softmax with number of class equal to vocabulary size.
In practice this is prohibitively costly to compute due to usually the fairly large size of vocabulary.

To overcome the issue,
a technique called negative sampling is used to approximate the original softmax probability over the entire vocabulary.
The idea is to *replace one softmax training example with multiple logistic regression training examples.*
The log-likelihood of a training pair hence becomes:

$$
\ln P(w_{t+j} \vert w_t) \approx
\underbrace{\ln\sigma(v_{t+j}^Tv_t) \vphantom{\sum_{i=1;w_i \sim P_n(W)}}}
_{\log P(Y = 1 \vert w_{t+j}, w_t)} +
\underbrace{\sum_{i=1;w_i \sim P_n(W)}^k\ln\sigma(-v_i^Tv_t)}
_{\sum_i \log P(Y = 0 \vert w_i, w_t)},
$$

where $\sigma$ is a standard sigmoid function,
$P_n(W)$ is a noise distribution and $k$ is the number of negative samples drawn from that distribution,
$Y$ is simply a dummy indicator of the correctness of the prediction.^[We use the fact that $\sigma(t) = \frac{1}{1 + e^{-t}} = \frac{e^t}{1 + e^t}$ and hence $1 - \sigma(t) = \frac{1}{1 + e^t} = \sigma(-t)$.]

Let's use our toy example to illustrate the idea of negative sampling further.
Assuming the text line

```
all models are wrong but some are useful
```

is our entire corpus and also our vocabulary set.
Now consider the following one training example in a skip-gram model:

```
are -> wrong
```

In a softmax setup,
our label is a one-hot encoded vector of the length equal to vocabulary size:

| target / context | all | models | are | wrong | but | some | are | useful |
|:----------------:|:---:|:------:|:---:|:-----:|:---:|:----:|:---:|:------:|
| are              | 0   | 0      | 0   |  1    | 0   | 0    | 0   | 0      |

For this single training example the model needs to learn that `wrong` is the right word to predict.

Now if we use a negative sampling with sample size $k = 2$,
the single softmax training example effectively expands into multiple binary training examples:

| target | context | y  |
|:------:|:-------:|:--:|
| are    | wrong   | 1  |
| are    | but     | 0  |
| are    | some    | 0  |

And the model now is going to learn that given the word `are` the probasbility of `wrong` should be high and the probability of `but` and `some` should be low.
(We assume `but` and `some` are the random draws from a noise distribution.)

For natural language modeling a classical choice of the noise distribution is the unigram distribution which can be estimated simply by vocabulary frequency counts.
In the original paper of `word2vec` the unigram distribution raised to the 3/4rd power seems to perform better in their evaluation setup.
It is actually also common in practice to use a simple uniform distribution from the vocabulary.

The negative sampling loss is indeed a simplifed version of a more general loss called the Noise-Contrastive Estimation (NCE) loss,
proposed by @mnih2013learning for also learning a word embedding model.
Since practically there is little difference between the two approaches we will skip the discussion on NCE loss for simplicity.^[In a nutshell, negative sampling approximates $\ln \frac{P(y \vert x)}{Q(y \vert x)}$ while NCE approximates $\ln P(y \vert x)$. $Q(\cdot)$ is the noise distribution. So NCE is asymptotically softmax but negative sampling is not. For the sake of learning word embeddings this is not a required condition since we don't use the model prediction for the downstream task at all; instead we extract the fitted embeddings of the entire vocabulary.]

### Dropout for Frequent Words

To counter the effect of imbalanced distribution among popular (and usually less informative) words.
@mikolov2013distributed also propose the idea of randomly discard training example based on a heuristic dropout probability:

$$
\begin{equation} \label{eq:dropout}
P(w_i) = 1 - \sqrt{\frac{t}{f(w_i)}},
\end{equation}
$$

where $f(w_i)$ can be any scaled frequency of word $w_i$ and $t$ is a hyperparameter around $10^{-5}$.
The idea is to drop in higher probability when a word is more frequent.

We can plot the function to see its property (Assuming a normalized frequency $f(w_i) \in [0, 1]$):

```{r word2vec_dropout_func}
# R
p <- function(fw, t=1e-5) {
  1 - sqrt(t / fw)
}

curve(p, from=0, to=1, n=5000, ylab="P(w)", xlab="f(w)",
      main="Dropout Probability based on Word Frequency")
```

If a word is dominant it is highly possible to be discarded.
But this is unlikely to happen for a meaningful corpus with a fair size.
Since the normalized word frequency is more likely to be around zero,
we can zoom in a bit:

```{r word2vec_dropout_func_zoom}
# R
options(scipen=999)
curve(p, from=1e-6, to=3e-4, n=5000,
      ylab="P(w)", xlab="f(w)",
      main="Dropout Probability based on Word Frequency")
abline(v=1e-5, col="red")
text(1e-5, -1, pos=4, col="red", bquote(t == 10^-5))
```

Note that the function does not return a valid probability in smaller $f(w_i)$.
Indeed $\lim_{f(w_i) \to 0} P(w_i) = -\infty$.
In case of a negative value we simply replace it with a zero:
no dropout.

The hyperparameter $t$ is chosen such that for words with frequency larger than $t$ the dropout start to apply,
increasing in frequency.

### A TensorFlow Implementation

The original implementation of `word2vec` is written in C and can be found [here](https://github.com/tmikolov/word2vec).
Though the program is highly efficient,
it is not very flexible to use and has platform compatibility issue.^[Specifically, as of August 2019, the source code can be compiled only under Linux. macOS won't compile.]

In this section,
instead,
we will try to use `tensorflow` (@tensorflow2015-whitepaper) to implement an end-to-end example of a skip-gram `word2vec` model.

```{python import_tf}
import logging
logging.getLogger("tensorflow").setLevel(logging.ERROR)

import tensorflow as tf
print(tf.__version__)
```

#### Subword Segmentation {-}

Lets' use Shakespeare as our corpus.

```{bash mkdir}
mkdir -p data
```

```{python word2vec_dl_shakespeare}
import os
import shutil

shakes_file = "data/shakespeare.txt"
if not os.path.exists(shakes_file):
  shakes_dl_path = tf.keras.utils.get_file(
    "shakespeare.txt",
    "https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt")
  shutil.move(shakes_dl_path, shakes_file)
shakespeare = open(shakes_file, "rb").read().decode(encoding="utf-8")
shakespeare = shakespeare.split("\n")

# Print the first few lines.
for sent in shakespeare[:20]:
  print(sent)
```

To prepare our vocabulary,
we use `sentencepiece` (@kudo2018sentencepiece) to learn subwords from Shakespeare.^[For more details on subword unit segmentation, reader can refer to the [notebook: On Subword Units](https://everdark.github.io/k9/natural_language_understanding/subword_units/subword_units.nb.html).]

```{python word2vec_spm, results="hide"}
import sentencepiece as spm

spm_args = "--input=data/shakespeare.txt"
spm_args += " --model_prefix=m"
spm_args += " --vocab_size=5000"
spm_args += " --model_type=unigram"
spm.SentencePieceTrainer.Train(spm_args)

sp = spm.SentencePieceProcessor()
sp.Load("m.model")
```

With the pre-trained vocabulary we can easily encode our entire corpus into sequence of integers:

```{python word2vec_encode_subword}
shakespeare_wids = [sp.EncodeAsIds(line) for line in shakespeare]

# Test a line.
test_ids = shakespeare_wids[213]
print(test_ids)
print(sp.DecodeIds(test_ids))
```

```{python word2vec_check_spm}
# Double check.
for i in test_ids:
  print("{:5} -> {}".format(i, sp.IdToPiece(i)))
```

#### Skip-Gram Conversion {-}

In the `keras` module we have a convenience function `skipgrams` to generate our training data from word indices.
Here is its usage:

```{python word2vec_keras_skipgrams}
from tensorflow.keras.preprocessing.sequence import skipgrams

# We don't shuffle the result here in order to easily observe the parsing result.
# Fro acutal application shuffle is a must since otherwise it will bias the SGD optimizer.
pairs, labels = skipgrams(
  test_ids, vocabulary_size=len(sp),
  window_size=1, negative_samples=1,
  shuffle=False, seed=777,
  sampling_table=None)

# Inspect the parsed training examples.
# We discard the meta char <U+2581> for a cleaner output.
sep = "\n----------------------------"
for i, (x, y) in enumerate(zip(pairs, labels)):
  if i == 0:
    print("{:6} -> {:12} | {}{}".format("target", "context", "label", sep))
  target, context = [sp.IdToPiece(i).replace("\u2581", "") for i in x]
  print("{:6} -> {:12} | {}".format(target, context, y))
```

The `sampling_table` argumwent can be used to implement the dropout for frequent words.
Indeed `keras.preprocessing.sequence` module also comes with a convenience function exactly for this purpose:

```{python word2vec_keras_droput}
from tensorflow.keras.preprocessing.sequence import make_sampling_table

# To use this function we need to make sure our vocabulary is indexed by frequency.
# That is, more frequent words come first.
# SentencePiece does exactly this so we don't need to do any further post-processing.

# We set a higher threshold for dropput since our vocabulary size is rather small.
sampling_table = make_sampling_table(len(sp), sampling_factor=2e-4)
```

The formula follows exactly equation $\eqref{eq:dropout}$,
with the word frequency approximated by a [Zipf's law](https://en.wikipedia.org/wiki/Zipf%27s_law) using the frequency rank.

$$
f(w_r) \sim \bigg(w_r \cdot (\log w_r + \gamma) + \frac{1}{2} - \frac{1}{12w_r}\bigg)^{-1},
$$

where $w_r$ is the frequency rank of word $w$,
$\gamma$ is the [Euler-Mascheroni constant](https://en.wikipedia.org/wiki/Euler%E2%80%93Mascheroni_constant).

In this manner the frequency only depends on the ranks but not the exact counts of words in vocabulary.

Let's examine the resulting dropout probabilities given our threshold (`sampling_factor`):

```{r woprd2vec_keras_dropout_plot}
# R
plot(1 - py$sampling_table, type="l",
     xlab="Subword Index", ylab="Dropout Probability",
     main="Zipf's Law Dropout Probabilities (Keras API)",
     sub=bquote(sampling_factor == 2e-4))
```

We can verify the word-rank-frequency conversion by manually compute the approximation:

```{r wrod2vec_zipflaw_freq}
# R
g <- 0.5772156649  # Euler-Mascheroni constant.

# Zipf's Law approximation from rank to frequency.
rank_to_freq <- function(r) {
  1/(r * (log(r) + g) + .5 - 1/(12 * r))
}

freq_to_drop <- function(fw, t=2e-4) {
  pmax(0, 1 - sqrt(t / fw))
}

plot(freq_to_drop(rank_to_freq(1:5000)), type="l",
     main="Zipf's Law Dropout Probabilities (Verification)",
     sub=bquote(sampling_factor == 2e-4))
```

How about the specification for the noise distribution for negative sampling?
Based on the [source code](https://github.com/keras-team/keras-preprocessing/blob/master/keras_preprocessing/sequence.py) of `sequence.skipgrams` in the `keras.preprocessing` module,
it only performs a uniform distribution random draw from the vocabulary as the negative sample.
For simplicity we will keep it as-is.

#### Data Pipeline {-}

Now let's implement the end-to-end data pipeline from raw text to trainable skip-gram pairs.
Even though previously we already encode the entire corpus into list of sequence of subword indices,
we can simply fit the entire training data in memory,
here instead we demonstrate a more production-ready approach with a data iterator via `tf.data.Dataset` module.

```{python word2vec_tf_data_pipeline}
vocab_size = len(sp)
batch_size = 128
sampling_table = make_sampling_table(vocab_size, sampling_factor=5e-3)  # Adjust dropout.


def parse_line(text_tensor):
  """Convert a raw text line (in tensor) into skp-gram training examples."""
  ids = sp.EncodeAsIds(text_tensor.numpy())
  pairs, labels = skipgrams(
    ids, vocabulary_size=vocab_size,
    window_size=2, negative_samples=5,
    shuffle=True, sampling_table=sampling_table
  )
  targets, contexts = list(zip(*pairs))
  return targets, contexts, labels


# Since each text line can result in different number of training pairs,
# we need to use flat_map to flatten the parsed results before batching.
def parse_line_map_fn(text_tensor):
  targets, contexts, labels = tf.py_function(
    parse_line, inp=[text_tensor], Tout=[tf.int64, tf.int64, tf.int64])
  return tf.data.Dataset.from_tensor_slices(
    ({"targets": targets, "contexts": contexts}, labels))


# For simplicity we drop text lines that are too short.
dataset = tf.data.TextLineDataset(shakes_file)
dataset = dataset.filter(lambda line: tf.greater(tf.strings.length(line), 10))
dataset = dataset.flat_map(parse_line_map_fn)
dataset = dataset.shuffle(buffer_size=1000).repeat(count=None)
dataset = dataset.batch(batch_size, drop_remainder=True)
dataset = dataset.prefetch(batch_size)


# Test.
for x, y in dataset:
  tf.print(x)
  tf.print(y)
  break
```

#### Model Architecture {-}

The entire `word2vec` model is nothing more than a shallow neural network with embedding lookups,
dot-products and cross entropy loss.
It is highly similar to a matrix factorization problem with a log-loss.
The main difference lies in how we prepare the training examples.^[For more on the matrix factorization problem, especially for application in recommender systems, one can refer to the [notebook: Matrix Factorization for Recommender Systems](https://everdark.github.io/k9/matrix_factorization/matrix_factorization.nb.html).]

```{python word2vec_tf_model}
# Dimension of embeddings. Typically 128 to 512.
k = 128

# We separate target and context word indices as two input layers.
input_targets = tf.keras.layers.Input(shape=(1,), name="targets")
input_contexts = tf.keras.layers.Input(shape=(1,), name="contexts")

# Word embeddings are looked up separately for target and context words.
embeddings = tf.keras.layers.Embedding(vocab_size, k, name="word_embeddings")
target_embeddings = embeddings(input_targets)
context_embeddings = embeddings(input_contexts)

# Dot-product of the target and context word embeddings with sigmoid activation.
dots = tf.keras.layers.Dot(axes=-1, name="logits")([target_embeddings, context_embeddings])
sigmoid = tf.keras.layers.Dense(1, activation="sigmoid", name="sigmoid")
outputs = sigmoid(dots)

model = tf.keras.Model(inputs=[input_targets, input_contexts], outputs=outputs, name="word2vec")
model.compile(loss="binary_crossentropy", optimizer="sgd")

print(model.summary())
```

```{python word2vec_tf_model_train}
model.fit(dataset, epochs=5, steps_per_epoch=1000, verbose=0)
```

```{r word2vec_tf_model_loss}
# R
plot(unlist(py$model$history$history["loss"]), pch="X", type="o",
     xlab="Epoch", ylab="Loss", main="Word2vec Model Loss Trace")
```

As one can see,
for such a the small corpus the model quickly converge.

```{python word2vec_tf_trained_vectors}
word_vectors = model.get_layer("word_embeddings").weights[0].numpy()
print(word_vectors)
```

```{python word2vec_cosine_sim}
import numpy as np
from sklearn.metrics.pairwise import cosine_similarity

ws_meta = "\u2581"
i = sp.PieceToId(ws_meta + "love")

scores = cosine_similarity(word_vectors[i,np.newaxis], word_vectors)
scores = np.squeeze(scores)

top_k = 10
sim_ind = np.squeeze(scores).argsort()[-top_k:][::-1]

for i, s in zip(sim_ind, scores[sim_ind]):
  print("{:10} | {}".format(sp.IdToPiece(int(i)).replace(ws_meta, ""), s))
```

## Word2Bits

Based on the `word2vec` approach,
1@lam2018word2bits propose an enhancement to considerably reduce the amount of vector space required for the embeddings while retain the same or even improve the quality in terms of several evaluation tests.
The enhanced model is referred to as `Word2Bits`.

## GloVe

GloVe stands for "Global Vectors" for word representation,
proposed by @pennington2014glove as another unsupervised approach to train word embeddings from a large corpus.
The idea is to combine a global matrix factorization that utilize the co-occurence statistics with a local context window method that utlizes the target-context predictive relatedness.
Based on a given context window size,
we first establish a word-word co-occurence matrix,
then factiorize the sparse matrix by training on only the nonzero elements.^[[Latent Semantic Analysis (LSA)](https://en.wikipedia.org/wiki/Latent_semantic_analysis) also uses matrix factorization but on a term-document matrix.]

### The learning objective

Let's denote the co-occurence matrix as $X$,
a $N$ by $N$ square matrix for a vocabulary of size $N$.
$X_{ij}$ is the co-occurence of word $i$ and word $j$.
More specifically,
$X_{ij}$ is the number of times word $j$ occurs in the context of word $i$.
Then we have $P(j|i) = \frac{X_{ij}}{X_i}$ as the conditional probability of context word $j$ given target word $i$,
where $X_i = \sum_{j=1}^N X_{ij}$.

The log-cooccurrence is then modeled by a factorization:

$$
\ln X_{ij} = W_i^TW_j + b_i + b_j,
$$

where $W_i$ is the embedding and $b_i$ the bias term for word $i$.

Now the task is to minimize the sum of least squares:

$$
\operatorname*{arg\,min}_{W, b} \sum_i\sum_j\big( W_i^TW_j + b_i + b_j - \ln X_{ij} \big)^2.
$$

To deal with distribution imbalance of co-occurrence,
a weighting function is further introduced such that

$$
\operatorname*{arg\,min}_{W, b} \sum_i\sum_j f(X_{ij})\big( W_i^TW_j + b_i + b_j - \ln X_{ij} \big)^2,
$$

where

$$
f(X_{ij}) = f(x) =
\left\{
  \begin{array}{ll}
    \Big(\frac{x}{x_{max}}\Big)^\alpha & \mbox{if } x < x_{max} \\
    1 & \mbox{otherwise}.
  \end{array}
\right.
$$

There are two hyperparameters:
$x_{max}$ and $alpha$.
In the original paper the setting is $x_{max} = 100$ and $alpha = \frac{3}{4}$.

Note that by explicitly introducing the above weighting function all the zero entries in the co-occurence matrix are effectively discarded during the training since we (on purpose) have $f(0) = 0$.

Essentially GloVe is a factorization model on the log of context co-occurence matrix with a weighted least-squares cost function.
The model can be trained by a stochastic gradient descent optimizer on random samples of non-zero elements in the co-occurrence matrix.

The model can also be interpreted as using embedding dot-product to approximate log of co-occurrence probability:

$$
\begin{aligned}
W_i^TW_j
&= \ln P_{ij} \\
&= \ln \frac{X_{ij}}{X_i} \\
&= \ln X_{ij} - \ln X_i.
\end{aligned}
$$

By imposing the constraint that $W_i^TW_j = W_j^TW_i$ for target-context inter-exchangability,
however,
we need to rewrite the above equation to be:

$$
W_i^TW_j = \ln X_{ij} - \ln X_i - \ln X_j.
$$

Now the term $\ln X_i$ and $\ln X_j$ correspond to the bias terms in the factorization model.

### A TensorFlow Implementation

Similar to `word2vec`,
[GloVe's original implementation](https://github.com/stanfordnlp/GloVe) is written in C.
Let's also try to implement the model with `tensorflow`.

The code will be highly similar to that of our implementation for `word2vec`.
The major difference is indeed how we create the data pipeline for our training triplet $(i, j, X_{ij})$.

#### Data Pipeline {-}

[TODO: implement generator for cooccurence matrix.]

```{python glove_tf_data_pipeline}
vocab_size = len(sp)
batch_size = 128


def parse_line(text_tensor):
  """Convert a raw text line (in tensor) into skp-gram training examples."""
  ids = sp.EncodeAsIds(text_tensor.numpy())
  pairs, labels = skipgrams(
    ids, vocabulary_size=vocab_size,
    window_size=2, negative_samples=5,
    shuffle=True, sampling_table=sampling_table
  )
  targets, contexts = list(zip(*pairs))
  return targets, contexts, labels


# Since each text line can result in different number of training pairs,
# we need to use flat_map to flatten the parsed results before batching.
def parse_line_map_fn(text_tensor):
  targets, contexts, labels = tf.py_function(
    parse_line, inp=[text_tensor], Tout=[tf.int64, tf.int64, tf.int64])
  return tf.data.Dataset.from_tensor_slices(
    ({"targets": targets, "contexts": contexts}, labels))


# For simplicity we drop text lines that are too short.
dataset = tf.data.TextLineDataset(shakes_file)
dataset = dataset.filter(lambda line: tf.greater(tf.strings.length(line), 10))
dataset = dataset.flat_map(parse_line_map_fn)
dataset = dataset.shuffle(buffer_size=1000).repeat(count=None)
dataset = dataset.batch(batch_size, drop_remainder=True)
dataset = dataset.prefetch(batch_size)


# Test.
for x, y in dataset:
  tf.print(x)
  tf.print(y)
  break
```

#### Model Architecture {-}

```{python glove_tf_model}
# Dimension of embeddings. Typically 128 to 512.
k = 64

# We separate target and context word indices as two input layers.
input_targets = tf.keras.layers.Input(shape=(1,), name="targets")
input_contexts = tf.keras.layers.Input(shape=(1,), name="contexts")

# Two set of embeddings are leared: target and context word embeddings.
# Theoretically they are the same thing since the problem is symmetric.
# But empirically by learning two separate sets of embeddings the results are better.
embeddings_targets = tf.keras.layers.Embedding(vocab_size, k, name="target_embeddings")
embeddings_contexts = tf.keras.layers.Embedding(vocab_size, k, name="context_embeddings")
target_embed = embeddings_targets(input_targets)
context_embed = embeddings_contexts(input_contexts)

# Dot-product of the target and context word embeddings with sigmoid activation.
dots = tf.keras.layers.Dot(axes=-1, name="logits")([target_embeddings, context_embeddings])
sigmoid = tf.keras.layers.Dense(1, activation="sigmoid", name="sigmoid")
outputs = sigmoid(dots)

model = tf.keras.Model(inputs=[input_targets, input_contexts], outputs=outputs, name="word2vec")
model.compile(loss="binary_crossentropy", optimizer="sgd")

print(model.summary())
```


### `text2vec`

[`text2vec`](https://github.com/dselivanov/text2vec) (@dmitriy2018text2vec) is a high-level R package with efficient C++ implementation of GloVe.
The APIs are elegantly designed and easy to use.
In this section we will spend some time exploring its [GloVe-related APIs](http://text2vec.org/glove.html).

Do note that all code chunks in this section is in the R language.

```{r, text2vec_import}
library(text2vec)
packageVersion("text2vec")
```

#### Create Co-Occurrence Matrix {-}

Here is a toy example of how we can use `text2vec::create_tcm` to generate a co-occurrence matrix from raw text:

```{r glove_text2vec_tcm}
text_line <- "all models are wrong but some are useful"

# Tokenize.
tokens <- space_tokenizer(text_line)
it <- itoken(tokens, progressbar=FALSE)  # A memory-efficient iterator.

# Build vocabulary.
vocab <- create_vocabulary(it)

# Build vectorizer based on vocabulary
vectorizer <- vocab_vectorizer(vocab)

# Create co-occurrence matrix using vectorizer and token iterator.
tcm <- create_tcm(it, vectorizer, skip_grams_window=1)

# A sparse matrix.
print(tcm)
```

Note that when using `skip_grams_window_context="symmetry"` (the default option) the matrix is established such that co-occurrence of $(i, j)$ equals co-occurrence of $(j, i)$,
but with the later entry masked to save space in the sparse matrix.
That is,
for example,
the entry `tcm["models", "all"]` can be interpreted as `tcm["all", "model"]` (if you are my context word then I'm also your context word) but the actual position in the latter entry holds zero count.
Technically,
the resulting matrix are always zero in the lower triangle,
and theoretically symmetric to its upper triangle if the corresponding values are not masked.

Without the storage saving trick,
the co-occurrence matrix would look like:

```{r glove_text2vec_tcm_check}
dtcm <- as.matrix(tcm)
tcm[lower.tri(tcm)] <- t(dtcm)[lower.tri(t(dtcm))]
print(tcm)
```

This reflects what the original GloVe paper mentioned:

>...for word-word co-occurrence matrices,
the distinction between a word and a context word is arbitrary and that we are free to exchange the two roles.

#### Decayed Context {-}

Note that for context window size > 1,
the GloVe model assign a decaying weight $\frac{1}{\mbox{distance}}$ to context word farer from the target word in the co-occurence matrix.
Here is an example of window size = 2:

```{r glove_text2vec_decayed_context}
(create_tcm(it, vectorizer, skip_grams_window=2))
```

#### Train GloVe {-}

```{r text2vec_demo}
text8_file = "~/text8"
if (!file.exists(text8_file)) {
  download.file("http://mattmahoney.net/dc/text8.zip", "~/text8.zip")
  unzip ("~/text8.zip", files = "text8", exdir = "~")
}
wiki <- readLines(text8_file, n=1, warn=FALSE)

tokens <- space_tokenizer(wiki)
it <- itoken(tokens, progressbar=FALSE)
vocab <- create_vocabulary(it)
vocab <- prune_vocabulary(vocab, term_count_min=5)
vectorizer <- vocab_vectorizer(vocab)
tcm <- create_tcm(it, vectorizer, skip_grams_window=2)
dim(tcm)

glove <- GlobalVectors$new(word_vectors_size=64, vocabulary=vocab, x_max=100, alpha=3/4)
target_embeddings <- glove$fit_transform(tcm, n_iter=5)

# Merge target and context embeddings as the final embeddings.
glove_embeddings <- target_embeddings + t(glove$components)
```

## FastText

@bojanowski2017enriching

Using subword emgeddings to overcome out-of-vocabulary issue.


# Context-Aware Word Embeddings

## BERT

@devlin2018bert

https://github.com/google-research/bert

### Attention

### Positional Encoding

## XLNet

@yang2019xlnet

https://github.com/zihangdai/xlnet

# Transfer Learning

## Pre-Trained Embeddings

### TensorFlow Hub

[TensorFlow Hub](https://www.tensorflow.org/hub) is a reository for sharing modules of pre-trained neural network embeddings not limited to natural language.
Both `word2vec` and `BERT` pre-trained embeddings are available under the [text embedding module](https://tfhub.dev/s?module-type=text-embedding).

[coding example of using tensorhub]

### Gensim

[coding example here]

# References
