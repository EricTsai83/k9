---
title: "On Word Embeddings"
subtitle: "Vector Representation for Natural Language"
author:
- name: Kyle Chung
  affiliation:
date: "`r format(Sys.time(), '%d %B %Y')` Last Updated"
output:
  html_notebook:
    highlight: pygments
    number_sections: yes
    theme: paper
    toc: yes
    toc_depth: 4
    toc_float: yes
    includes:
      in_header: /tmp/meta_header.html
  code_download: true
bibliography: word_embeddings.bib
abstract: |
  Word embeddings are the building blocks for neural network models that solve natural language understanding (NLU) tasks. In this notebook we review in details several influential models that are designed to learn word embeddings for other downstream machine learning application, a.k.a. transfer learning. We will cover both context-free embeddings (word2vec, GloVe, and fastText) and context-aware embeddings (BERT and XLNet). Especially the latter recently has become the state-of-the-art methodology for several important NLU applications.
---
<!--For equation reference in Rmd.-->
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: { equationNumbers: { autoNumber: "AMS" } }
});
</script>

<!--For controling code folding by chunk.-->
<script src="../../site_libs/utils/hide_output.js"></script>

```{r meta, include=FALSE}
meta_header_file <- file("/tmp/meta_header.html")
meta <- c(
    '<meta name="author" content="Kyle Chung">',
    '<meta property="og:title" content="On Word Embeddings: Vector Representation for Language Modeling">',
    '<meta property="og:url" content="https://everdark.github.io/k9/natural_language_understanding/word_embeddings/word_embeddings.nb.html">',
    '<meta property="og:image" content="https://everdark.github.io/k9/assets/avatar.jpg">',
    '<meta property="og:description" content="A data science notebook about word embeddings for natural language modeling.">'
)
writeLines(meta, meta_header_file)
close(meta_header_file)
```

```{r setup, include=FALSE}
library(reticulate)
r <- try(use_python(Sys.getenv("PYTHON_PATH"), required=TRUE), silent=TRUE)
if ( is(r, "try-error") ) {
  r <- try(use_virtualenv(Sys.getenv("PYTHON_PATH"), required=TRUE), silent=TRUE)
  if ( is(r, "try-error") ) use_condaenv(Sys.getenv("PYTHON_PATH"), required=TRUE)
}
```

# The Motivation for Embeddings

Generally speaking,
machine learning models handle only numerics.
Every piece of information must be represented by numbers,
in one way or another.
This is especially true for neural network models.

But not all features have their numerical representation by nature.
One notable example is natural language.
Words are not numbers and do not have a numerical interpretation on their own.
A classical way of converting words into numbers is through one-hot encoding.
So a word can be represented by a numerical row from a term-document sparse matrix given a corpus.

One-hot encoding has its limitation though.
As a baseline approach it works surprisingly well in lots of traditional machine learning tasks.
But it fails to deliver usable results when it comes to more complicated tasks such as machine translation,
question answering,
or other advanced natural language understanding (NLU) tasks.

Embeddings are vector representation of features that are otherwise non-numerical.
All weights in the vector are trainable parameters given a machine learning task.
Depending on the task itself,
the resulting learned embeddings can be informative in the vector space.

For example,
in factorization model for recommender systems,
users and items are represented by embeddings and their similarity can be calculated by calculating the dot-product or the cosine distance of the underlying vectors.

The same idea can be applied to natural language,
where each word is represented by its embeddings.
Word embeddings are the building blocks for modern neural network models that can successfully handle a variety of NLU tasks.
In this notebook we are going to review thoroughly several most popular unsupervised approaches for learning word embeddings.
The learned embeddings can then be used by a downstream machine learning model to solve further problems at an application level.
This is referred to as *transfer learning*.

# Context-Free Word Embeddings

If the embedding for the same word remain the same no matter where the word appears,
it is a context-free word embedding.
There will be a universal embedding lookup tables for each word in a pre-determined vocabulary.

There are 3 popular models to learn context-free word embeddings:

+ Word2vec
+ GloVe
+ FastText

All three approaches are unsupervised in the sense that the labels come directly from the training corpus so there is no extra effort for data labeling.
But inherently they all apply supervised learning algorithms,
which we will see clearly in the following sections one by one.

## Word2vec

@mikolov2013distributed propose the famous `word2vec` model,
followed by a subsequent massive researches in the literature on how word embeddings can be learned with a variety of different algorithms.
Though it is no longer the most popular choice of pre-trained word embeddings after years of evolution,
its concept remain solid and it is very helpful to understand what the model is trying to learn,
so we can lay down the foundation for natural language modeling.

### The Learning Objective

The core idea of learning `word2vec` is extremely simple:
to *train a predictive model that can predict the surrounding context of a word.*
There are two ways of construct such a predictive learning task:
the continous bag-of-words model and the skip-gram model.

#### Continuous Bag-of-Words {-}

CBOW predicts the target word from its context words.
Take the following text line for example:

```
all models are wrong but some are useful
```

Given a window size of 2,
we may like to predict the target word `wrong` by its context (surrounding) words `models are` and also `but some`.
A context window of size 1 instead will give us the following training examples:

```
(window)
[target] -> [context]

(all models are)
  all -> models
  are -> models
(models are wrong)
  models -> are
  wrong -> are
(are wrong but)
  are -> wrong
  but -> wrong
(wrong but some)
  wrong -> but
  some -> but
(but some are)
  but -> some
  are -> some
(some are useful)
  some -> are
  useful -> are
```

CBOW is known to have better results for smaller training corpus.
For large corpus the skip-gram model performs better.
For this reason in the following discussion we will focus more on the skip-gram model.

#### Skip-Gram {-}

A skip-gram model does exactly the opposite of a CBOW.
It learns to predict the surrounding context words from a given target word.
So for the same example above,
assuming again a window size of 1,
we will have the following training examples:

```
(window)
[target] -> [context]

(all models are)
  models -> all
  models -> are
(models are wrong)
  are -> models
  are -> wrong
(are wrong but)
  wrong -> are
  wrong -> but
(wrong but some)
  but -> wrong
  but -> some
(but some are)
  some -> but
  some -> are
(some are useful)
  are -> some
  are -> useful
```

In mathametical notation,
the objectvie function to maximize is:

$$
\frac{1}{T}\sum_{t=1}^T\sum_{-c\le j \le c; j \ne 0}\ln P(w_{t+j} \vert w_t),
$$

where $c$ is the skip-gram window size,
$T$ is the total number of words in the training corpus,
and $w_t$ ius the $t$-th word in the corpus.

Assuming the word embedding for $w_t$ is $v_t$,
the natural choice of estimator for $P(w_{t+j} \vert w_t)$ is the softmax using embedding dot-product:

$$
P(w_{t+j} \vert w_t) = \frac{\exp({v_{t+j}^Tv_t})}{\sum_{w \in W}\exp(v_w^Tv_t)},
$$

where $W$ is the vocabulary set.

### Negative Sampling

Essentially the learner is a supervised multi-class estimator,
even though we don't need to explicitly label our data.
The labels come directly from the training corpus once the context window is determined,
which is considered a hyperparameter of the model.

The model is learned by maximizing the likelihood of all the surrounding words within a context window.
Theoretically,
the loss function will be a softmax with number of class equal to vocabulary size.
In practice this is prohibitively costly to compute due to usually the fairly large size of vocabulary.

To overcome the issue,
a technique called negative sampling is used to approximate the original softmax probability over the entire vocabulary.
The idea is to *replace one softmax training example with multiple logistic regression training examples.*
The log-likelihood of a training pair hence becomes:

$$
\ln P(w_{t+j} \vert w_t) \approx
\underbrace{\ln\sigma(v_{t+j}^Tv_t) \vphantom{\sum_{i=1;w_i \sim P_n(W)}}}
_{\log P(Y = 1 \vert w_{t+j}, w_t)} +
\underbrace{\sum_{i=1;w_i \sim P_n(W)}^k\ln\sigma(-v_i^Tv_t)}
_{\sum_i \log P(Y = 0 \vert w_i, w_t)},
$$

where $\sigma$ is a standard sigmoid function,
$P_n(W)$ is a noise distribution and $k$ is the number of negative samples drawn from that distribution,
$Y$ is simply a dummy indicator of the correctness of the prediction.^[We use the fact that $\sigma(t) = \frac{1}{1 + e^{-t}} = \frac{e^t}{1 + e^t}$ and hence $1 - \sigma(t) = \frac{1}{1 + e^t} = \sigma(-t)$.]

Let's use our toy example to illustrate the idea of negative sampling further.
Assuming the text line

```
all models are wrong but some are useful
```

is our entire corpus and also our vocabulary set.
Now consider the following one training example in a skip-gram model:

```
are -> wrong
```

In a softmax setup,
our label is a one-hot encoded vector of the length equal to vocabulary size:

| target / context | all | models | are | wrong | but | some | are | useful |
|:----------------:|:---:|:------:|:---:|:-----:|:---:|:----:|:---:|:------:|
| are              | 0   | 0      | 0   |  1    | 0   | 0    | 0   | 0      |

For this single training example the model needs to learn that `wrong` is the right word to predict.

Now if we use a negative sampling with sample size $k = 2$,
the single softmax training example effectively expands into multiple binary training examples:

| target | context | y  |
|:------:|:-------:|:--:|
| are    | wrong   | 1  |
| are    | but     | 0  |
| are    | some    | 0  |

And the model now is going to learn that given the word `are` the probasbility of `wrong` should be high and the probability of `but` and `some` should be low.
(We assume `but` and `some` are the random draws from a noise distribution.)

For natural language modeling a classical choice of the noise distribution is the unigram distribution which can be estimated simply by vocabulary frequency counts.
In the original paper of `word2vec` the unigram distribution raised to the 3/4rd power seems to perform better in their evaluation setup.
It is actually also common in practice to use a simple uniform distribution from the vocabulary.

The negative sampling loss is indeed a simplifed version of a more general loss called the Noise-Contrastive Estimation (NCE) loss,
proposed by @mnih2013learning for also learning a word embedding model.
Since practically there is little difference between the two approaches we will skip the discussion on NCE loss for simplicity.^[In a nutshell, negative sampling approximates $\ln \frac{P(y \vert x)}{Q(y \vert x)}$ while NCE approximates $\ln P(y \vert x)$. $Q(\cdot)$ is the noise distribution. So NCE is asymptotically softmax but negative sampling is not. For the sake of learning word embeddings this is not a required condition since we don't use the model prediction for the downstream task at all; instead we extract the fitted embeddings of the entire vocabulary.]

### Dropout for Frequent Words

To counter the effect of imbalanced distribution among popular (and usually less informative) words.
@mikolov2013distributed also propose the idea of randomly discard training example based on a heuristic dropout probability:

$$
\begin{equation} \label{eq:dropout}
P(w_i) = 1 - \sqrt{\frac{t}{f(w_i)}},
\end{equation}
$$

where $f(w_i)$ can be any scaled frequency of word $w_i$ and $t$ is a hyperparameter around $10^{-5}$.
The idea is to drop in higher probability when a word is more frequent.

We can plot the function to see its property (Assuming a normalized frequency $f(w_i) \in [0, 1]$):

```{r word2vec_dropout_func}
# R
p <- function(fw, t=1e-5) {
  1 - sqrt(t / fw)
}

curve(p, from=0, to=1, n=5000, ylab="P(w)", xlab="f(w)",
      main="Dropout Probability based on Word Frequency")
```

If a word is dominant it is highly possible to be discarded.
But this is unlikely to happen for a meaningful corpus with a fair size.
Since the normalized word frequency is more likely to be around zero,
we can zoom in a bit:

```{r word2vec_dropout_func_zoom}
# R
options(scipen=999)
curve(p, from=1e-6, to=3e-4, n=5000,
      ylab="P(w)", xlab="f(w)",
      main="Dropout Probability based on Word Frequency")
abline(v=1e-5, col="red")
text(1e-5, -1, pos=4, col="red", bquote(t == 10^-5))
```

Note that the function does not return a valid probability in smaller $f(w_i)$.
Indeed $\lim_{f(w_i) \to 0} P(w_i) = -\infty$.
In case of a negative value we simply replace it with a zero:
no dropout.

The hyperparameter $t$ is chosen such that for words with frequency larger than $t$ the dropout start to apply,
increasing in frequency.

### A TensorFlow Implementation

The original implementation of `word2vec` is written in C and can be found [here](https://github.com/tmikolov/word2vec).
Though the program is highly efficient,
it is not very flexible to use and has platform compatibility issue.^[Specifically, as of August 2019, the source code can be compiled only under Linux. macOS won't compile.]

In this section,
instead,
we will try to use `tensorflow` (@tensorflow2015-whitepaper) to implement an end-to-end example of a skip-gram `word2vec` model.

```{python import_tf}
import logging
logging.getLogger("tensorflow").setLevel(logging.ERROR)

import tensorflow as tf
print(tf.__version__)
```

#### Subword Segmentation {-}

Lets' use Shakespeare as our corpus.

```{bash mkdir}
mkdir -p data
```

```{python word2vec_dl_shakespeare}
import os
import shutil

shakes_file = "data/shakespeare.txt"
if not os.path.exists(shakes_file):
  shakes_dl_path = tf.keras.utils.get_file(
    "shakespeare.txt",
    "https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt")
  shutil.move(shakes_dl_path, shakes_file)
shakespeare = open(shakes_file, "rb").read().decode(encoding="utf-8")
shakespeare = shakespeare.split("\n")

# Print the first few lines.
for sent in shakespeare[:20]:
  print(sent)
```

To prepare our vocabulary,
we use `sentencepiece` (@kudo2018sentencepiece) to learn subwords from Shakespeare.^[For more details on subword unit segmentation, reader can refer to the [notebook: On Subword Units](https://everdark.github.io/k9/natural_language_understanding/subword_units/subword_units.nb.html).]

```{python word2vec_spm, results="hide"}
import sentencepiece as spm

spm_args = "--input=data/shakespeare.txt"
spm_args += " --model_prefix=m"
spm_args += " --vocab_size=5000"
spm_args += " --model_type=unigram"
spm.SentencePieceTrainer.Train(spm_args)

sp = spm.SentencePieceProcessor()
sp.Load("m.model")
```

With the pre-trained vocabulary we can easily encode our entire corpus into sequence of integers:

```{python word2vec_encode_subword}
shakespeare_wids = [sp.EncodeAsIds(line) for line in shakespeare]

# Test a line.
test_ids = shakespeare_wids[213]
print(test_ids)
print(sp.DecodeIds(test_ids))
```

```{python word2vec_check_spm}
# Double check.
for i in test_ids:
  print("{:5} -> {}".format(i, sp.IdToPiece(i)))
```

#### Skip-Gram Conversion {-}

In the `keras` module we have a convenience function `skipgrams` to generate our training data from word indices.
Here is its usage:

```{python word2vec_keras_skipgrams}
from tensorflow.keras.preprocessing.sequence import skipgrams

# We don't shuffle the result here in order to easily observe the parsing result.
# Fro acutal application shuffle is a must since otherwise it will bias the SGD optimizer.
pairs, labels = skipgrams(
  test_ids, vocabulary_size=len(sp),
  window_size=1, negative_samples=1,
  shuffle=False, seed=777,
  sampling_table=None)

# Inspect the parsed training examples.
# We discard the meta char <U+2581> for a cleaner output.
sep = "\n----------------------------"
for i, (x, y) in enumerate(zip(pairs, labels)):
  if i == 0:
    print("{:6} -> {:12} | {}{}".format("target", "context", "label", sep))
  target, context = [sp.IdToPiece(i).replace("\u2581", "") for i in x]
  print("{:6} -> {:12} | {}".format(target, context, y))
```

The `sampling_table` argumwent can be used to implement the dropout for frequent words.
Indeed `keras.preprocessing.sequence` module also comes with a convenience function exactly for this purpose:

```{python word2vec_keras_droput}
from tensorflow.keras.preprocessing.sequence import make_sampling_table

# To use this function we need to make sure our vocabulary is indexed by frequency.
# That is, more frequent words come first.
# SentencePiece does exactly this so we don't need to do any further post-processing.

# We set a higher threshold for dropput since our vocabulary size is rather small.
sampling_table = make_sampling_table(len(sp), sampling_factor=2e-4)
```

The formula follows exactly equation $\eqref{eq:dropout}$,
with the word frequency approximated by a [Zipf's law](https://en.wikipedia.org/wiki/Zipf%27s_law) using the frequency rank.

$$
f(w_r) \sim \bigg(w_r \cdot (\log w_r + \gamma) + \frac{1}{2} - \frac{1}{12w_r}\bigg)^{-1},
$$

where $w_r$ is the frequency rank of word $w$,
$\gamma$ is the [Euler-Mascheroni constant](https://en.wikipedia.org/wiki/Euler%E2%80%93Mascheroni_constant).

In this manner the frequency only depends on the ranks but not the exact counts of words in vocabulary.

Let's examine the resulting dropout probabilities given our threshold (`sampling_factor`):

```{r woprd2vec_keras_dropout_plot}
# R
plot(1 - py$sampling_table, type="l",
     xlab="Subword Index", ylab="Dropout Probability",
     main="Zipf's Law Dropout Probabilities (Keras API)",
     sub=bquote(sampling_factor == 2e-4))
```

We can verify the word-rank-frequency conversion by manually compute the approximation:

```{r wrod2vec_zipflaw_freq}
# R
g <- 0.5772156649  # Euler-Mascheroni constant.

# Zipf's Law approximation from rank to frequency.
rank_to_freq <- function(r) {
  1/(r * (log(r) + g) + .5 - 1/(12 * r))
}

freq_to_drop <- function(fw, t=2e-4) {
  pmax(0, 1 - sqrt(t / fw))
}

plot(freq_to_drop(rank_to_freq(1:5000)), type="l",
     main="Zipf's Law Dropout Probabilities (Verification)",
     sub=bquote(sampling_factor == 2e-4))
```

How about the specification for the noise distribution for negative sampling?
Based on the [source code](https://github.com/keras-team/keras-preprocessing/blob/master/keras_preprocessing/sequence.py) of `sequence.skipgrams` in the `keras.preprocessing` module,
it only performs a uniform distribution random draw from the vocabulary as the negative sample.
For simplicity we will keep it as-is.

#### Data Pipeline {-}

Now let's implement the end-to-end data pipeline from raw text to trainable skip-gram pairs.
Even though previously we already encode the entire corpus into list of sequence of subword indices,
we can simply fit the entire training data in memory,
here instead we demonstrate a more production-ready approach with a data iterator via `tf.data.Dataset` module.

```{python word2vec_tf_data_pipeline}
vocab_size = len(sp)
batch_size = 128
sampling_table = make_sampling_table(vocab_size, sampling_factor=5e-3)  # Adjust dropout.


def parse_line(text_tensor):
  """Convert a raw text line (in tensor) into skp-gram training examples."""
  ids = sp.EncodeAsIds(text_tensor.numpy())
  pairs, labels = skipgrams(
    ids, vocabulary_size=vocab_size,
    window_size=2, negative_samples=5,
    shuffle=True, sampling_table=sampling_table
  )
  targets, contexts = list(zip(*pairs))
  return targets, contexts, labels


# Since each text line can result in different number of training pairs,
# we need to use flat_map to flatten the parsed results before batching.
def parse_line_map_fn(text_tensor):
  targets, contexts, labels = tf.py_function(
    parse_line, inp=[text_tensor], Tout=[tf.int64, tf.int64, tf.int64])
  return tf.data.Dataset.from_tensor_slices(
    ({"targets": targets, "contexts": contexts}, labels))


# For simplicity we drop text lines that are too short.
dataset = tf.data.TextLineDataset(shakes_file)
dataset = dataset.filter(lambda line: tf.greater(tf.strings.length(line), 10))
dataset = dataset.flat_map(parse_line_map_fn)
dataset = dataset.shuffle(buffer_size=1000).repeat(count=None)
dataset = dataset.batch(batch_size, drop_remainder=True)
dataset = dataset.prefetch(batch_size)


# Test.
for x, y in dataset:
  tf.print(x)
  tf.print(y)
  break
```

#### Model Architecture {-}

The entire `word2vec` model is nothing more than a shallow neural network with embedding lookups,
dot-products and cross entropy loss.
It is highly similar to a matrix factorization problem with a log-loss.
The main difference lies in how we prepare the training examples.^[For more on the matrix factorization problem, especially for application in recommender systems, one can refer to the [notebook: Matrix Factorization for Recommender Systems](https://everdark.github.io/k9/matrix_factorization/matrix_factorization.nb.html).]

```{python word2vec_tf_model}
# Dimension of embeddings. Typically 128 to 512.
k = 128

# We separate target and context word indices as two input layers.
input_targets = tf.keras.layers.Input(shape=(1,), name="targets")
input_contexts = tf.keras.layers.Input(shape=(1,), name="contexts")

# Word embeddings are looked up separately for target and context words.
embeddings = tf.keras.layers.Embedding(vocab_size, k, name="word_embeddings")
target_embeddings = embeddings(input_targets)
context_embeddings = embeddings(input_contexts)

# Dot-product of the target and context word embeddings with sigmoid activation.
dots = tf.keras.layers.Dot(axes=-1, name="logits")([target_embeddings, context_embeddings])
sigmoid = tf.keras.layers.Dense(1, activation="sigmoid", name="sigmoid")
outputs = sigmoid(dots)

model = tf.keras.Model(inputs=[input_targets, input_contexts], outputs=outputs, name="word2vec")
model.compile(loss="binary_crossentropy", optimizer="sgd")

print(model.summary())
```

```{python word2vec_tf_model_train}
model.fit(dataset, epochs=5, steps_per_epoch=1000, verbose=0)
```

```{r word2vec_tf_model_loss}
# R
plot(unlist(py$model$history$history["loss"]), pch="X", type="o",
     xlab="Epoch", ylab="Loss", main="Word2vec Model Loss Trace")
```

As one can see,
for such a the small corpus the model quickly converge.

```{python word2vec_tf_trained_vectors}
word_vectors = model.get_layer("word_embeddings").weights[0].numpy()
print(word_vectors)
```

```{python word2vec_cosine_sim}
import numpy as np
from sklearn.metrics.pairwise import cosine_similarity

ws_meta = "\u2581"
i = sp.PieceToId(ws_meta + "love")

scores = cosine_similarity(word_vectors[i,np.newaxis], word_vectors)
scores = np.squeeze(scores)

top_k = 10
sim_ind = np.squeeze(scores).argsort()[-top_k:][::-1]

for i, s in zip(sim_ind, scores[sim_ind]):
  print("{:10} | {}".format(sp.IdToPiece(int(i)).replace(ws_meta, ""), s))
```

## Word2Bits

Based on the `word2vec` approach,
1@lam2018word2bits propose an enhancement to considerably reduce the amount of vector space required for the embeddings while retain the same or even improve the quality in terms of several evaluation tests.
The enhanced model is referred to as `Word2Bits`.

## GloVe

GloVe stands for "Global Vectors" for word representation,
proposed by @pennington2014glove as another unsupervised approach to train word embeddings from a large corpus.
The idea is to combine a global matrix factorization that utilize the co-occurence statistics with a local context window method that utlizes the target-context predictive relatedness.
Based on a given context window size,
we first establish a word-word co-occurence matrix,
then factiorize the sparse matrix by training on only the nonzero elements.^[[Latent Semantic Analysis (LSA)](https://en.wikipedia.org/wiki/Latent_semantic_analysis) also uses matrix factorization but on a term-document matrix.]

### The Learning Objective

Let's denote the co-occurence matrix as $X$,
a $N$ by $N$ square matrix for a vocabulary of size $N$.
$X_{ij}$ is the co-occurence of word $i$ and word $j$.
More specifically,
$X_{ij}$ is the number of times word $j$ occurs in the context of word $i$.
Then we have $P(j|i) = \frac{X_{ij}}{X_i}$ as the conditional probability of context word $j$ given target word $i$,
where $X_i = \sum_{j=1}^N X_{ij}$.

The log-cooccurrence is then modeled by a factorization:

$$
\ln X_{ij} = W_i^TW_j + b_i + b_j,
$$

where $W_i$ is the embedding and $b_i$ the bias term for word $i$.

Now the task is to minimize the sum of least squares:

$$
\operatorname*{arg\,min}_{W, b} \sum_i\sum_j\big( W_i^TW_j + b_i + b_j - \ln X_{ij} \big)^2.
$$

To deal with distribution imbalance of co-occurrence,
a weighting function is further introduced such that

$$
\operatorname*{arg\,min}_{W, b} \sum_i\sum_j f(X_{ij})\big( W_i^TW_j + b_i + b_j - \ln X_{ij} \big)^2,
$$

where

$$
f(X_{ij}) = f(x) =
\left\{
  \begin{array}{ll}
    \Big(\frac{x}{x_{max}}\Big)^\alpha & \mbox{if } x < x_{max} \\
    1 & \mbox{otherwise}.
  \end{array}
\right.
$$

There are two hyperparameters:
$x_{max}$ and $\alpha$.
In the original paper the setting is $x_{max} = 100$ and $\alpha = \frac{3}{4}$.

Note that by explicitly introducing the above weighting function all the zero entries in the co-occurence matrix are effectively discarded during the training since we (on purpose) have $f(0) = 0$.

Essentially GloVe is a factorization model on the log of context co-occurence matrix with a weighted least-squares cost function.
The model can be trained by a stochastic gradient descent optimizer on random samples of non-zero elements in the co-occurrence matrix.

The model can also be interpreted as using embedding dot-product to approximate the log of co-occurrence probability:

$$
\begin{aligned}
W_i^TW_j
&= \ln P_{ij} \\
&= \ln \frac{X_{ij}}{X_i} \\
&= \ln X_{ij} - \ln X_i.
\end{aligned}
$$

By imposing the constraint that $W_i^TW_j = W_j^TW_i$ for target-context inter-exchangability,
however,
we need to rewrite the above equation to be:

$$
W_i^TW_j = \ln X_{ij} - \ln X_i - \ln X_j.
$$

Now the term $\ln X_i$ and $\ln X_j$ correspond to the bias terms in the factorization model.

### A TensorFlow Implementation

Similar to `word2vec`,
[GloVe's original implementation](https://github.com/stanfordnlp/GloVe) is written in C.
Let's also try to implement the model with `tensorflow`.

The code will be highly similar to that of our implementation for `word2vec`.
The major difference is indeed how we create the data pipeline for our training triplet $(i, j, X_{ij})$.

#### Data Pipeline {-}

Since the co-occurrence matrix is a global count of co-occurrence,
we need to traverse the entire corpus to create the training triplets.
Unlike the previous on-line parsing example of `TextLineDataset`,
we will use `tf.data.Dataset.from_tensor_slices` directly.^[If the co-occurence triplets are too huge to fit into memory, we can write them to file first and use `TextLineDataset` to read them back by batch.]

Here we also follow the original paper to discount the co-occurence by distance.
For context window size > 1,
the GloVe model assign a decaying weight $\frac{1}{\mbox{distance}}$ to context word farer from the target word in the co-occurence matrix.

```{python glove_tf_data_preprocess}
from collections import defaultdict

def count_cooccurrence(seq_list, window_size):
  cooccur_dict = defaultdict(int)
  for seq in seq_list:
    seq_len = len(seq)
    for i, wi in enumerate(seq):
      window_start = max(0, i - window_size)
      window_end = min(seq_len, i + window_size + 1)
      for j in range(window_start, window_end):
        if j != i:
          dist = abs(i - j)
          wj = seq[j]
          cooccur_dict[(wi, wj)] += 1 / dist
  return cooccur_dict

# Pre-parse corpus into word ids.
shakespeare_wids = [sp.EncodeAsIds(line) for line in shakespeare]

# Create co-occurence training triplets.
cooccur_dict = count_cooccurrence(shakespeare_wids, 2)
triplets = np.array([(*k, v) for k, v in cooccur_dict.items()])

# Print the resulting triplets.
print(triplets[:5])
```

```{python glove_tf_data_pipeline}
batch_size = 128

dataset = tf.data.Dataset.from_tensor_slices(
  ({"targets": triplets[:,0].astype(int),
    "contexts": triplets[:,1].astype(int)},
  triplets[:,2]
))
dataset = dataset.shuffle(buffer_size=1000).repeat(count=None)
dataset = dataset.batch(batch_size, drop_remainder=True)
dataset = dataset.prefetch(batch_size)

# Test.
for x, y in dataset:
  tf.print(x)
  tf.print(y)
  break
```

#### Model Architecture {-}

The model architecture looks almost the same as for a matrix factorization problem.

<div class="fold s">
```{r glove_nn_graph, fig.width=8, fig.height=4}
DiagrammeR::grViz("
digraph subscript {
  labelloc='t'
  label='GloVe Model Architecture'

  graph [layout = dot rankdir = LR ordering = in style=dotted]

  node [shape = circle]

  subgraph cluster_indicator_layer {
    label = 'Context Window'
    c [label = 'Context']
    t [label = 'Target']
  }

  subgraph cluster_embedding_layer {
    label = 'Word Embeddings'
    C [label = 'C']
    T [label = 'T']
  }

  subgraph cluster_bias_layer {
    label = 'Biases'
    Bc [label = 'B@_{C}']
    Bt [label = 'B@_{T}']
  }

  subgraph cluster_output_layer_rv {
    label = 'Model Score'
    s [label = 'S']
  }

  subgraph cluster_output_layer_b {
    label = 'Log(Co-Occurrence Count)'
    y [label = 'y']
  }

  edge [arrowsize = .25]

  c -> C [label = 'embedding lookup']
  t -> T [label = 'embedding lookup']
  C -> dot
  T -> dot
  dot -> s
  Bc -> s
  Bt -> s
  s -> y [label = 'Predict']

}")
```
</div>

We need to implement a custom loss for GloVe.
The loss function is a sum of weigted squared error between the log of co-occurence and the model prediction.

```{python glove_tf_custom_loss}
from tensorflow.keras import backend as K

def weighted_sum_squared_error(y_true, y_pred):
  """Custom loss for GloVe.
  y_true is the co-occurrence counts.
  y_pred is the model predicted scores (embedding dot-products plus biases).
  """
  weights = K.pow(K.clip(y_true / 100, 0, 1), 3/4)
  squared_err = K.square(y_pred - K.log(y_true))
  return K.sum(weights * squared_err, axis=-1)
```

Since target and context are inter-exchangable,
theoretically we are only learning one set of embeddings.
But instead we will learn for 2 sets separately:
target and context embeddings.
The final embedding of a word can be obtained by summing the two embeddings together.

```{python glove_tf_model}
# Dimension of embeddings. Typically 128 to 512.
k = 64
vocab_size = len(sp)

# We separate target and context word indices as two input layers.
input_targets = tf.keras.layers.Input(shape=(1,), name="targets")
input_contexts = tf.keras.layers.Input(shape=(1,), name="contexts")

# Two set of embeddings are leared: target and context word embeddings.
# Theoretically they are the same thing since the problem is symmetric.
# But empirically by learning two separate sets of embeddings the results are better.
embeddings_targets = tf.keras.layers.Embedding(vocab_size, k, name="target_embeddings")
embeddings_contexts = tf.keras.layers.Embedding(vocab_size, k, name="context_embeddings")
target_embed = embeddings_targets(input_targets)
context_embed = embeddings_contexts(input_contexts)

# Dot-product of the target and context word embeddings.
dots = tf.keras.layers.Dot(axes=-1, name="dots")([target_embed, context_embed])

# Bias.
target_biases = tf.keras.layers.Embedding(
  input_dim=vocab_size, output_dim=1, name="target_biases")(input_targets)
context_biases = tf.keras.layers.Embedding(
  input_dim=vocab_size, output_dim=1, name="context_biases")(input_contexts)

# Model outputs.
scores = tf.keras.layers.Add(name="scores")([dots, target_biases, context_biases])

glove = tf.keras.Model(inputs=[input_targets, input_contexts], outputs=scores, name="glove")
glove.compile(loss=weighted_sum_squared_error, optimizer="sgd")

print(glove.summary())
```

```{python glove_tf_model_train}
glove.fit(dataset, epochs=5, steps_per_epoch=1000, verbose=0)
```

```{r glove_tf_model_loss}
# R
plot(unlist(py$glove$history$history["loss"]), pch="X", type="o",
     xlab="Epoch", ylab="Loss", main="GloVe Model Loss Trace")
```

As one can see,
for such a the small corpus the model quickly converge.

```{python glove_tf_trained_vectors}
target_word_vectors = glove.get_layer("target_embeddings").weights[0].numpy()
context_word_vectors = glove.get_layer("context_embeddings").weights[0].numpy()
glove_vectors = target_word_vectors + context_word_vectors
print(glove_vectors.shape)
```

```{python glove_cosine_sim}
ws_meta = "\u2581"
i = sp.PieceToId(ws_meta + "love")

scores = cosine_similarity(glove_vectors[i,np.newaxis], glove_vectors)
scores = np.squeeze(scores)

top_k = 10
sim_ind = np.squeeze(scores).argsort()[-top_k:][::-1]

for i, s in zip(sim_ind, scores[sim_ind]):
  print("{:10} | {}".format(sp.IdToPiece(int(i)).replace(ws_meta, ""), s))
```

### `text2vec`

[`text2vec`](https://github.com/dselivanov/text2vec) (@dmitriy2018text2vec) is a high-level R package with efficient C++ implementation of GloVe.
The APIs are elegantly designed and easy to use.
In this section we will spend some time exploring its [GloVe-related APIs](http://text2vec.org/glove.html).

Do note that all code chunks in this section is in the R language.

```{r, text2vec_import}
library(text2vec)
packageVersion("text2vec")
```

#### Create Co-Occurrence Matrix {-}

Here is a toy example of how we can use `text2vec::create_tcm` to generate a co-occurrence matrix from raw text:

```{r glove_text2vec_tcm}
text_line <- "all models are wrong but some are useful"

# Tokenize.
tokens <- space_tokenizer(text_line)
it <- itoken(tokens, progressbar=FALSE)  # A memory-efficient iterator.

# Build vocabulary.
vocab <- create_vocabulary(it)

# Build vectorizer based on vocabulary
vectorizer <- vocab_vectorizer(vocab)

# Create co-occurrence matrix using vectorizer and token iterator.
tcm <- create_tcm(it, vectorizer, skip_grams_window=1)

# A sparse matrix.
print(tcm)
```

Note that when using `skip_grams_window_context="symmetry"` (the default option) the matrix is established such that co-occurrence of $(i, j)$ equals co-occurrence of $(j, i)$,
but with the later entry masked to save space in the sparse matrix.
That is,
for example,
the entry `tcm["models", "all"]` can be interpreted as `tcm["all", "model"]` (if you are my context word then I'm also your context word) but the actual position in the latter entry holds zero count.
Technically,
the resulting matrix are always zero in the lower triangle,
and theoretically symmetric to its upper triangle if the corresponding values are not masked.

Without the storage saving trick,
the co-occurrence matrix would look like:

```{r glove_text2vec_tcm_check}
dtcm <- as.matrix(tcm)
tcm[lower.tri(tcm)] <- t(dtcm)[lower.tri(t(dtcm))]
print(tcm)
```

This reflects what the original GloVe paper mentioned:

>...for word-word co-occurrence matrices,
the distinction between a word and a context word is arbitrary and that we are free to exchange the two roles.

`text2vec` also by default implement the decayed co-occurence by distance to the target word.
Here is an example of window size = 2:

```{r glove_text2vec_decayed_context}
(create_tcm(it, vectorizer, skip_grams_window=2))
```

#### Train GloVe {-}

```{r text2vec_demo}
text8_file = "data/text8"
if (!file.exists(text8_file)) {
  download.file("http://mattmahoney.net/dc/text8.zip", "data/text8.zip")
  unzip ("data/text8.zip", files="text8", exdir="data")
}
wiki <- readLines(text8_file, n=1, warn=FALSE)

tokens <- space_tokenizer(wiki)
it <- itoken(tokens, progressbar=FALSE)
vocab <- create_vocabulary(it)
vocab <- prune_vocabulary(vocab, term_count_min=5)
vectorizer <- vocab_vectorizer(vocab)
tcm <- create_tcm(it, vectorizer, skip_grams_window=2)
dim(tcm)

glove <- GlobalVectors$new(word_vectors_size=64, vocabulary=vocab, x_max=100, alpha=3/4)
target_embeddings <- glove$fit_transform(tcm, n_iter=5)

# Merge target and context embeddings as the final embeddings.
glove_embeddings <- target_embeddings + t(glove$components)
```

## FastText

@bojanowski2017enriching extend the skip-gram `word2vec` model with subword units to improve the quality of word embeddings.
The model is called `fastText`.

### Character N-Grams

In `fastText` each word is represented by a bag of character n-grams as the subword information.
Embeddings are learned on the n-gram level instead of a fullword level.
The final word embeddings of a word is the sum of its bag of n-gram embeddings.
One particular benefit of this is the ability to overcome out-of-vocabulary issue for very large application or langauge with rich internal structure of words.

To take into account word boundaries,
beginning and end of word position is coded specially.
For example,
the world "where" with a tri-gram encoding will have the following result:

```
<wh, whe, her, ere, re>, <where>
```

Effectively we will establish two vocabulary:
One for fullwords and one for n-grams.
In the original paper n-grams for n greater or equal to 3 and smaller or equal to 6 are used.

#### A `scikit-learn` Approach {-}

Though we can implement our own n-gram parser,
let's take advantage of the high-level API from `scikit-learn` for a quick experiment.

[TODO: should ngram id start from 1? otherwise when do sparse lookup the sparse zero will be mapped to id 0.]

```{python fasttext_ngram_sklearn}
from sklearn.feature_extraction.text import CountVectorizer

corpus = shakespeare

# Create fullword vocabulary.
word_vectorizer = CountVectorizer(analyzer="word", lowercase=True)
word_vectorizer.fit(corpus)

# Create ngram vocabulary.
init_vectorizer = CountVectorizer(analyzer="char_wb", ngram_range=(3, 6), lowercase=True)
init_vectorizer.fit(corpus)

# Note that ngrams at edge are padded with space.
# Post-process the fitted vocabulary to exclude fullword.
ngrams = init_vectorizer.get_feature_names()
ngram_vocab = {}
ngram_vocab["<PAD>"] = 0
i = 1  # Start at 1 for zero-padding purpose.
for ngram in ngrams:
  if not (ngram.startswith(" ") and ngram.endswith(" ")):
    ngram_vocab[ngram] = i
    i += 1

# Re-create ngram vectorizer with processed vocabulary.
ngram_vectorizer = CountVectorizer(analyzer="char_wb", ngram_range=(3, 6), lowercase=True,
                             vocabulary=ngram_vocab)
```

```{python fasttext_ngram_sklearn_test}
print(len(word_vectorizer.vocabulary_))
print(word_vectorizer.transform(["all"]))
print(word_vectorizer.inverse_transform(word_vectorizer.transform(["all"])))

print(len(ngram_vectorizer.get_feature_names()))
print(ngram_vectorizer.transform(["all"]))
print(ngram_vectorizer.inverse_transform(ngram_vectorizer.transform(["all"])))
```

#### A `keras` Approach {-}

Keras text preprocessing module does not have ngram support!

```{python fasttext_ngram_keras}
from tensorflow.keras.preprocessing.text import Tokenizer

keras_vectorizer = Tokenizer(char_level=True, lower=True)

keras_vectorizer.fit_on_texts(["all models are wrong but some are useful"])

print(keras_vectorizer.word_index)
```

#### A `tensorflow-text` Approach {-}

The [TF.Text](https://github.com/tensorflow/text) module is a new module designated for text processing in TF 2.0.

```{python fasttext_ngram_tf_text}
import os

if os.name == "nt":
  # As of now, Windows is not supported.
  pass
else:
  import tensorflow_text as text


tt = tf.strings.unicode_split(["all models are wrong but some are useful"], input_encoding="UTF-8")
# No word boundary detection.
tt
text.ngrams(tt, width=2, reduction_type=text.Reduction.STRING_JOIN, string_separator="")
# No bos and eos. Need to manually add to the original string.

tokenizer = text.UnicodeScriptTokenizer()
kk = tokenizer.tokenize(["all models are wrong but some are useful"])
kk

kk2 = tf.strings.unicode_split(kk, input_encoding="UTF-8")
kk2
text.ngrams(kk2, width=3, reduction_type=text.Reduction.STRING_JOIN, string_separator="")
```

### Feature Hashing

Since the number of character n-grams can be huge,
feature hashing is used to control the final size of n-gram vocabulary.
The hash function used in `fastText` is [FNV-1a](https://en.wikipedia.org/wiki/Fowler%E2%80%93Noll%E2%80%93Vo_hash_function),
and the final size of n-gram vocabulary is $2 \times 10^6$.

### A TensorFlow Implementation

Comparing to `word2vec` and `GloVe`,
`fastText`'s [original implementation](https://github.com/facebookresearch/fastText.git)(written in C++) has a rather rich command line API and also a Python wrapper.
For educational purpose we will still try to use `tensorflow` to implement the model.

#### Data Pipeline {-}

Extra engineering effort must be made do deal with ngram lookups since now for each word we not only need its fullword id mapping we also need the corresponding list of character ngram id mappings,
which is rather tricky since it is not a fixed-length array for each training example.

To save computation cost we will use a cached dictionary to maintain the word id -> ngram list mapping for quick lookups.

```{python fasttext_ngram_cached_lookup}
# Create id-word inverse mappings.
id_to_word = {i: w for w, i in word_vectorizer.vocabulary_.items()}

# Create a cached word id to ngram id list mappings.
cached_map = dict()

def lookup_ngram_ids(word_id):
  if word_id not in cached_map:
    word = id_to_word[word_id]
    ngram_ids = ngram_vectorizer.transform([word]).nonzero()[1].tolist()
    cached_map[word_id] = ngram_ids
  else:
    ngram_ids = cached_map[word_id]
  return ngram_ids
```

Also,
we will need *zero-padding* for embedding lookups from tensors with variable length since each full word can be related to a variable number of ngrams.^[As of now, Keras doesn't support using sparse tensor to lookup embeddings. In tensorflow API we could use `tf.nn.embedding_lookup_sparse` to accomplish that.]
To determine the padding length,
we scan for the entire fullword vocabulary to find out the maximum possible length of ngram list for a known word.

```{python fasttext_get_padding_len}
# Estimate the maxlen for ngram list on the corpus.
# If the corpus is too huge we can use a sample to arrive at a reasonable number.
maxlen = 0
for w in word_vectorizer.get_feature_names():
  l = ngram_vectorizer.transform([w]).nnz
  if l > maxlen:
    maxlen = l

print(maxlen)
```

For each text line,
we parse it into a list of tuple of skip-gram training examples along with the corresponding ngram id list for both target and context word.

```{python fasttext_text_parser}
from tensorflow.keras.preprocessing.sequence import pad_sequences

word_vocab_size = len(word_vectorizer.vocabulary_)
sampling_table = make_sampling_table(word_vocab_size, sampling_factor=5e-3)

def parse_line(text_tensor):
  """Convert a raw text line (in tensor) into skp-gram training examples."""
  dtm = word_vectorizer.transform([text_tensor.numpy()])
  ids = dtm.nonzero()[1]
  pairs, labels = skipgrams(
    ids, vocabulary_size=word_vocab_size,
    window_size=2, negative_samples=5,
    shuffle=True, sampling_table=sampling_table
  )
  if len(pairs):
    target_word_ids, context_word_ids = list(zip(*pairs))
    # Parse character ngrams: A padded list of list of ngram ids.
    target_ngram_ids = []
    context_ngram_ids = []
    for pair in pairs:
      target_wi, context_wi = pair
      target_ngram_ids.append(lookup_ngram_ids(target_wi))
      context_ngram_ids.append(lookup_ngram_ids(context_wi))
    target_ngram_ids = pad_sequences(target_ngram_ids, maxlen=maxlen,
                                     padding="post", truncating="post")
    context_ngram_ids = pad_sequences(context_ngram_ids, maxlen=maxlen,
                                      padding="post", truncating="post")
    return (target_word_ids, context_word_ids,
      target_ngram_ids, context_ngram_ids, labels)
```

Other than the extra padded ngram tensors,
the rest of our pipeline looks very similar as before.

```{python fasttext_tf_data_pipeline}
batch_size = 128

def parse_line_map_fn(text_tensor):
  targets, contexts, target_ngram_ids, context_ngram_ids, labels = tf.py_function(
    parse_line, inp=[text_tensor], Tout=[tf.int64, tf.int64, tf.int64, tf.int64, tf.int64])
  return tf.data.Dataset.from_tensor_slices(
    ({"target_words": targets, "context_words": contexts,
      "target_ngrams": target_ngram_ids, "context_ngrams": context_ngram_ids},
     labels))

dataset = tf.data.TextLineDataset(shakes_file)
dataset = dataset.filter(lambda line: tf.greater(tf.strings.length(line), 50))
dataset = dataset.flat_map(parse_line_map_fn)
dataset = dataset.shuffle(buffer_size=1000).repeat(count=None)
dataset = dataset.batch(batch_size, drop_remainder=True)
dataset = dataset.prefetch(batch_size)

for x, y in dataset:
  tf.print(x)
  tf.print(y)
  break
```

To implement the embedding lookup from a padded tensor,
we will need to utilize the *zero-masking* feature.
In the following chunk we demonstrate a sparse lookup with summation as the combiner function:

```{python fasttext_test_embedding_lookup_sparse}
from tensorflow.keras import backend as K

padded_input = np.array([[1,1,0], [2,0,0]])
embedding_layer = tf.keras.layers.Embedding(3, 2, mask_zero=True)
embedded = embedding_layer(padded_input)
print(embedded)

mask = K.expand_dims(K.cast(embedded._keras_mask, tf.float32))
print(K.sum(embedded * mask, axis=1, keepdims=True))
```

Remember that we start our ngram vocabulary indices at 1 rather than 0,
so we can effectively mask out the embedding from 0s in the summation operation.

#### Model Architecture {-}

To fully implement the zero-padding embedding sparse lookup feature that is compatible with keras model API,
we need to create a custom layer for summing the non-zero embeddings instead of directly call the backend methods.
Here is a minimum such implementation:

```{python fasttext_combiner_layer}
class CombineEmbedding(tf.keras.layers.Layer):

  def __init__(self, **kwargs):
    super(CombineEmbedding, self).__init__(**kwargs)

  def call(self, x, mask=None):
    mask = K.expand_dims(K.cast(mask, tf.float32))
    return K.sum(x * mask, axis=1, keepdims=True)

# Check the result. Should be the same as in the previous direct backend call.
CombineEmbedding()(embedded)
```

Now we can turn into our full model implementation:

```{python fasttext_tf_model}
# Dimension of embeddings. Typically 128 to 512.
k = 128
ngram_vocab_size = len(ngram_vectorizer.get_feature_names())

# Input layers for fullword.
input_target_words = tf.keras.layers.Input(shape=(1,), name="target_words")
input_context_words = tf.keras.layers.Input(shape=(1,), name="context_words")

# Input layers for ngrams.
input_target_ngrams = tf.keras.layers.Input(shape=(maxlen,), name="target_ngrams")
input_context_ngrams = tf.keras.layers.Input(shape=(maxlen,), name="context_ngrams")

# Word embeddings.
word_embedding_layer = tf.keras.layers.Embedding(word_vocab_size, k, name="word_embeddings")
target_word_embeddings = word_embedding_layer(input_target_words)
context_word_embeddings = word_embedding_layer(input_context_words)

# Ngram embeddings.
ngram_embedding_layer = tf.keras.layers.Embedding(ngram_vocab_size, k, mask_zero=True, 
                                                  name="ngram_embeddings")
target_ngram_embeddings = ngram_embedding_layer(input_target_ngrams)
context_ngram_embeddings = ngram_embedding_layer(input_context_ngrams)

# Combine embeddings.
combiner = CombineEmbedding(name="ngram_combiner")
add = tf.keras.layers.Add(name="final_embeddings")
target_embeddings = add([combiner(target_ngram_embeddings), target_word_embeddings])
context_embeddings = add([combiner(context_ngram_embeddings), context_word_embeddings])

# Dot-product of the target and context embeddings with sigmoid activation.
logits = tf.keras.layers.Dot(axes=-1, name="logits")([target_embeddings, context_embeddings])
outputs = tf.keras.layers.Dense(1, activation="sigmoid", name="sigmoid")(logits)

# Compile model.
inputs = [input_target_words, input_context_words, input_target_ngrams, input_context_ngrams]
fasttext = tf.keras.Model(inputs=inputs, outputs=outputs, name="fastText")
fasttext.compile(loss="binary_crossentropy", optimizer="sgd")

print(fasttext.summary())
```

```{python fasttext_tf_model_train}
fasttext.fit(dataset, epochs=10, steps_per_epoch=100, verbose=0)
```

```{r fasttext_tf_model_loss}
# R
plot(unlist(py$fastext$history$history["loss"]), pch="X", type="o",
     xlab="Epoch", ylab="Loss", main="Word2vec Model Loss Trace")
```

As one can see,
for such a the small corpus the model quickly converge.

```{python fastext_tf_trained_vectors}
fastext_vectors = fastext.get_layer("final_embeddings").weights[0].numpy()
print(fastext_vectors)
```

# Context-Aware Word Embeddings

## BERT

@devlin2018bert propose [BERT](https://github.com/google-research/bert): Bidirectional Encoder Representations from Transformers,
as a deep neural network model to learn high quality word embeddings in again an unsupervised manner.
The model is based on Transformer (@vaswani2017attention),
a neural machine translation (NMT) network architecture that replaces traditional RNN or CNN layers with specialized self-attention layers.

To understand BERT it is crucial to understand the underlying attention mechanism first since it is the most important element in the underlying Transformer model.

### Attention

Attention is a neural network modeling technique first introduced in NMT task to handle the issue of long-distance dependencies in a recurrent neural network architecture.
Since then its relative simplicity and effectiveness has brought it popular presence in many other applications not limited to natural language modeling.

[Attention in RNN]

[Generallized attention with K-V-Q configuration used in Transformer]

### Multi-Head Attention

### Positional Encoding

## XLNet

@yang2019xlnet

https://github.com/zihangdai/xlnet

# Transfer Learning

The use of pre-trained word embeddings is a realization of transfer learning.
The embeddings are learned from a general language model such as those we discussed in this notebook,
but then used by a downstream model that deal with another task.
Transfer learning doesn't ensure a better result.
It depends on if the downstream task has a well-connected nature with the original embedding model and the learning corpus.

Transfer learning is not limited to natural language modeling.
It is also widely used in models handling image or video data.
For example,
in the [Youtube-8M project](https://everdark.github.io/k9/projects/yt8m/yt8m.html) for a video classification problem,
we use a pre-trained image net to extract per-frame image embeddings for the video classifier.

Though the original embedding models usually come with a set of pre-trained embeddings for several large public corpus,
the format is not standardized and how we load them into our model can differ case by case.
In this section we will walk through some coding examples using well-known libraries and their APIs in a more integrated way.

## TensorFlow Hub

[TensorFlow Hub](https://www.tensorflow.org/hub) is a reository for sharing modules of pre-trained neural network embeddings not limited to natural language.
Both `word2vec` and `BERT` pre-trained embeddings are available under the [text embedding module](https://tfhub.dev/s?module-type=text-embedding).

[coding example of using tensorhub]

## Gensim

[coding example here]

## BPEmb

>
[BPEmb](https://github.com/bheinzerling/bpemb) is a collection of pre-trained subword embeddings in 275 languages,
based on Byte-Pair Encoding (BPE) and trained on Wikipedia.

[coding example here]

# References
