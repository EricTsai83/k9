---
title: "Context-Free Word Embeddings"
subtitle: "From word2vec, GloVe, to fastText"
author:
- name: Kyle Chung
  affiliation:
date: "`r format(Sys.time(), '%d %B %Y')` Last Updated (21 Nov 2019 First Uploaded)"
output:
  html_notebook:
    highlight: tango
    number_sections: yes
    theme: paper
    toc: yes
    toc_depth: 4
    toc_float: yes
    includes:
      in_header: /tmp/meta_header.html
  code_download: true
bibliography: word_embeddings.bib
nocite: |
  @reticulate
  @tensorflow2015-whitepaper
  @heinzerling2018bpemb
abstract: |
  Word embeddings are the building blocks for neural network models that solve natural language understanding (NLU) tasks. In this notebook we review in details several influential models that are designed to learn context-free word embeddings for other downstream machine learning application, a.k.a. transfer learning. We also exercise extensively to use TensorFlow to demonstrate how we can implement each of the models. At the end, we demonstrate in short several popular frameworks to handle pre-trained embeddings.
---
<!--For equation reference in Rmd.-->
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: { equationNumbers: { autoNumber: "AMS" } }
});
</script>

<!--For controling code folding by chunk.-->
<script src="../../site_libs/utils/hide_output.js"></script>

```{r meta, include=FALSE}
meta_header_file <- file("/tmp/meta_header.html")
meta <- c(
    '<meta name="author" content="Kyle Chung">',
    '<meta property="og:title" content="Context-Free Word Embeddings: From word2vec, GloVe, to fastText">',
    '<meta property="og:type" content="article">',
    '<meta property="og:url" content="https://everdark.github.io/k9/natural_language_understanding/word_embeddings/word_embeddings.nb.html">',
    '<meta property="og:image" content="https://everdark.github.io/k9/assets/avatar.jpg">',
    '<meta property="og:description" content="A data science notebook about word embeddings for natural language modeling.">'
)
contents <- meta

# Add Github corner.
github_corner_svg <- "../../assets/github_corner.html"
github_corner_conf <- list(github_link="https://github.com/everdark/k9/tree/master/programming/r/rcpp")
contents <- c(contents, stringr::str_interp(readLines(github_corner_svg), github_corner_conf))
writeLines(contents, meta_header_file)

close(meta_header_file)
```

```{r setup, include=FALSE}
library(reticulate)
r <- try(use_python(Sys.getenv("PYTHON_PATH"), required=TRUE), silent=TRUE)
if ( is(r, "try-error") ) {
  r <- try(use_virtualenv(Sys.getenv("PYTHON_PATH"), required=TRUE), silent=TRUE)
  if ( is(r, "try-error") ) use_condaenv(Sys.getenv("PYTHON_PATH"), required=TRUE)
}
```

---

This notebook is written with [`reticulate`](https://github.com/rstudio/reticulate),
a package that allows inter-operation between R and Python.

---

# The Motivation for Embeddings

Generally speaking,
machine learning models handle only numerics.
Every piece of information must be represented by numbers,
in one way or another.
This is especially true for neural network models.

But not all features have their numerical representation by nature.
One notable example is natural language.
Words are not numbers and do not have a numerical interpretation on their own.
A classical way of converting words into numbers is through one-hot encoding.
So a word can be represented by a numerical row from a term-document sparse matrix given a corpus.

One-hot encoding has its limitation though.
As a baseline approach it works surprisingly well in lots of traditional machine learning tasks.
But it fails to deliver usable results when it comes to more complicated tasks such as machine translation,
question answering,
or other advanced natural language understanding (NLU) tasks.

Embeddings are vector representation of features that are otherwise non-numerical.^[Embeddings are indeed not limited to non-numerical features. Essentially anything can be embeded into the vector space. But embedding a numercial feature may not have as much benefit as embedding a non-numerical feature.]
All weights in the vector are trainable parameters given a machine learning task.
Depending on the task itself,
the resulting learned embeddings can be informative in the vector space.

For example,
in factorization model for recommender systems,
users and items are represented by embeddings and their similarity can be calculated by calculating the dot-product or the cosine distance of the underlying vectors.

The same idea can be applied to natural language,
where each word is represented by its own embedding.
Word embeddings are the building blocks for modern neural network models that can successfully handle a variety of NLU tasks.
In this notebook we are going to review thoroughly several most popular and influential unsupervised approaches for learning word embeddings.
The learned embeddings can then be used by a downstream machine learning model to solve further problems at the application level.
This is referred to as *transfer learning*.

# Context-Free Word Embeddings

If the embedding for the same word remain the same no matter where the word appears,
it is a context-free word embedding.
There will be a universal embedding lookup tables for each word in a pre-determined vocabulary.

There are 3 popular models to learn context-free word embeddings:

+ Word2vec
+ GloVe
+ FastText

All three approaches are unsupervised in the sense that the labels come directly from the training corpus so there is no extra effort for data labeling.
But inherently they all apply supervised learning algorithms,
which we will see clearly in the following sections one by one.

## Word2vec

@mikolov2013distributed propose the famous `word2vec` model,
followed by a subsequent massive researches in the literature on how word embeddings can be learned with a variety of different algorithms.
Though it is no longer the most popular choice of pre-trained word embeddings after years of evolution,
its concept remain solid and it is very helpful to understand what the model is trying to learn,
so we can lay down the foundation for natural language modeling.

### The Learning Objective

The core idea of learning `word2vec` is extremely simple:
to *train a predictive model that can predict the surrounding context of a word.*
There are two ways of construct such a predictive learning task:
the *continous bag-of-words* model and the *skip-gram* model.

#### Continuous Bag-of-Words {-}

CBOW predicts the target word from its context words.
Take the following text line for example:

```
all models are wrong but some are useful
```

Given a window size of 2,
we may like to predict the target word `wrong` by its context (surrounding) words `models are` and also `but some`.
The target word is a training label and its surrounding words serve as training features.

A context window of size 1 instead will give us the following training examples:

```
(window)
[target] -> [context]

(all models are)
  all -> models
  are -> models
(models are wrong)
  models -> are
  wrong -> are
(are wrong but)
  are -> wrong
  but -> wrong
(wrong but some)
  wrong -> but
  some -> but
(but some are)
  but -> some
  are -> some
(some are useful)
  some -> are
  useful -> are
```

CBOW is known to have better results for smaller training corpus.
For large corpus the skip-gram model performs better.
For this reason in the following discussion we will focus more on the skip-gram model.

#### Skip-Gram {-}

A skip-gram model does exactly the opposite of a CBOW.
It learns to predict the surrounding context words from a given target word.
So for the same example above,
assuming again a window size of 1,
we will have the following training examples:

```
(window)
[target] -> [context]

(all models are)
  models -> all
  models -> are
(models are wrong)
  are -> models
  are -> wrong
(are wrong but)
  wrong -> are
  wrong -> but
(wrong but some)
  but -> wrong
  but -> some
(but some are)
  some -> but
  some -> are
(some are useful)
  are -> some
  are -> useful
```

In mathametical notation,
the objectvie function to maximize is:

$$
\frac{1}{T}\sum_{t=1}^T\sum_{-c\le j \le c; j \ne 0}\ln P(w_{t+j} \vert w_t),
$$

where $c$ is the skip-gram window size,
$T$ is the total number of words in the training corpus,
and $w_t$ is the $t$-th word in the corpus.

Assuming the word embedding for $w_t$ is $v_t$,
the natural choice of estimator for $P(w_{t+j} \vert w_t)$ is the softmax using embedding dot-product:

$$
P(w_{t+j} \vert w_t) = \frac{\exp({v_{t+j}^Tv_t})}{\sum_{w \in W}\exp(v_w^Tv_t)},
$$

where $W$ is the vocabulary set.

### Negative Sampling

Essentially the learner is a supervised multi-class estimator,
even though we don't need to explicitly label our data.
The labels come directly from the training corpus once the context window is determined,
which is considered a hyperparameter of the model.

The model is learned by maximizing the likelihood of all the surrounding words within a context window.
Theoretically,
the loss function will be a softmax with number of classes equal to vocabulary size.
In practice this is prohibitively costly to compute due to usually the fairly large size of vocabulary.

To overcome the issue,
a technique called negative sampling is used to approximate the original softmax probability over the entire vocabulary.
The idea is to *replace one softmax training example with multiple logistic regression training examples.*
The log-likelihood of a training pair hence becomes:

$$
\ln P(w_{t+j} \vert w_t) \approx
\underbrace{\ln\sigma(v_{t+j}^Tv_t) \vphantom{\sum_{i=1;w_i \sim P_n(W)}}}
_{\log P(Y = 1 \vert w_{t+j}, w_t)} +
\underbrace{\sum_{i=1;w_i \sim P_n(W)}^k\ln\sigma(-v_i^Tv_t)}
_{\sum_i \log P(Y = 0 \vert w_i, w_t)},
$$

where $\sigma$ is a standard sigmoid function,
$P_n(W)$ is a *noise distribution* and $k$ is the number of negative samples drawn from that distribution,
$Y$ is simply a dummy indicator of the correctness of the prediction.^[We use the fact that $\sigma(t) = \frac{1}{1 + e^{-t}} = \frac{e^t}{1 + e^t}$ and hence $1 - \sigma(t) = \frac{1}{1 + e^t} = \sigma(-t)$.]

Let's use our toy example to illustrate the idea of negative sampling further.
Assuming the text line

```
all models are wrong but some are useful
```

is our entire corpus and also our vocabulary set.
Now consider the following one training example in a skip-gram model:

```
are -> wrong
```

In a softmax setup,
our label is a one-hot encoded vector of the length equal to vocabulary size:

| target / context | all | models | are | wrong | but | some | are | useful |
|:----------------:|:---:|:------:|:---:|:-----:|:---:|:----:|:---:|:------:|
| are              | 0   | 0      | 0   |  1    | 0   | 0    | 0   | 0      |

For this single training example the model needs to learn that `wrong` is the right word to predict.

Now if we use a negative sampling with sample size $k = 2$,
the single softmax training example effectively expands into multiple binary training examples:

| target | context | y  |
|:------:|:-------:|:--:|
| are    | wrong   | 1  |
| are    | but     | 0  |
| are    | some    | 0  |

And the model now is going to learn that given the word `are` the probability of `wrong` should be high and the probability of `but` and `some` should be low.
(Here we assume `but` and `some` are the random draws from a noise distribution.)

For natural language modeling a classical choice of the noise distribution is the unigram distribution which can be estimated simply by vocabulary frequency counts.
In the original paper of `word2vec` the unigram distribution raised to the 3/4rd power seems to perform better in their evaluation setup.
It is actually also common in practice to use a simple uniform distribution from the vocabulary.

The negative sampling loss is indeed a simplifed version of a more general loss called the Noise-Contrastive Estimation (NCE) loss,
proposed by @mnih2013learning for also learning a word embedding model.
Since practically there is little difference between the two approaches we will skip the discussion on NCE loss for simplicity.^[In a nutshell, negative sampling approximates $\ln \frac{P(y \vert x)}{Q(y \vert x)}$ while NCE approximates $\ln P(y \vert x)$. $Q(\cdot)$ is the noise distribution. So NCE is asymptotically softmax but negative sampling is not. For the sake of learning word embeddings this is not a required condition since we don't use the model prediction for the downstream task at all; instead we extract the fitted embeddings (learned parameters) of the entire vocabulary.]

### Dropout for Frequent Words

To counter the effect of imbalanced distribution among popular (and usually less informative) words.
@mikolov2013distributed also propose the idea of randomly discarding training example based on a heuristic dropout probability:

$$
\begin{equation} \label{eq:dropout}
P(w_i) = 1 - \sqrt{\frac{t}{f(w_i)}},
\end{equation}
$$

where $f(w_i)$ can be any scaled frequency of word $w_i$ and $t$ is a hyperparameter around $10^{-5}$.
The idea is to drop in higher probability when a word is more frequent.

We can plot the function to see its property (Assuming a normalized frequency $f(w_i) \in [0, 1]$):

```{r word2vec_dropout_func}
# R
p <- function(fw, t=1e-5) {
  1 - sqrt(t / fw)
}

curve(p, from=0, to=1, n=5000, ylab="P(w)", xlab="f(w)",
      main="Dropout Probability based on Word Frequency")
```

If a word is dominant it is highly possible to be discarded.
But this is unlikely to happen for a meaningful corpus with a fair size.
Since the normalized word frequency is more likely to be around zero,
we can zoom in a bit:

```{r word2vec_dropout_func_zoom}
# R
options(scipen=999)
curve(p, from=1e-6, to=3e-4, n=5000,
      ylab="P(w)", xlab="f(w)",
      main="Dropout Probability based on Word Frequency")
abline(v=1e-5, col="red")
text(1e-5, -1, pos=4, col="red", bquote(t == 10^-5))
```

Note that the function does not return a valid probability in small $f(w_i)$.
Indeed we have $\lim_{f(w_i) \to 0} P(w_i) = -\infty$.
In case of a negative value we simply replace it with a zero:
no dropout.

The hyperparameter $t$ is chosen such that for words with frequency larger than $t$ the dropout start to apply,
increasing in frequency.

### A TensorFlow Implementation

The original implementation of `word2vec` is written in C and can be found [here](https://github.com/tmikolov/word2vec).
Though the program is highly efficient,
it is not very flexible to use and has platform compatibility issue.^[Specifically, as of August 2019, the source code can be compiled only under Linux. macOS won't compile.]

In this section,
instead,
we will try to use `tensorflow` to implement an end-to-end example of a skip-gram `word2vec` model,
for educational purpose.

```{python import_tf}
import logging
logging.getLogger("tensorflow").setLevel(logging.ERROR)
import warnings
warnings.simplefilter(action="ignore", category=FutureWarning)

import tensorflow as tf
print(tf.__version__)
if tf.test.is_gpu_available():
  print(tf.test.gpu_device_name())
```

#### Training Corpus {-}

Since skip-gram works better when the corpus is larger,
let's use a subset of wikipedia articles (from [Wikimedia Downloads](https://dumps.wikimedia.org/)) to demonstrate the training of `word2vec`.
We use the [WikiExtractor](https://github.com/attardi/wikiextractor) tool to process the raw dump.

```{bash mkdir}
# Bash
mkdir -p data

OUTDIR=data/enwiki_texts
if [ ! -d $OUTDIR ]
then
  # Download the data if not found locally.
  WIKIDUMP=enwiki-latest-pages-articles1.xml-p10p30302
  if [ ! -f data/"$WIKIDUMP" ]
  then
    wget https://dumps.wikimedia.org/enwiki/latest/$WIKIDUMP.bz2
    mv $WIKIDUMP.bz2 data
    bzip2 -d -k $WIKIDUMP.bz2
  fi
  # Extract clean text from xml dump.
  WikiExtractor.py --json --no_templates --processes 4 \
    -o $OUTDIR --min_text_length 10 data/$WIKIDUMP
fi
```

Check one of the parsed file:

```{python}
import re
import jsonlines

infile = "data/enwiki_texts/AA/wiki_00"
def parse_wiki_sent(infile):
  paragraphs = []
  with jsonlines.open(infile) as jlines:
    for j in jlines:
      paragraphs.append(j["text"])  # One paragraph per line.

  # Break by newline.
  lines = []
  for p in paragraphs:
    lines.extend(p.lower().split("\n"))

  # Further break by sentence. We use a naive approach here just for simplicity.
  sents = []
  for l in lines:
    if len(l) >= 20:
      sents.extend(re.split(r"\. |! |\? ", l))

  return sents

sents = parse_wiki_sent(infile)
print(len(sents))

for s in sents[:3]:
  print(s + "\n")
```

#### Subword Segmentation {-}

To prepare our vocabulary,
we use `sentencepiece` (@kudo2018sentencepiece) to learn subwords from Shakespeare.^[For more details on subword unit segmentation, reader can refer to the [notebook: On Subword Units](https://everdark.github.io/k9/natural_language_understanding/subword_units/subword_units.nb.html).]

```{python word2vec_spm, results="hide"}
import os
import string

import sentencepiece as spm

vocab_size = 30000
model_prefix = "enwiki"

# Write out sentence files from processed wiki dumps.
indir = "data/enwiki_texts/AA"
outfile = "data/enwiki_sents.txt"
if not os.path.exists(outfile):
  # For notebook user:
  # This may cause RStudio to hang forever. In that case, run it in a separate Python session.
  with open(outfile, "w", encoding="utf-8") as f:
    for infile in os.listdir(indir):
      sents = parse_wiki_sent(os.path.join(indir, infile))
      for s in sents:
        # Remove all punctuation.
        s = s.translate(str.maketrans("", "", string.punctuation))
        f.write(s + "\n")
  # Train a sentencepiece model.
  spm_args = "--input={}".format(outfile)
  spm_args += " --model_prefix={}".format(model_prefix)
  spm_args += " --vocab_size={}".format(vocab_size)
  spm_args += " --model_type=unigram"
  spm.SentencePieceTrainer.Train(spm_args)

sp = spm.SentencePieceProcessor()
sp.Load(model_prefix + ".model")
```

With the pre-trained vocabulary we can easily encode our entire corpus into sequence of integers:

```{python word2vec_encode_subword}
sent_wids = [sp.EncodeAsIds(s) for s in sents]

# Test a line.
test_ids = sent_wids[319]
print(test_ids)
print(sp.DecodeIds(test_ids))
```

```{python word2vec_check_spm}
# Double check.
for cnt, i in enumerate(test_ids):
  if cnt < 5:
    print("{:5} -> {}".format(i, sp.IdToPiece(i)))
```

#### Skip-Gram Conversion {-}

In the `keras` module we have a convenience function `skipgrams` to generate our training data from word indices.
Here is its usage:

```{python word2vec_keras_skipgrams}
from tensorflow.keras.preprocessing.sequence import skipgrams

print(test_ids[:5]) # Demo short sentence.

# We don't shuffle the result here in order to easily observe the parsing result.
# Fro acutal application shuffle is a must since otherwise it will bias the SGD optimizer.
pairs, labels = skipgrams(
  test_ids[:5], vocabulary_size=len(sp),
  window_size=1, negative_samples=1,
  shuffle=False, sampling_table=None)

# Inspect the parsed training examples.
# We discard the meta char <U+2581> for a cleaner output.
sep = "\n------------------------------------------"
for i, (x, y) in enumerate(zip(pairs, labels)):
  if i == 0:
    print("{:15} -> {:15} | {}{}".format("target", "context", "label", sep))
  target, context = [sp.IdToPiece(i).replace("\u2581", "") for i in x]
  print("{:15} -> {:15} | {}".format(target, context, y))
```

The `sampling_table` argumwent can be used to implement the dropout for frequent words.
Indeed `keras.preprocessing.sequence` module also comes with a convenience function exactly for this purpose:

```{python word2vec_keras_droput}
from tensorflow.keras.preprocessing.sequence import make_sampling_table

# To use this function we need to make sure our vocabulary is indexed by frequency.
# That is, more frequent words come first.
# SentencePiece does exactly this so we don't need to do any further post-processing.

sampling_table = make_sampling_table(len(sp), sampling_factor=1e-3)
```

The formula follows exactly equation $\eqref{eq:dropout}$,
with the word frequency approximated by a [Zipf's law](https://en.wikipedia.org/wiki/Zipf%27s_law) using the frequency rank.

$$
f(w_r) \sim \bigg(w_r \cdot (\log w_r + \gamma) + \frac{1}{2} - \frac{1}{12w_r}\bigg)^{-1},
$$

where $w_r$ is the frequency rank of word $w$,
$\gamma$ is the [Euler-Mascheroni constant](https://en.wikipedia.org/wiki/Euler%E2%80%93Mascheroni_constant).
In this manner the frequency only depends on the ranks but not the exact counts of words in vocabulary.

Let's examine the resulting dropout probabilities given our vocabulary size with some commonly chosen thresholds:

```{r woprd2vec_keras_dropout_plot}
# R
sampling_table_1e3 <- py$make_sampling_table(py$vocab_size, 1e-3)
sampling_table_1e4 <- py$make_sampling_table(py$vocab_size, 1e-4)
sampling_table_1e5 <- py$make_sampling_table(py$vocab_size, 1e-5)

plot(1 - sampling_table_1e3, type="l",
     xlab="Subword Index", ylab="Dropout Probability",
     main="Zipf's Law Dropout Probabilities (Keras API)",
     sub=bquote(sampling_factor == 1e-3))
lines(1 - sampling_table_1e4, type="l", col="blue")
lines(1 - sampling_table_1e5, type="l", col="red")
```

The blue curve correspond to a factor of `1e-4`,
and red of `1e-5`.
We can see that given our not-so-big vocabulary a factor size of `1e-3` may be more appropriate.
Let's zoom-in the plot:

```{r woprd2vec_keras_dropout_plot_zoom}
first_n <- 1000
plot(1 - sampling_table_1e3[1:first_n], type="l",
     xlab="Subword Index", ylab="Dropout Probability",
     main="Zipf's Law Dropout Probabilities (Keras API)",
     sub=sprintf("(First %s Words)", first_n))
lines(1 - sampling_table_1e4[1:first_n], type="l", col="blue")
lines(1 - sampling_table_1e5[1:first_n], type="l", col="red")
```

We can verify the word-rank-frequency conversion by manually compute the approximation:

```{r wrod2vec_zipflaw_freq}
# R
g <- 0.5772156649  # Euler-Mascheroni constant.

# Zipf's Law approximation from rank to frequency.
rank_to_freq <- function(r) {
  1/(r * (log(r) + g) + .5 - 1/(12 * r))
}

freq_to_drop <- function(fw, t) {
  pmax(0, 1 - sqrt(t / fw))
}

plot(freq_to_drop(rank_to_freq(1:first_n), t=1e-3), type="l",
     main="Zipf's Law Dropout Probabilities (Verification)",
     ylab="Dropout Probability",
     sub=bquote(sampling_factor == 1e-3))
```

How about the specification for the noise distribution for negative sampling?
Based on the [source code](https://github.com/keras-team/keras-preprocessing/blob/master/keras_preprocessing/sequence.py) of `sequence.skipgrams` in the `keras.preprocessing` module,
it only performs a uniform distribution random draw from the vocabulary as the negative sample.
For simplicity we will keep it as-is.

##### Further Down-Sampling in Skipgram

The Keras `skipgrams` module will generate pair of target-context words in an exhaustive manner,
subect to subsampling of frequent words.
This turns out to be still too much for us to train a model efficiently.
For example,
a sentence of only 5 words with window size = 2 and 3 negative samples will result in up to 56 training examples!
Given any sizable corpus the training examples will easily expand to hundreds of millions,
with too many redundant information that doesn't really help the model to learn.

@mikolov2013efficient propose a randomized context window approach to effectively reduce training examples whenever context window size is larger than 1.
So instead of doing

```python
skipgrams(ids, window_size=5, ...)
```

we do

```python
skipgrams(ids, window_size=np.random.randint(5 - 1) + 1, ...)
```

Consequently,
more distant words will be subject to dropout.

In addition,
we will avoid to build the context window with consecutive target words.
Instead,
we build the window in a non-overlapping manner:

```{python word2vec_downsample_skipgram}
import numpy as np

def downsample_skipgrams(ids, vocab_size, subsample=1e-3, window=2, neg=2):
  # Limitation:
  # If a word in the target position also appears in other positions,
  # training pairs for those positions are also included.
  # We still achieve a quite good downsampling anyway.
  w = []
  y = []
  sampling_table = make_sampling_table(vocab_size, sampling_factor=subsample)
  span = 2 * window + 1
  targets = ids[window::span]
  pairs, labels = skipgrams(
    ids, vocabulary_size=vocab_size, window_size=np.random.randint(window - 1) + 1,
    negative_samples=neg, sampling_table=sampling_table, shuffle=True)
  for (t, c), l in zip(pairs, labels):
    if t in targets:
      w.append([t, c])
      y.append(l)
  return w, y
```

#### Model Architecture {-}

The entire `word2vec` model is nothing more than a shallow neural network with embedding lookups,
dot-products and cross entropy loss optimization.
It is highly similar to a matrix factorization problem with a log-loss.
The main difference lies in just how we prepare the training examples.^[For more on the matrix factorization problem, especially for application in recommender systems, one can refer to the [notebook: Matrix Factorization for Recommender Systems](https://everdark.github.io/k9/matrix_factorization/matrix_factorization.nb.html).]

```{python word2vec_tf_model}
# Dimension of embeddings. Typically 128 to 512.
embedding_size = 128

# We separate target and context word indices as two input layers.
input_targets = tf.keras.layers.Input(shape=(1,), name="targets")
input_contexts = tf.keras.layers.Input(shape=(1,), name="contexts")

# Word embeddings are looked up separately for target and context words.
embeddings = tf.keras.layers.Embedding(len(sp), embedding_size, name="word_embeddings")
target_embeddings = embeddings(input_targets)
context_embeddings = embeddings(input_contexts)

# Dot-product of the target and context word embeddings with sigmoid activation.
dots = tf.keras.layers.Dot(axes=-1, name="logits")([target_embeddings, context_embeddings])
outputs = tf.keras.layers.Dense(1, activation="sigmoid", name="sigmoid")(dots)
outputs = tf.keras.layers.Reshape((1,), input_shape=(1, 1))(outputs)

optimizer = tf.keras.optimizers.SGD(lr=.5, decay=1e-5, momentum=.9)
model = tf.keras.Model(inputs=[input_targets, input_contexts], outputs=outputs, name="word2vec")
model.compile(loss="binary_crossentropy", optimizer=optimizer)

print(model.summary())
```

Note that instead of having only the dot-product in the sigmoid activation,
we include a coefficient and a bias as well.
This is why we have in total `vocab_size * embedding_size + 2` trainable parameters,
as pointed out in the model summary.
It turns out that this help the model learn meaningful embeddings faster.

There is one thing worth noting.
Since our output label will be a single dimension of binary outcome,
we make sure the sigmoid activation also returns a single dimension by squeezing the redundant dimenssion with a `Reshape` layer.
This can be easily seen from the above model summary.

Even though the model may be extremely simple at the first glance,
the training time can be very long if we stick to our Keras model implementation.
This is mainly because number of training examples is HUGE,
and our training procedure is sequential.

For such simple factorization there are ways to optimize but won't fit into the high-level APIs we adopted here.
We will demonstrate in latter section how fast the training can be achieved should we choose to use a library with specific optimization done for this kind of model.
But as of now,
for educational purpose,
let's train the model with a variety of different ways.

#### Model Training {-}

##### In-Memory Training {-}

Provided that we have enough memory,
we can parse the entire corpus into memory in skip-gram pairs,
and use the `fit` method:

```python
infile = "data/enwiki_sents.txt"
x = []
y = []
with open(infile, "r", encoding="utf-8") as f:
  for i, line in enumerate(f):
    ids = sp.EncodeAsIds(line)
    pairs, labels = downsample_skipgrams(ids, len(sp), window=2, neg=2)
    x.extend(pairs)
    y.extend(labels)
x = np.array(x)
y = np.array(y)

model.fit(x=[x[:,0], x[:,1]], y=y, batch_size=512, epochs=1, verbose=1)
```

The above will have more than an hour to finish just one epoch.
With GPU it could be a little bit faster,
but the general overhead is on data streaming so it won't really boost the performance too much.

Or to save memory footprint we can train by sentence (varying batch size),
only parse the skip-grams on the fly with the `train_on_batch` method:

```python
sent_ids = []
with open(infile, "r", encoding="utf-8") as f:
  for i, line in enumerate(f):
    sent_ids.append(sp.EncodeAsIds(line))

n_epoch = 1
for epoch in range(n_epoch):
  total_loss = 0
  for i, sent in enumerate(sent_ids):
    if len(sent) >= 5:
      pairs, labels = downsample_skipgrams(sent, len(sp), window=2, neg=2)
      if len(pairs):  # May be empty due to dropouts.
          x = np.array(pairs)
          loss = model.train_on_batch(x=[x[:,0], x[:,1]], y=np.array(labels))
          total_loss += loss
  print("Epoch {}, total_loss={}".format(epoch, loss))
```

##### On-Disk Training {-}

A better way to train any neural network model with `tensorflow` is probably through a data generator.
Let's implement the end-to-end data pipeline from raw text to trainable skip-gram pairs with `tf.data.Dataset` API.

```{python word2vec_tf_data_pipeline}
vocab_size = len(sp)
batch_size = 4
sampling_table = make_sampling_table(vocab_size, sampling_factor=1e-3)


def parse_line(text_tensor):
  """Convert a raw text line (in tensor) into skp-gram training examples."""
  ids = sp.EncodeAsIds(text_tensor.numpy())
  pairs, labels = downsample_skipgrams(ids, len(sp), window=2, neg=2)
  if len(pairs):
    targets, contexts = list(zip(*pairs))
  else:
    targets, contexts = [], []
  return targets, contexts, labels


# Since each text line can result in different number of training pairs,
# we need to use flat_map to flatten the parsed results before batching.
def parse_line_map_fn(text_tensor):
  targets, contexts, labels = tf.py_function(
    parse_line, inp=[text_tensor], Tout=[tf.int64, tf.int64, tf.int64])
  return tf.data.Dataset.from_tensor_slices(
    ({"targets": targets, "contexts": contexts}, labels))


# For simplicity we drop text lines that are too short.
dataset = tf.data.TextLineDataset(outfile)
dataset = dataset.filter(lambda line: tf.greater(tf.strings.length(line), 20))
dataset = dataset.flat_map(parse_line_map_fn)
dataset = dataset.shuffle(buffer_size=10000).repeat(count=None)
dataset = dataset.batch(batch_size, drop_remainder=True)
dataset = dataset.prefetch(batch_size)


# Test.
for x, y in dataset:
  print(x)
  print(y)
  break
```

Note that since each text line can generate a varying number of training examples due to the nature of skip-gram and the sampling table,
without actually traversing through the entire corpus we won't know how many training examples are actually generated from our corpus.

Let's verify the operation step-by-step to make sure the model definition is working as expected.
We can use the actual data from our iterator as testing case for the verification:

```{python check_keras_layer_def}
x1 = embeddings(x["targets"])
x2 = embeddings(x["contexts"])
dot_op = tf.keras.layers.Dot(axes=-1, name="logits")
act_op = tf.keras.layers.Activation("sigmoid", name="sigmoid")

d = dot_op([x1, x2])
print(d.numpy())

a = act_op(d)
print(a.numpy())

print((x1.numpy() * x2.numpy()).sum(axis=1, keepdims=True))  # Dot products.

print(tf.sigmoid(d.numpy()).numpy())  # Activation with sigmoid.

print(tf.keras.losses.binary_crossentropy(
  tf.cast(y, tf.float32),
  tf.keras.backend.squeeze(a, 1),
  from_logits=False))

print(tf.keras.losses.binary_crossentropy(
  tf.cast(y, tf.float32),
  tf.keras.backend.squeeze(d, 1),
  from_logits=True))  # Built-in sigmoid here may have a slightly different smoothing factor.

-np.mean(y.numpy() * np.log(a.numpy()[:,0]) + (1 - y.numpy()) * np.log(1 - a.numpy()[:,0]))  # Log-loss.
```

To train the model, ideally we can simply call the `fit_generator` method on our `tf.data.Dataset`,
just like this:^[As of the notebook is written, `fit()` is buggy in TF 2.0.0 stable release (but not in the 2.0 beta), so we use `fit_generator()` instead.]

```python
model.fit_generator(dataset, epochs=1, steps_per_epoch=30000, verbose=1)
```

But `fit_generator` plays better with Keras' own [`Sequence`](https://www.tensorflow.org/api_docs/python/tf/keras/utils/Sequence) class of data iterator.
Let's use `train_on_batch` method still.
In this way we need to write our own training loop:

```python
batch_size = 512  # Use a larger batch size.
dataset = tf.data.TextLineDataset(outfile)
dataset = dataset.filter(lambda line: tf.greater(tf.strings.length(line), 20))
dataset = dataset.flat_map(parse_line_map_fn)
dataset = dataset.shuffle(buffer_size=10000).repeat(count=None)
dataset = dataset.batch(batch_size, drop_remainder=True)
dataset = dataset.prefetch(batch_size)

n_steps = 30000
losses = []
for i, (x, y) in enumerate(dataset):
  if i < n_steps:
    loss = model.train_on_batch(x, y, reset_metrics=True)
    if i % 1000 == 0:
      print("Step {}, loss={}".format(i, loss), end="\r")
      losses.append(loss)
  else:
    break

# We can check the final learning rate decayed.
print(model.optimizer._decayed_lr("float32").numpy())

# Save the entire model.
model.save("models/enwiki_w2v.h5")
```

(We won't run the code in notebook rendering to save time.)

#### Word Similarity

The learned embeddings can be retrieved from a layer of our neural nets.
It is simply a matrix of dimension with vocabulary size by embedding size.

```{python word2vec_tf_trained_vectors}
model = tf.keras.models.load_model("models/enwiki_w2v.h5")
word_vectors = model.get_layer("word_embeddings").weights[0].numpy()
print(word_vectors.shape)
print(word_vectors)
```

Let's examine the learned embeddings with a word similarity task:

```{python word2vec_cosine_sim_func}
from sklearn.metrics.pairwise import cosine_similarity

def find_similar_words(w, wv, top_k=10):
  ws_meta = "\u2581"  # The sentencepiece special meta char.
  i = sp.PieceToId(ws_meta + w)
  scores = cosine_similarity(wv[i,np.newaxis], wv)
  scores = np.squeeze(scores)
  sim_ind = np.squeeze(scores).argsort()[-top_k:][::-1]
  for i, s in zip(sim_ind, scores[sim_ind]):
    print("{:10} | {}".format(sp.IdToPiece(int(i)).replace(ws_meta, ""), s))
```

Some words are learned quite well even if we only train the model with approximately 1 epoch:

```{python word2vec_good_sim}
find_similar_words("man", wv=word_vectors)
find_similar_words("computer", wv=word_vectors)
find_similar_words("taiwan", wv=word_vectors)
find_similar_words("1", wv=word_vectors)
```

But we also see some examples not well represented yet:

```{python word2vec_bad_sim}
find_similar_words("love", wv=word_vectors)
find_similar_words("girl", wv=word_vectors)
```

With more time and resources,
we can definitely learn better embeddings than this.

### Training with High-Level Framework

For completeness let's use a high-level framework to train a `word2vec` model from scratch.
Here we use [`gensim`](https://radimrehurek.com/gensim/index.html).
This is how easy it can be done in just a few lines:

```{python gensim_w2v}
from gensim.models import Word2Vec

# Train word2vec.
gensim_w2v_model_file = "gensim_w2v.model"
if os.path.exists(gensim_w2v_model_file):
  gensim_w2v = Word2Vec.load(gensim_w2v_model_file)
else:
  # Encode sentences into list of tokens as training input.
  # Here we use our pre-trained sentencepiece model as the tokenizer.
  sp_sents = []
  with open(outfile, "r", encoding="utf-8") as f:
    for _, line in enumerate(f):
      sp_sents.append(sp.EncodeAsPieces(line))
  gensim_w2v = Word2Vec(sp_sents, sg=1, size=128, window=3, hs=0, negative=5, min_count=5, workers=4)
  gensim_w2v.save(gensim_w2v_model_file)

# Check some similarity.
ws_meta = "\u2581"  # The sentencepiece special meta char.

for l in gensim_w2v.wv.most_similar(ws_meta + "love"):
  print(l)

for l in gensim_w2v.wv.most_similar(ws_meta + "man"):
  print(l)

for l in gensim_w2v.wv.most_similar(ws_meta + "taiwan"):
  print(l)

for l in gensim_w2v.wv.most_similar(ws_meta + "1"):
  print(l)
```

With the optimized code for `word2vec` in `gensim`,
we can easily use a shorter training time (a couple of minutes in this case, for ~740k sentences from a partial wikipedia) to achieve much better results.
As one can see,
`word2vec` is able to capture words that are used in similar context in terms of their surrounding words.

## Word2Bits

Based on the `word2vec` approach,
@lam2018word2bits propose an interesting enhancement to considerably reduce the amount of vector space required for the embeddings while retain the same or even improve the quality in terms of several evaluation tests.
The enhanced model is referred to as `Word2Bits`.

To save space we will skip the discussion on this model.
Readers who are interestefd can refer to the author's [source repository](https://github.com/agnusmaximus/Word2Bits).

## GloVe

GloVe stands for "Global Vectors" for word representation,
proposed by @pennington2014glove as another unsupervised approach to train word embeddings from a large corpus.
The idea is to combine a global *matrix factorization* that utilize the local *co-occurence* statistics with a context window method that utlizes the target-context predictive relatedness.
Based on a given context window size,
we first establish a word-word co-occurence matrix,
then factiorize the sparse matrix by training on only the nonzero elements.^[[Latent Semantic Analysis (LSA)](https://en.wikipedia.org/wiki/Latent_semantic_analysis) also uses matrix factorization in this manner but on a term-document matrix instead.]

### The Learning Objective

Let's denote the co-occurence matrix as $X$,
a $N$ by $N$ square matrix for a vocabulary of size $N$.
$X_{ij}$ is the co-occurence of word $i$ and word $j$.
More specifically,
$X_{ij}$ is the number of times word $j$ occurs in the context of word $i$.
Then we have $P(j|i) = \frac{X_{ij}}{X_i}$ as the conditional probability of context word $j$ given target word $i$,
where $X_i = \sum_{j=1}^N X_{ij}$.

The log-cooccurrence is then modeled by a factorization:

$$
\ln X_{ij} = W_i^TW_j + b_i + b_j,
$$

where $W_i$ is the embedding and $b_i$ the bias term for word $i$.

Now the task is to minimize the sum of least squares:

$$
\operatorname*{arg\,min}_{W, b} \sum_i\sum_j\big( W_i^TW_j + b_i + b_j - \ln X_{ij} \big)^2.
$$

To deal with distribution imbalance of co-occurrence,
a weighting function is further introduced such that

$$
\operatorname*{arg\,min}_{W, b} \sum_i\sum_j f(X_{ij})\big( W_i^TW_j + b_i + b_j - \ln X_{ij} \big)^2,
$$

where

$$
f(X_{ij}) = f(x) =
\left\{
  \begin{array}{ll}
    \Big(\frac{x}{x_{max}}\Big)^\alpha & \mbox{if } x < x_{max} \\
    1 & \mbox{otherwise}.
  \end{array}
\right.
$$

There are two hyperparameters:
$x_{max}$ and $\alpha$.
In the original paper the setting is $x_{max} = 100$ and $\alpha = \frac{3}{4}$.

Note that by explicitly introducing the above weighting function all the zero entries in the co-occurence matrix are effectively discarded during the training since we (on purpose) have $f(0) = 0$.

Essentially GloVe is a factorization model on the log of context co-occurence matrix with a weighted least-squares cost function.
The model can be trained by a stochastic gradient descent optimizer on randomly ordered samples of non-zero elements in the co-occurrence matrix.

The model can also be interpreted as using embedding dot-product to approximate the log of co-occurrence probability:

$$
\begin{aligned}
W_i^TW_j
&= \ln P_{ij} \\
&= \ln \frac{X_{ij}}{X_i} \\
&= \ln X_{ij} - \ln X_i.
\end{aligned}
$$

By imposing the constraint that $W_i^TW_j = W_j^TW_i$ for target-context inter-exchangability,
however,
due to the general condition that $\ln X_i \ne \ln X_j$,
we will need to rewrite the above equation to be:

$$
W_i^TW_j = \ln X_{ij} - \ln X_i - \ln X_j.
$$

Now the term $\ln X_i$ and $\ln X_j$ correspond to the bias terms in the factorization model.

### A TensorFlow Implementation

Similar to `word2vec`,
[GloVe's original implementation](https://github.com/stanfordnlp/GloVe) is written in C.
Let's also try to implement the model with `tensorflow` for educational purpose.

The code will be highly similar to that of our implementation for `word2vec`.
The major difference is indeed how we create the data pipeline for our training triplet $(i, j, X_{ij})$.

#### Data Pipeline {-}

Since the co-occurrence matrix is a global count of co-occurrence,
we need to traverse the entire corpus to create the training triplets.
Unlike the previous on-line parsing example of `TextLineDataset`,
we will use `tf.data.Dataset.from_tensor_slices` directly.^[If the co-occurence triplets are too huge to fit into memory, we can write them to file first and use `TextLineDataset` to read them back by batch.]

Here we also follow the original paper to discount the co-occurence by distance.
For context window size > 1,
the GloVe model assign a decaying weight $\frac{1}{\mbox{distance}}$ to context word farer from the target word in the co-occurence matrix.

```{python glove_tf_data_preprocess}
from collections import defaultdict

def count_cooccurrence(seq_list, window_size):
  cooccur_dict = defaultdict(int)
  for seq in seq_list:
    seq_len = len(seq)
    for i, wi in enumerate(seq):
      window_start = max(0, i - window_size)
      window_end = min(seq_len, i + window_size + 1)
      for j in range(window_start, window_end):
        if j != i:
          dist = abs(i - j)
          wj = seq[j]
          cooccur_dict[(wi, wj)] += 1 / dist
  return cooccur_dict

# Pre-parse corpus into word ids.
with open(outfile, "r", encoding="utf-8") as f:
  wiki_sents = f.read().split("\n")
wiki_sents_wids = [sp.EncodeAsIds(s) for s in wiki_sents]

# Create co-occurence training triplets.
cooccur_dict = count_cooccurrence(wiki_sents_wids, 3)
triplets = np.array([(*k, v) for k, v in cooccur_dict.items()])

print(triplets.shape)
```

```{r glove_print_triplets}
# Print the resulting triplets. Use R for better print format.
head(py$triplets)
```

```{python glove_tf_data_pipeline}
batch_size = 4

dataset = tf.data.Dataset.from_tensor_slices(
  ({"targets": triplets[:,0].astype(int),
    "contexts": triplets[:,1].astype(int)},
  triplets[:,2]
))
dataset = dataset.shuffle(buffer_size=10000).repeat(count=None)
dataset = dataset.batch(batch_size, drop_remainder=True)
dataset = dataset.prefetch(batch_size)

# Test.
for x, y in dataset:
  print(x)
  print(y)
  break
```

#### Model Architecture {-}

The model architecture looks almost the same as for a matrix factorization problem.

<div class="fold s">
```{r glove_nn_graph, fig.width=8, fig.height=4}
DiagrammeR::grViz("
digraph subscript {
  labelloc='t'
  label='GloVe Model Architecture'

  graph [layout = dot rankdir = LR ordering = in style=dotted]

  node [shape = circle]

  subgraph cluster_indicator_layer {
    label = 'Context Window'
    c [label = 'Context']
    t [label = 'Target']
  }

  subgraph cluster_embedding_layer {
    label = 'Word Embeddings'
    C [label = 'C']
    T [label = 'T']
  }

  subgraph cluster_bias_layer {
    label = 'Biases'
    Bc [label = 'B@_{C}']
    Bt [label = 'B@_{T}']
  }

  subgraph cluster_output_layer_rv {
    label = 'Model Score'
    s [label = 'S']
  }

  subgraph cluster_output_layer_b {
    label = 'Log(Co-Occurrence Count)'
    y [label = 'y']
  }

  edge [arrowsize = .25]

  c -> C [label = 'embedding lookup']
  t -> T [label = 'embedding lookup']
  C -> dot
  T -> dot
  dot -> s
  Bc -> s
  Bt -> s
  s -> y [label = 'Predict']

}")
```
</div>

We need to implement a custom loss for GloVe.
The loss function is a sum of weigted squared error between the log of co-occurence and the model prediction.

```{python glove_tf_custom_loss}
from tensorflow.keras import backend as K

def weighted_sum_squared_error(y_true, y_pred):
  """Custom loss for GloVe.
  y_true is the co-occurrence counts.
  y_pred is the model predicted scores (embedding dot-products plus biases).
  """
  weights = K.pow(K.clip(y_true / 100, 0, 1), 3/4)
  squared_err = K.square(y_pred - K.log(y_true))
  return K.sum(weights * squared_err, axis=-1)
```

Since target and context are inter-exchangable,
theoretically we are only learning one set of embeddings.
But instead we will learn for 2 sets separately:
one set for target and the other for context embeddings.
The final embedding of a word can be obtained by summing the two embeddings together.
Empirically this shows better results than sharing embeddings between target and context words.

```{python glove_tf_model}
embedding_size = 64  # Dimension of embeddings.
vocab_size = len(sp)

# We separate target and context word indices as two input layers.
input_targets = tf.keras.layers.Input(shape=(1,), name="targets")
input_contexts = tf.keras.layers.Input(shape=(1,), name="contexts")

# Two set of embeddings are leared: target and context word embeddings.
# Theoretically they are the same thing since the problem is symmetric.
# But empirically by learning two separate sets of embeddings the results are better.
embeddings_targets = tf.keras.layers.Embedding(
  vocab_size, embedding_size, name="target_embeddings")
embeddings_contexts = tf.keras.layers.Embedding(
  vocab_size, embedding_size, name="context_embeddings")
target_embed = embeddings_targets(input_targets)
context_embed = embeddings_contexts(input_contexts)

# Dot-product of the target and context word embeddings.
dots = tf.keras.layers.Dot(axes=-1, name="dots")([target_embed, context_embed])

# Bias.
target_biases = tf.keras.layers.Embedding(
  vocab_size, 1, name="target_biases")(input_targets)
context_biases = tf.keras.layers.Embedding(
  vocab_size, 1, name="context_biases")(input_contexts)

# Model outputs.
scores = tf.keras.layers.Add(name="scores")([dots, target_biases, context_biases])

glove = tf.keras.Model(inputs=[input_targets, input_contexts], outputs=scores, name="glove")
glove.compile(loss=weighted_sum_squared_error, optimizer="sgd")

print(glove.summary())
```

#### Model Training {-}

Here is a training loop implementation with `train_on_batch` method:
(We don't run it in notebook to save rendering time.)

```python
batch_size = 512
dataset = tf.data.Dataset.from_tensor_slices(
  ({"targets": triplets[:,0].astype(int),
    "contexts": triplets[:,1].astype(int)},
  triplets[:,2]
))
dataset = dataset.shuffle(buffer_size=10000).repeat(count=None)
dataset = dataset.batch(batch_size, drop_remainder=True)
dataset = dataset.prefetch(batch_size)

n_epoch = 5
n_steps = len(triplets) // batch_size
for epoch in range(n_epoch):
  losses = []
  for i, (x, y) in enumerate(dataset):
    if i < n_steps:
      loss = glove.train_on_batch(x, y, reset_metrics=True)
      if i % 1000 == 0:
        print("Epoch {} Step {}, current batch loss = {}".format(epoch, i, loss), end="\r")
        losses.append(loss)
    else:
      print("\nEpoch {}, mean batch loss = {}\n".format(epoch, np.mean(losses)))
      continue

# Save model.
glove.save("models/enwiki_glove.h5")
```

#### Word Similarity {-}

```{python glove_tf_trained_vectors}
glove = tf.keras.models.load_model("models/enwiki_glove.h5")
target_word_vectors = glove.get_layer("target_embeddings").weights[0].numpy()
context_word_vectors = glove.get_layer("context_embeddings").weights[0].numpy()
glove_vectors = target_word_vectors + context_word_vectors
print(glove_vectors.shape)
```

```{python glove_cosine_sim}
find_similar_words("man", wv=glove_vectors)
find_similar_words("computer", wv=glove_vectors)
find_similar_words("taiwan", wv=glove_vectors)
find_similar_words("1", wv=glove_vectors)

find_similar_words("love", wv=glove_vectors)
find_similar_words("girl", wv=glove_vectors)
find_similar_words("elephant", wv=glove_vectors)
```

### Training with High-Level Framework

`gensim` does not support training GloVe from scratch.
In this section instead we will explore [`text2vec`](https://github.com/dselivanov/text2vec) (@dmitriy2018text2vec),
a high-level R package with very efficient C++ implementation of GloVe.
The APIs are elegantly designed and easy to use.
We will focus on its [GloVe-related APIs](http://text2vec.org/glove.html).

Do note that all code chunks in this section is in the R language.

```{r, text2vec_import}
library(text2vec)
packageVersion("text2vec")
```

#### Create Co-Occurrence Matrix {-}

Here is a toy example of how we can use `text2vec::create_tcm` to generate a co-occurrence matrix from raw text:

```{r glove_text2vec_tcm}
text_line <- "all models are wrong but some are useful"

# Tokenize.
tokens <- space_tokenizer(text_line)
it <- itoken(tokens, progressbar=FALSE)  # A memory-efficient iterator.

# Build vocabulary.
vocab <- create_vocabulary(it)

# Build vectorizer based on vocabulary
vectorizer <- vocab_vectorizer(vocab)

# Create co-occurrence matrix using vectorizer and token iterator.
tcm <- create_tcm(it, vectorizer, skip_grams_window=1)

# A sparse matrix.
print(tcm)
```

Note that when using `skip_grams_window_context="symmetry"` (the default option) the matrix is established such that co-occurrence of $(i, j)$ equals co-occurrence of $(j, i)$,
but with the later entry masked to save space in the sparse matrix.
That is,
for example,
the entry `tcm["models", "all"]` can be interpreted as `tcm["all", "model"]` (if you are my context word then I'm also your context word) but the actual position in the latter entry holds zero count.
Technically,
the resulting matrix are always zero in the lower triangle,
and theoretically symmetric to its upper triangle if the corresponding values are not masked.

Without the storage saving trick,
the co-occurrence matrix would look like:

```{r glove_text2vec_tcm_check}
dtcm <- as.matrix(tcm)
tcm[lower.tri(tcm)] <- t(dtcm)[lower.tri(t(dtcm))]
print(tcm)
```

This reflects what the original GloVe paper mentioned:

>...for word-word co-occurrence matrices,
the distinction between a word and a context word is arbitrary and that we are free to exchange the two roles.

`text2vec` also by default implement the decayed co-occurence by distance to the target word.
Here is an example of window size = 2:

```{r glove_text2vec_decayed_context}
(create_tcm(it, vectorizer, skip_grams_window=2))
```

#### Train GloVe {-}

Let's use the wiki texts again for a quick demo.
Note that we didn't use the pre-trained subwords but re-build the vocabulary with `text2vec`'s simple tokenization API.

```{r text2vec_shakes}
sents <- readLines("data/enwiki_sents.txt")

tokens <- word_tokenizer(sents)
it <- itoken(tokens, progressbar=FALSE)
vocab <- create_vocabulary(it)
vocab <- prune_vocabulary(vocab, term_count_min=10)
vectorizer <- vocab_vectorizer(vocab)
tcm <- create_tcm(it, vectorizer, skip_grams_window=3)
dim(tcm)

glove <- GlobalVectors$new(word_vectors_size=64, vocabulary=vocab, x_max=100, alpha=3/4)
target_embeddings <- glove$fit_transform(tcm, n_iter=20)

# Merge target and context embeddings as the final embeddings.
glove_embeddings <- target_embeddings + t(glove$components)
str(glove_embeddings)
```

```{r text2vece_sim}
# Calculate similarity.
query <- sim2(glove_embeddings, glove_embeddings["man",,drop=FALSE])
tail(sort(query[,1]))

query <- sim2(glove_embeddings, glove_embeddings["love",,drop=FALSE])
tail(sort(query[,1]))

query <- sim2(glove_embeddings, glove_embeddings["girl",,drop=FALSE])
tail(sort(query[,1]))

query <- sim2(glove_embeddings, glove_embeddings["elephant",,drop=FALSE])
tail(sort(query[,1]))

query <- sim2(glove_embeddings, glove_embeddings["taiwan",,drop=FALSE])
tail(sort(query[,1]))

query <- sim2(glove_embeddings, glove_embeddings["computer",,drop=FALSE])
tail(sort(query[,1]))

query <- sim2(glove_embeddings, glove_embeddings["1",,drop=FALSE])
tail(sort(query[,1]))

query <- sim2(glove_embeddings, glove_embeddings["10",,drop=FALSE])
tail(sort(query[,1]))
```

In the second similarity task outcome,
apparently "hong" should be "hong kong" instead,
but our naive tokenizer fails to make it as one single word.
Extension to a bigram approach could be a cure.

Additionally,
the word "love" seems not learned well in our simple setup.

Anyway,
one should notice that the training speed is considerably faster with `text2vec` than our educational `tensorflow` implementation.
(To re-imeplement the model in `tensorflow` with comparable performance a huge effort will be involved including porting key operation to C++ and customize data pipeline...)

## FastText

@bojanowski2017enriching extend the skip-gram `word2vec` model with subword units to improve the quality of word embeddings.
The model is called `fastText`.

### Character N-Grams

In `fastText` each word is represented by a bag of character n-grams as the subword information.
Embeddings are learned on the n-gram level instead of a fullword level.
The final word embeddings of a word is the sum of its bag of n-gram embeddings.
One particular benefit of this is the ability to overcome out-of-vocabulary issue for very large application or langauge with rich internal structure of words.

To take into account word boundaries,
beginning and end of word position is coded specially.
For example,
the world "where" with a tri-gram encoding will have the following result:

```
<wh, whe, her, ere, re>, <where>
```

Effectively we will establish two vocabulary:
One for fullwords and one for n-grams.
In the original paper n-grams for n greater or equal to 3 and smaller or equal to 6 are used.

#### A `scikit-learn` Approach {-}

Though we can implement our own n-gram parser,
let's take advantage of the high-level API from `scikit-learn` for a quick experiment.

```{python fasttext_ngram_sklearn}
from sklearn.feature_extraction.text import CountVectorizer

corpus = wiki_sents

# Create fullword vocabulary.
word_vectorizer = CountVectorizer(analyzer="word", lowercase=True)
word_vectorizer.fit(corpus)

# Create ngram vocabulary.
init_vectorizer = CountVectorizer(analyzer="char_wb", ngram_range=(3, 6), lowercase=True)
init_vectorizer.fit(corpus)

# Note that ngrams at edge are padded with space.
# Post-process the fitted vocabulary to exclude fullword.
ngrams = init_vectorizer.get_feature_names()
ngram_vocab = {}
ngram_vocab["<PAD>"] = 0
i = 1  # Start at 1 for zero-padding purpose.
for ngram in ngrams:
  if not (ngram.startswith(" ") and ngram.endswith(" ")):
    ngram_vocab[ngram] = i
    i += 1

# Re-create ngram vectorizer with processed vocabulary.
ngram_vectorizer = CountVectorizer(analyzer="char_wb", ngram_range=(3, 6), lowercase=True,
                                   vocabulary=ngram_vocab)
```

```{python fasttext_ngram_sklearn_test}
print(len(word_vectorizer.vocabulary_))
print(word_vectorizer.transform(["all"]))
print(word_vectorizer.inverse_transform(word_vectorizer.transform(["all"])))

print(len(ngram_vectorizer.get_feature_names()))
print(ngram_vectorizer.transform(["all"]))
print(ngram_vectorizer.inverse_transform(ngram_vectorizer.transform(["all"])))
```

#### A `keras` Approach {-}

Suprisingly, Keras text preprocessing module does not support ngram.
Here is just a simple demo of the tokenizer API:

```{python fasttext_ngram_keras}
from tensorflow.keras.preprocessing.text import Tokenizer

keras_vectorizer = Tokenizer(char_level=True, lower=True)
keras_vectorizer.fit_on_texts(["all models are wrong but some are useful"])

print(keras_vectorizer.word_index)
```

#### A `tensorflow-text` Approach {-}

The [TF.Text](https://github.com/tensorflow/text) module is a rather new module designated for text processing in TF 2.0.

Here are just some simple demo of the API usage:

```{python fasttext_ngram_tf_text}
import os

if os.name == "nt":
  # As of now, Windows is not supported.
  pass
else:
  import tensorflow_text as text


splitted = tf.strings.unicode_split(["all models are wrong but some are useful"], input_encoding="UTF-8")
# No word boundary detection.
splitted

text.ngrams(splitted, width=2, reduction_type=text.Reduction.STRING_JOIN, string_separator="")
# No bos and eos. Need to manually add to the original string.

tokenizer = text.UnicodeScriptTokenizer()
tokens = tokenizer.tokenize(["all models are wrong but some are useful"])
tokens

tokens2 = tf.strings.unicode_split(tokens, input_encoding="UTF-8")
tokens2
text.ngrams(tokens2, width=3, reduction_type=text.Reduction.STRING_JOIN, string_separator="")
```

### Feature Hashing

Since the number of character n-grams can be huge,
feature hashing is used to control the final size of n-gram vocabulary.
The hash function used in `fastText` is [FNV-1a](https://en.wikipedia.org/wiki/Fowler%E2%80%93Noll%E2%80%93Vo_hash_function),
and the final size of n-gram vocabulary is $2 \times 10^6$.

### A TensorFlow Implementation

Comparing to `word2vec` and `GloVe`,
`fastText`'s [original implementation](https://github.com/facebookresearch/fastText.git)(written in C++) has a rather rich command line API and also a Python wrapper.

Here is the command line usage of `fasttext`:

```
usage: fasttext <command> <args>

The commands supported by fasttext are:

  supervised              train a supervised classifier
  quantize                quantize a model to reduce the memory usage
  test                    evaluate a supervised classifier
  test-label              print labels with precision and recall scores
  predict                 predict most likely labels
  predict-prob            predict most likely labels with probabilities
  skipgram                train a skipgram model
  cbow                    train a cbow model
  print-word-vectors      print word vectors given a trained model
  print-sentence-vectors  print sentence vectors given a trained model
  print-ngrams            print ngrams given a trained model and word
  nn                      query for nearest neighbors
  analogies               query for analogies
  dump                    dump arguments,dictionary,input/output vectors
```

For educational purpose we will still try to use `tensorflow` to implement the model.

#### Data Pipeline {-}

Extra engineering effort must be made do deal with ngram lookups since now for each word we not only need its fullword id mapping we also need the corresponding list of character ngram id mappings,
which is rather tricky since it is not a fixed-length array for each training example.

To save computation cost we will use a cached dictionary to maintain the word id -> ngram list mapping for quick lookups.

```{python fasttext_ngram_cached_lookup}
# Create id-word inverse mappings.
id_to_word = {i: w for w, i in word_vectorizer.vocabulary_.items()}

# Create a cached word id to ngram id list mappings.
cached_map = dict()

def lookup_ngram_ids(word_id):
  if word_id not in cached_map:
    word = id_to_word[word_id]
    ngram_ids = ngram_vectorizer.transform([word]).nonzero()[1].tolist()
    cached_map[word_id] = ngram_ids
  else:
    ngram_ids = cached_map[word_id]
  return ngram_ids
```

Also,
we will need *zero-padding* for embedding lookups from tensors with variable length since each full word can be related to a variable number of ngrams.^[As of now, Keras doesn't support using sparse tensor to lookup embeddings. In tensorflow API we could use `tf.nn.embedding_lookup_sparse` to accomplish that.]
To determine the padding length,
we scan for the entire fullword vocabulary to find out the maximum possible length of ngram list for a known word.

```{python fasttext_get_padding_len}
# Estimate the maxlen for ngram list on the corpus.
# If the corpus is too huge we can use a sample to arrive at a reasonable number.
maxlen = 0
for w in word_vectorizer.get_feature_names():
  l = ngram_vectorizer.transform([w]).nnz
  if l > maxlen:
    maxlen = l

print(maxlen)
```

For each text line,
we parse it into a list of tuple of skip-gram training examples along with the corresponding ngram id list for both target and context word.

```{python fasttext_text_parser}
from tensorflow.keras.preprocessing.sequence import pad_sequences

word_vocab_size = len(word_vectorizer.vocabulary_)
sampling_table = make_sampling_table(word_vocab_size, sampling_factor=5e-3)

def parse_line(text_tensor):
  """Convert a raw text line (in tensor) into skp-gram training examples."""
  dtm = word_vectorizer.transform([text_tensor.numpy()])
  ids = dtm.nonzero()[1]
  pairs, labels = skipgrams(
    ids, vocabulary_size=word_vocab_size,
    window_size=2, negative_samples=5,
    shuffle=True, sampling_table=sampling_table
  )
  if len(pairs):
    target_word_ids, context_word_ids = list(zip(*pairs))
    # Parse character ngrams: A padded list of list of ngram ids.
    target_ngram_ids = []
    context_ngram_ids = []
    for pair in pairs:
      target_wi, context_wi = pair
      target_ngram_ids.append(lookup_ngram_ids(target_wi))
      context_ngram_ids.append(lookup_ngram_ids(context_wi))
    target_ngram_ids = pad_sequences(target_ngram_ids, maxlen=maxlen,
                                     padding="post", truncating="post")
    context_ngram_ids = pad_sequences(context_ngram_ids, maxlen=maxlen,
                                      padding="post", truncating="post")
    return (target_word_ids, context_word_ids,
      target_ngram_ids, context_ngram_ids, labels)
```

Other than the extra padded ngram tensors,
the rest of our pipeline looks very similar as before.

```{python fasttext_tf_data_pipeline}
batch_size = 128

def parse_line_map_fn(text_tensor):
  targets, contexts, target_ngram_ids, context_ngram_ids, labels = tf.py_function(
    parse_line, inp=[text_tensor], Tout=[tf.int64, tf.int64, tf.int64, tf.int64, tf.int64])
  return tf.data.Dataset.from_tensor_slices(
    ({"target_words": targets, "context_words": contexts,
      "target_ngrams": target_ngram_ids, "context_ngrams": context_ngram_ids},
     labels))

dataset = tf.data.TextLineDataset(shakes_file)
dataset = dataset.filter(lambda line: tf.greater(tf.strings.length(line), 50))
dataset = dataset.flat_map(parse_line_map_fn)
dataset = dataset.shuffle(buffer_size=1000).repeat(count=None)
dataset = dataset.batch(batch_size, drop_remainder=True)
dataset = dataset.prefetch(batch_size)

for x, y in dataset:
  print(x)
  print(y)
  break
```

To implement the embedding lookup from a padded tensor,
we will need to utilize the *zero-masking* feature.
In the following chunk we demonstrate a sparse lookup with summation as the combiner function:

```{python fasttext_test_embedding_lookup_sparse}
from tensorflow.keras import backend as K

padded_input = np.array([[1,1,0], [2,0,0]])
embedding_layer = tf.keras.layers.Embedding(3, 2, mask_zero=True)
embedded = embedding_layer(padded_input)
print(embedded)

mask = K.expand_dims(K.cast(embedded._keras_mask, tf.float32))
print(K.sum(embedded * mask, axis=1, keepdims=True))
```

Remember that we start our ngram vocabulary indices at 1 rather than 0,
so we can effectively mask out the embedding from 0s in the summation operation.

#### Model Architecture {-}

To fully implement the zero-padding embedding sparse lookup feature that is compatible with keras model API,
we need to create a custom layer for summing the non-zero embeddings instead of directly call the backend methods.
Here is a minimum such implementation:

```{python fasttext_combiner_layer}
class CombineEmbedding(tf.keras.layers.Layer):

  def __init__(self, **kwargs):
    super(CombineEmbedding, self).__init__(**kwargs)

  def call(self, x, mask=None):
    mask = K.expand_dims(K.cast(mask, tf.float32))
    return K.sum(x * mask, axis=1, keepdims=True)

# Check the result. Should be the same as in the previous direct backend call.
print(CombineEmbedding()(embedded))
```

Now we can turn into our full model implementation:

```{python fasttext_tf_model}
# Dimension of embeddings. Typically 128 to 512.
k = 128
ngram_vocab_size = len(ngram_vectorizer.get_feature_names())

# Input layers for fullword.
input_target_words = tf.keras.layers.Input(shape=(1,), name="target_words")
input_context_words = tf.keras.layers.Input(shape=(1,), name="context_words")

# Input layers for ngrams.
input_target_ngrams = tf.keras.layers.Input(shape=(maxlen,), name="target_ngrams")
input_context_ngrams = tf.keras.layers.Input(shape=(maxlen,), name="context_ngrams")

# Word embeddings.
word_embedding_layer = tf.keras.layers.Embedding(word_vocab_size, k, name="word_embeddings")
target_word_embeddings = word_embedding_layer(input_target_words)
context_word_embeddings = word_embedding_layer(input_context_words)

# Ngram embeddings.
ngram_embedding_layer = tf.keras.layers.Embedding(ngram_vocab_size, k, mask_zero=True, 
                                                  name="ngram_embeddings")
target_ngram_embeddings = ngram_embedding_layer(input_target_ngrams)
context_ngram_embeddings = ngram_embedding_layer(input_context_ngrams)

# Combine embeddings.
combiner = CombineEmbedding(name="ngram_combiner")
add = tf.keras.layers.Add(name="final_embeddings")
target_embeddings = add([combiner(target_ngram_embeddings), target_word_embeddings])
context_embeddings = add([combiner(context_ngram_embeddings), context_word_embeddings])

# Dot-product of the target and context embeddings with sigmoid activation.
logits = tf.keras.layers.Dot(axes=-1, name="logits")([target_embeddings, context_embeddings])
outputs = tf.keras.layers.Dense(1, activation="sigmoid", name="sigmoid")(logits)

# Compile model.
inputs = [input_target_words, input_context_words, input_target_ngrams, input_context_ngrams]
fasttext = tf.keras.Model(inputs=inputs, outputs=outputs, name="fastText")
fasttext.compile(loss="binary_crossentropy", optimizer="sgd")

print(fasttext.summary())
```

```{python fasttext_tf_model_train}
fasttext.fit_generator(dataset, epochs=10, steps_per_epoch=100, verbose=0)
```

```{r fasttext_tf_model_loss}
# R
plot(unlist(py$fasttext$history$history["loss"]), pch="X", type="o",
     xlab="Epoch", ylab="Loss", main="FastText Model Loss Trace")
```

As one can see,
for such a the small corpus the model quickly converge.

Finally, we can collect the learned embeddings:

```{python fasttext_tf_trained_vectors}
fasttext_word_vectors = fasttext.get_layer("word_embeddings").weights[0].numpy()
fasttext_ngram_vectors = fasttext.get_layer("ngram_embeddings").weights[0].numpy()
```

# Transfer Learning with Pre-Trained Embeddings

The use of pre-trained word embeddings is a realization of transfer learning.
The embeddings are learned from a general language model such as those we discussed in this notebook,
but then used by a downstream model that deal with another task.
Transfer learning doesn't ensure a better result.
It depends on if the downstream task has a well-connected nature with the original embedding model and the learning corpus.

Transfer learning is not limited to natural language modeling.
It is also widely used in models handling image or video data.
For example,
in the [Youtube-8M project](https://everdark.github.io/k9/projects/yt8m/yt8m.html) for a video classification problem,
we use a pre-trained image net to extract per-frame image embeddings for the video classifier.

Though the original embedding models usually come with a set of pre-trained embeddings for several large public corpus,
the format is not standardized and how we load them into our model can differ case by case.
In this section we will walk through some coding examples using well-known libraries and their APIs in a more integrated way.

## TensorFlow Hub

[TensorFlow Hub](https://www.tensorflow.org/hub) is a reository for sharing modules of pre-trained neural network embeddings not limited to natural language.
Both `word2vec` and `BERT` pre-trained embeddings are available under the [text embedding module](https://tfhub.dev/s?module-type=text-embedding).

As a demo, a pre-trained `word2vec` embeddings of dimension size 250 can be loaded by the following lines:

```{python tf_hub}
import tensorflow_hub as hub

# This may take some time to download for the first time.
# The model size is about 800+ mb.
embed = hub.load("https://tfhub.dev/google/Wiki-words-250/2")
embeddings = embed(["all models are wrong"])
embeddings.shape
```

The sentence embeddings is derived by the `sqrtn` combiner from individual word embeddings.
We can do a quick check:

```{python tf_hub_combiner_check}
import math

embeddings_4 = embed(["all", "models", "are", "wrong"])
((tf.reduce_sum(embeddings_4, axis=0) / math.sqrt(4)) == embeddings).numpy().all()
```

## Gensim

[`gensim`](https://radimrehurek.com/gensim/index.html) is a framework focusing on natural language processing tasks.
We can easily load up pre-trained `word2vec` embeddings with it:

```python
from gensim.models import KeyedVectors

infile = "data/GoogleGoogleNews-vectors-negative300.bin"
gensim_w2v = KeyedVectors.load_word2vec_format(infile, binary=True)
```

We skip the actual download since the pre-trained embeddings is pretty huge.
For the pre-trained file source please refer to the [official repo of Google's word2vec project](https://code.google.com/archive/p/word2vec/).

Or similarly we can load the pre-trained `fastText`:

```python
from gensim.models.wrappers import FastText

infile = "data/wiki-news-300d-1M.vec.zip"
gensim_ft = FastText.load_fasttext_format(infile)
```

For the source file one can download from the [official site of fastText](https://fasttext.cc/).

Other than ust loading pre-trained embeddings,
`gensim` is indeed a high-level library to train different kinds of language model.
`Wrod2vec`, `GloVe`, and `fastText` are all supported and can be easily trained from scratch with only a couple of lines in coding, provided the corpus is ready.
We will skip the demo and for readers interested in this please refer to their official documents.

## spaCy

[`spaCy`](https://spacy.io/) is yet another popular NLP framework which also comes with pre-trained embeddings.
The embeddings are a built-in feature in spaCy.
Once we download the language file,
vector representation will be readily available without explicitly preparing additional pre-trained file.

We will skip the demo to save space here.

## BPEmb

[BPEmb](https://github.com/bheinzerling/bpemb) is a collection of pre-trained subword embeddings in 275 languages,
based on Byte-Pair Encoding (BPE) and trained on Wikipedia.

`bpemb` can be used to encode string tino subwords and also lookup pre-trained embeddings:

```{python bpemb}
from bpemb import BPEmb

bpemb = BPEmb(lang="en", vs=10000, dim=50)  # This will download a small pre-trained file.

print(bpemb.encode("all models are wrong")) # Encode into subwords.

print(bpemb.vectors.shape)  # The pre-trained embeddings in numpy.

print(bpemb.emb)  # Under the hood it is a gensim word2vec model implementation.
```

# References
