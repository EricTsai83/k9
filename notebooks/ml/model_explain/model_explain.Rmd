---
title: "On Model Explainability"
subtitle: "From LIME, SHAP, to Explainable Boosting"
author:
- name: Kyle Chung
  affiliation:
date: "`r format(Sys.time(), '%d %b %Y')` Last Updated (05 Dec 2019 First Uploaded)"
output:
  html_notebook:
    highlight: tango
    number_sections: yes
    theme: paper
    toc: yes
    toc_depth: 3
    toc_float: yes
    includes:
      in_header: /tmp/meta_header.html
  code_download: true
bibliography: model_explain.bib
nocite: |
  @reticulate
  @maas-EtAl:2011:ACL-HLT2011
  @scikit-learn
  @tensorflow2015-whitepaper
abstract: |
  Model explainability has gained more and more attention recently among machine learning practitioners. Especially with the popularization of deep learning frameworks, which further promotes the use of increasingly complicated models to improve accuracy. In the reality, however, model with the highest accuracy may not be the one that can be deployed. Trust is one important factor affecting the adoption of complicated models. In this notebook we give a brief introduction to several popular methods on model explainability. And we focus more on the hands-on which demonstrates how we can actually explain a model, under a variety of use cases.
---
<!--For equation reference in Rmd.-->
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: { equationNumbers: { autoNumber: "AMS" } }
});
</script>

```{r meta, include=FALSE}
meta_header_file <- file("/tmp/meta_header.html")

# Add open graph meta.
meta <- c(
  '<meta name="author" content="Kyle Chung">',
  '<meta property="og:title" content="On Model Explainability: From Shap, Lime, to Interpretable Boosting">',
  '<meta property="og:type" content="article">',
  '<meta property="og:url" content="https://everdark.github.io/k9/notebooks/ml/model_explain/model_explain.nb.html">',
  '<meta property="og:image" content="https://everdark.github.io/k9/assets/androidify.jpg">',
  '<meta property="og:description" content="A data science notebook about machine learning model explainability.">'
)
contents <- meta

# Add Github corner.
github_corner_svg <- "../../../assets/github_corner.html"
github_corner_conf <- list(github_link="https://github.com/everdark/k9/tree/master/notebooks/ml/model_explain")
contents <- c(contents, stringr::str_interp(readLines(github_corner_svg), github_corner_conf))
writeLines(contents, meta_header_file)

close(meta_header_file)
```

```{r setup, include=FALSE}
library(reticulate)
r <- try(use_python(Sys.getenv("PYTHON_PATH"), required=TRUE), silent=TRUE)
if ( is(r, "try-error") ) {
  r <- try(use_virtualenv(Sys.getenv("PYTHON_PATH"), required=TRUE), silent=TRUE)
  if ( is(r, "try-error") ) use_condaenv(Sys.getenv("PYTHON_PATH"), required=TRUE)
}

# Auto show matplotlib plots.
matplotlib <- import("matplotlib")
matplotlib$use("Agg", force=TRUE)
```

---

This notebook is written with [`reticulate`](https://github.com/rstudio/reticulate),
a package that allows inter-operation between R and Python.

---

# Motivation

Why do we need to explain a machine learning model?
The benefit of an explanable model against a black-box model is for the model to be *trusted*.
Trust can be important in many real applications where the successful deployment of a machine learning model requires the trust from end users.
Sometimes trust plays a even bigger role than model accuracy.

Other than trust,
model explainability (or interpretability, interchangeably used hereafter) may also guide us in the correct direction to further improve the model.

In general,
linear model is more interpretable than non-linear model.
But the former also suffers from lower accuracy.
More advanced and hence complicated model usually has worse interpretability.

One should not confuse model explainability with the actual causality.
Being able to explain a model doesn't mean that we can identify any ground-truth causal relation behind the model.
Model explainability is for and only for the model,
but not for the facts we'd like to model.
Nevertheless,
understand how we can reason the model definitely will help us better model the actual pattern behind the scence.

In this notebook we will walk through 3 popular approaches of model prediction explanation,
each of them comes with a dedicated Python package:

1. [`shap`](https://github.com/slundberg/shap)
2. [`lime`](https://github.com/marcotcr/lime)
3. [`interpret`](https://github.com/interpretml/interpret)

# Explanation Models

An explanation model $g(x)$ is an *interpretable approximation* of the original model $f(x)$.
Its sole purpose is to give extra explainability the original model fails to provide,
due to its own complexity.

The general idea is to use a simplified input $x\prime$ such that $x = h_x(x\prime)$,
where $h_x(\cdot)$ is a mapping function for any given raw input $x$.
Then the interpretable approximation can be written as:

$$
g(x\prime) \approx f(h_x(x\prime)).
$$

The *additive feature attribution methods* specify the explanation model of the following form:

$$
g(x\prime) = \phi_0 + \sum_{i = 1}^m \phi_i x_i\prime,
$$

where $m$ is total number of simplified features,
$x\prime \in \{0, 1\}$ simply an indicator.^[In many such methods, the simplified input is the indicator of feature presence. One example: Shapley regression values.]
Apparently,
the choice of an additive model is for (linear) intrepretability.
The simplified features are an *interpretable representation* of the original model features.

# LIME

One very popular such above additive model is LIME (@ribeiro2016should).
LIME stands for **Local Interpretable Model-Agnostic Explanations.**
As its full name suggests,
LIME can be applied to *any* machine learning model.
LIME achieves prediction-level interpretability by approxmiating the original model with an explanation model locally around that prediction.

## On Text Classifiers

For text classification problem,
the most straightforward interpretable representation of the model features will be a binary indicator vector of bag of words.
So the explanation model will try to reason which word or token is driving the prediction in what direction.
And this is true no matter the form of the original model feature.
May it be a word count matrix,
a term frequency-inverse document frequency (TF-IDF) matrix,
or numerical embeddings.

In the following we will use [Large Movie Review Dataset](https://ai.stanford.edu/~amaas/data/sentiment/) to do a binary sentiment classification exercise.
We will use machine learning libraries such as `scikit-learn` and `tensorflow` to quickly build models and use `lime` to experiment explanation modeling.

```{python import_some}
import os
import logging
logging.getLogger("tensorflow").setLevel(logging.ERROR)
import warnings
warnings.simplefilter(action="ignore", category=UserWarning)
warnings.simplefilter(action="ignore", category=FutureWarning)

import matplotlib.pyplot as plt

import tensorflow as tf
print(tf.__version__)
if tf.test.is_gpu_available():
  print(tf.test.gpu_device_name())

import sklearn
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report
import joblib

print(sklearn.__version__)
```

```{bash mkdir}
# Create model dir to cache all models trained in the notebook.
mkdir -p models
```

First,
we prepare the movie review dataset.^[Keras comes with the dataset preprocessed as integer sequences (`from tf.keras.datasets import imdb`).]

```{python maybe_download, results="hide"}
import tensorflow_datasets as tfds

# Load the data as tf.data.Dataset.
imdb = tfds.load(name="imdb_reviews", as_supervised=True)
```

The dataset is a perfectly balanced dataset with 50,000 examples,
half for positive and half for negative sentiment.

```{python prepare_text_data}
import numpy as np

# Extract all texts as list since we want to use libraries other than tensorflow as well.
# And since this is a small dataset, we don't care about memory usage.
imdb_reviews_train = []
imdb_reviews_test = []
y_train = []
y_test = []
for x, y in imdb["train"].batch(128):
  imdb_reviews_train.extend(x.numpy())
  y_train.extend(y.numpy())
for x, y in imdb["test"].batch(128):
  imdb_reviews_test.extend(x.numpy())
  y_test.extend(y.numpy())

# tf works on bytes, but other packages mostly work on decoded str.
imdb_reviews_train = [b.decode("utf8") for b in imdb_reviews_train]
imdb_reviews_test = [b.decode("utf8") for b in imdb_reviews_test]
y_train = np.array(y_train)
y_test = np.array(y_test)

# Take one.
print(imdb_reviews_train[87])
```

We use the data prepared by `tensorflow-datasets` here just to save some time.
For those who want to process the data in its very original format (where one review is in one `.txt` file),
the files can be downloaded by this piece of code:

```python
home = os.path.expanduser("~")
cache_dir = os.path.join(home, ".keras")
imdb_remote_path = "https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz"
imdb_local_path = os.path.join(cache_dir, "datasets", "aclImdb_v1.tar.gz")
imdb_fname = os.path.basename(imdb_local_path)
if not os.path.exists(imdb_local_path):
  _ = tf.keras.utils.get_file(fname=imdb_fname, origin=imdb_remote_path,
                              extract=True, cache_dir=cache_dir)
```

### Explain Random Forest

Let's build a random forest with TF-IDF as our feature space.
We will use the popular `scikit-learn` library for implementation.^[It will be much faster if we choose `xgboost`'s or `lightgbm`'s implementation of random forest. However, to demonstrate compatibility of `lime` with `scikit-learn` we purposely choose the slower implementation here.]

```{python text_tfidf}
vectorizer = TfidfVectorizer(lowercase=True, min_df=10)
X_train = vectorizer.fit_transform(imdb_reviews_train)
X_test = vectorizer.transform(imdb_reviews_test)
print(len(vectorizer.vocabulary_))  # Without OOV token.
```

```{python text_clf_rf}
rf_model_file = "models/text_rf.joblib"

# Save/reload the model to save notebook rendering time.
if os.path.exists(rf_model_file):
  rf = joblib.load(rf_model_file)
else:
  rf = RandomForestClassifier(n_estimators=300)
  _ = rf.fit(X_train, y_train)
  _ = joblib.dump(rf, rf_model_file)
rf_pred = rf.predict(X_test)

print(classification_report(y_test, rf_pred))
```

As a baseline without extensive tuning (we didn't tune anything indeed!),
random forest seems to perform fairly well on this dataset judged by the classification report above.

Now move on to model explanation with LIME:

```{python lime_on_rf}
from lime.lime_text import LimeTextExplainer
from sklearn.pipeline import make_pipeline

# We need a pipeline since LimeTextExplainer.explain_instance expects raw text input.
rf_pipe = make_pipeline(vectorizer, rf)
explainer = LimeTextExplainer(class_names=["Negative", "Positive"])

test_id = 0
exp = explainer.explain_instance(
  imdb_reviews_test[test_id], rf_pipe.predict_proba, num_features=6)
# For ipynb, one can simply call exp.show_in_notebook(text=True) to embed the html output.
exp.save_to_file("/tmp/explain_text_rf.html")

# True label of the tested example.
print(y_test[test_id])
```

```{r, echo=FALSE}
htmltools::includeHTML("/tmp/explain_text_rf.html")
```

**TODO: Discuss the result.**

### Explain Neural Nets with Word Embeddings

Now let's try a neural network model with word embeddings trained from scratch.
We use `tensorflow.keras` API to quickly build and train a neural net.
We average word embeddings as the document embeddings for each review,
then feed-forward a ReLU layer before the sigmoid activation for cross-entropy optimization.

As an exercise,
instead of re-using the vocabulary built by `TfidfVectorizer` with `scikit-learn`,
we will re-tokenize the text data with `keras.preprocessing` module.
The inherent consistency under the Keras framework will also simplify our latter works on network layering.

```{python lime_nn_embeddings}
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences

# Build vocabulary. We use similar size as in our previous TfidfVectorizer.
# Since we will use zero padding, 0 cannot be used as OOV index.
# Keras tokenizer by default reserves 0 already. OOV token, if used, will be indexed at 1.
# Note that len(tokenizer.index_word) will be all vocabulary instead of `num_words`.
vocab_size = 20001  # +1 for 0 index used for padding.
oov_token = "<unk>"
tokenizer = Tokenizer(lower=True, oov_token=oov_token, num_words=vocab_size - 1)
tokenizer.fit_on_texts(imdb_reviews_train)

# Encode text with padding to ensure fixed-length input.
seq_train = tokenizer.texts_to_sequences(imdb_reviews_train)
seq_train_padded = pad_sequences(seq_train, padding="post")
maxlen = seq_train_padded.shape[1]
seq_test = tokenizer.texts_to_sequences(imdb_reviews_test)
seq_test_padded = pad_sequences(seq_test, padding="post", maxlen=maxlen)

# Wrap Keras Sequential model with scikit-learn API.
# This is because LimeTextExplainer seems buggy with a native Keras model.
nn_model_file = "models/text_clf_nn.h5"

def model_fn():
  embedding_size = 64
  model = tf.keras.Sequential([
    tf.keras.layers.Input(shape=(maxlen,), name="sequence"),
    tf.keras.layers.Embedding(vocab_size, embedding_size, mask_zero=True, name="word_embedding"),
    tf.keras.layers.GlobalAveragePooling1D(name="doc_embedding"),
    tf.keras.layers.Dense(embedding_size / 2, activation="relu", name="relu"),
    tf.keras.layers.Dense(1, activation="sigmoid", name="sigmoid")
  ], name="nn_classifier")
  model.compile(optimizer="adam",
                loss="binary_crossentropy",
                metrics=["accuracy"])
  return model

print(model_fn().summary(line_length=90))

nn_model = tf.keras.wrappers.scikit_learn.KerasClassifier(model_fn)
metrics = nn_model.fit(
  x=seq_train_padded, y=y_train,
  batch_size=256, epochs=10,
  validation_data=(seq_test_padded, y_test),
  validation_steps=20,
  callbacks=[
    tf.keras.callbacks.EarlyStopping(monitor="val_loss", patience=2),
    tf.keras.callbacks.ModelCheckpoint(nn_model_file, monitor="val_loss", save_best_only=True)
  ],
  verbose=2)

# TODO: tf.keras.models.load_model(nn_model_file) fails?
# https://github.com/tensorflow/tensorflow/issues/34901

nn_pred = (np.squeeze(nn_model.predict(seq_test_padded)) > .5).astype(int)
print(classification_report(y_test, nn_pred))
```

```{python lime_on_nn_embeddings}
def nn_predict_fn(text):
  # This is for sklearn wrapper only.
  seq = tokenizer.texts_to_sequences(text)
  seq = pad_sequences(seq, padding="post", maxlen=maxlen)
  return nn_model.predict_proba(seq)

explainer = LimeTextExplainer(class_names=["Negative", "Positive"])
model_exp = explainer.explain_instance(imdb_reviews_test[test_id], nn_predict_fn, num_features=6)
model_exp.save_to_file("/tmp/explain_text_nn.html")
```

```{r, echo=FALSE}
htmltools::includeHTML("/tmp/explain_text_nn.html")
```

**TODO: Discuss the difference between RF and NN.**

### Explain Transfer Learning

One step further,
let's use pre-trained word embeddings for the neural nets and build another explanation model.
We will use [GloVe](https://nlp.stanford.edu/projects/glove/) (@pennington2014glove).
We use just the smaller GloVe model since our dataset is quite small.
In building the GloVe embeddings we need to take special care about out-of-vocabulary token AND padding index since we will be using the Keras API.

```{python lime_transfer_learning_vocab}
import pandas as pd

# Download GloVe pre-trained embeddings.
home = os.path.expanduser("~")
cache_dir = os.path.join(home, ".keras")
glove6b_remote_path = "http://nlp.stanford.edu/data/glove.6B.zip"
glove6b_local_path = os.path.join(cache_dir, "datasets", "glove.6B.50d.txt")
glove6b_fname = os.path.basename(glove6b_local_path)
if not os.path.exists(glove6b_local_path):
  _ = tf.keras.utils.get_file(fname=glove6b_fname, origin=glove6b_remote_path,
                              extract=True, cache_dir=cache_dir)

glove_all = pd.read_csv(glove6b_local_path, sep=" ", header=None, index_col=0, quoting=3)

# Map vocabulary to pre-trained embeddings.
matched_toks = []
for i, w in tokenizer.index_word.items():
  if i < vocab_size:
    if w in glove_all.index:
      matched_toks.append(w)
    else:
      matched_toks.append(oov_token)

# Note that GloVe pre-trained embeddings does not include its own OOV token.
# We will use a global average embedding to represent OOV token.
print(len([t for t in matched_toks if t == oov_token]))  # How many OOVs?

glove_all.loc[oov_token] = glove_all.values.mean(axis=0)
glove = glove_all.loc[matched_toks].values

# Append dummy 0-index vector to support padding.
glove = np.vstack([np.zeros((1, glove.shape[1])), glove])
print(glove.shape)
```

Now let's build the neural network.
Most of the code will be the same as before,
only the `Embedding` layer now we will use a constant matrix for initialization.

```{python lime_transfer_learning_model}
tr_model_file = "models/text_clf_tr.h5"

def tr_model_fn():
  embedding_size = glove.shape[1]
  model = tf.keras.Sequential([
    tf.keras.layers.Input(shape=(maxlen,), name="sequence"),
    tf.keras.layers.Embedding(
      vocab_size, embedding_size,
      embeddings_initializer=tf.keras.initializers.Constant(glove),
      trainable=True, mask_zero=True, name="glove_embedding"),
    tf.keras.layers.GlobalAveragePooling1D(name="doc_embedding"),
    tf.keras.layers.Dense(embedding_size / 2, activation="relu", name="relu"),
    tf.keras.layers.Dense(1, activation="sigmoid", name="sigmoid")
  ], name="tr_classifier")
  model.compile(optimizer="adam",
                loss="binary_crossentropy",
                metrics=["accuracy"])
  return model

print(tr_model_fn().summary(line_length=90))

tr_model = tf.keras.wrappers.scikit_learn.KerasClassifier(tr_model_fn)
metrics = tr_model.fit(
  x=seq_train_padded, y=y_train,
  batch_size=256, epochs=20,
  validation_data=(seq_test_padded, y_test),
  validation_steps=20,
  callbacks=[
    tf.keras.callbacks.EarlyStopping(monitor="val_loss", patience=2),
    tf.keras.callbacks.ModelCheckpoint(tr_model_file, monitor="val_loss", save_best_only=True)
  ],
  verbose=2)

tr_pred = (np.squeeze(tr_model.predict(seq_test_padded)) > .5).astype(int)
print(classification_report(y_test, tr_pred))
```

```{python lime_on_transfer_learning}
def tr_predict_fn(text):
  # This is for sklearn wrapper only.
  seq = tokenizer.texts_to_sequences(text)
  seq = pad_sequences(seq, padding="post", maxlen=maxlen)
  return tr_model.predict_proba(seq)

explainer = LimeTextExplainer(class_names=["Negative", "Positive"])
model_exp = explainer.explain_instance(imdb_reviews_test[test_id], tr_predict_fn, num_features=6)
model_exp.save_to_file("/tmp/explain_text_tr.html")
```

```{r, echo=FALSE}
htmltools::includeHTML("/tmp/explain_text_tr.html")
```

**TODO: Summarize all models.**

## On Tabular Data Classifier

**TODO: Find am interesting tabular dataset for demo.**

```{python}
from lime.lime_tabular import LimeTabularExplainer
```

## On Image Classifier

**TODO: Use a pre-trained model?**

# Shapley Regression Values

**TBC**

# SHAP

@NIPS2017_7062 propose SHAP (**SHapley Additive exPlanations**),
yet another additive feature attribution method for model explainability.
It is a more general approach where LIME is indeed only a special case of it.
Just like LIME,
in theory it can be applied to *any* machine learning model,
but comes with a customized fast implementation particularly for gradient boosting trees (GBT).
It supports APIs of well-known GBT libraries such as
[`xgboost`](https://github.com/dmlc/xgboost),
[`lightgbm`](https://github.com/microsoft/LightGBM),
and [`catboost`](https://github.com/catboost/catboost).

The interpretability provided by SHAP is again *local*.
It assigns each feature an importance value *for a particular prediction.*
Hence it provides for any given model prediction what may be the driving force for the model to make such prediction.

`shap` also comes with more visualization methods for feature investigation,
especially for feature interaction exploration.

## On Text Classifiers

### Explain Random Forest

For exactly the same model we build in the previous section about `lime`,
this time we use `shap` to build the explanation model.

```{python shap_on_rf}
import shap

sorted_vocab = sorted(vectorizer.vocabulary_.items(), key=lambda kv: kv[1])
sorted_vocab = [w for w, i in sorted_vocab]

# shap doesn't support sparse matrix input for scikit-learn model.
X_test_d = X_test.toarray()

# Run explanation approximation for all testing examples at once.
# TODO: Performance issue here for shap on sklearn RF.
explain_top_k = 100
#rf_shape_explainer = shap.TreeExplainer(rf)
#rf_shap_values = rf_shape_explainer.shap_values(X_test_d[:explain_top_k])
#
#rf_shap_p = shap.force_plot(
#  rf_shape_explainer.expected_value[1],
#  rf_shap_values[1][0,:],
#  X_test_d[0,:],
#  feature_names=sorted_vocab
#)
#shap.save_html("/tmp/rf_shap_val.html", rf_shap_p)
```

```{r, echo=FALSE}
#htmltools::includeHTML("/tmp/rf_shap_val.html")
```

```{python shap_on_rf_summary_plot}
top_k = 20
#shap.summary_plot(rf_shap_values, X_test_d[:explain_top_k],
#                  feature_names=sorted_vocab, max_display=top_k, plot_size=.25)
```

```{python rf_feat_imp}
rf_feat_imp = pd.Series(rf.feature_importances_, index=sorted_vocab).sort_values()
rf_feat_imp.tail(top_k).plot(kind="barh")
```

### Explain Gradient Boosting Trees

Gradient boosting trees (GBT) is a powerful model family proven to work exceptionally well in many different applications.
Yet due to its ensembling nature,
GBT is also hard to intrepret in general.
Like random forest,
after building a model we will have access to the overall feature importance.
But at per-instance prediction level there is nothing much to say about why the outcome is what it is.

Let's quickly build a GBT using `lightgbm` (with the same TF-IDF vectorization as we did for the random forest model):

```{python shap_lgb}
import lightgbm as lgb

# lightgbm does not allow utf-8 encoded feature names.
# Since important tokens are most likely ascii-compatible for our dataset,
# we simply strip non-ascii as a workaround for this exercise.
def remove_non_ascii(s):
  return "".join([i if ord(i) < 128 else " " for i in s])

sorted_vocab_ascii = [remove_non_ascii(v) for v in sorted_vocab]

d_train = lgb.Dataset(X_train, label=y_train, feature_name=sorted_vocab_ascii)
d_test = lgb.Dataset(X_test, label=y_test, feature_name=sorted_vocab_ascii)

lgb_params = {
  "learning_rate": .05,
  "boosting_type": "gbdt",
  "objective": "binary",
  "metric": ["binary_logloss", "auc"],
  "num_leaves": 16,
  "max_depth": 4,
  "min_data_per_leaf": 20,
  "verbose": -1
}

lgb_model_file = "models/text_clf_lgb.txt"

# Save/reload model to save notebook rendering time.
if os.path.exists(lgb_model_file):
  # TODO:
  # Parameters are not loaded back? A bug? (Which cause the subsequent call to shap_values fail.)
  # https://github.com/microsoft/LightGBM/issues/2613
  lgbooster = lgb.Booster(model_file=lgb_model_file, params=lgb_params)
else:
  lgbooster = lgb.train(
    params=lgb_params,
    num_boost_round=1000, early_stopping_rounds=20,
    train_set=d_train, valid_sets=[d_test],
    verbose_eval=100)
  _ = lgbooster.save_model(lgb_model_file)

lgb_pred = (lgbooster.predict(X_test) > .5).astype(int)
print(classification_report(y_test, lgb_pred))
```

For the overall feature importance derived by GBT:

```{python shap_lgb_feat_imp}
ax = lgb.plot_importance(lgbooster, max_num_features=20)
plt.show()
```

Since `shap.TreeExplainer` is customized for GBT for speed,
unlike the previous random forest example here we can feed in all testing examples to calculate all shap values at once.

```{python shap_on_lgb}
# Sparse matrix is supported for lightgbm model.
lgb_explainer = shap.TreeExplainer(lgbooster)
lgb_shap_values = lgb_explainer.shap_values(X_test)

lgb_shap_p = shap.force_plot(
  lgb_explainer.expected_value[1],
  lgb_shap_values[1][test_id,:],
  X_test[test_id,:].toarray(),  # We still need a dense matrix here.
  feature_names=sorted_vocab
)
shap.save_html("/tmp/lgb_shap_val.html", lgb_shap_p)
```

```{r, echo=FALSE}
htmltools::includeHTML("/tmp/lgb_shap_val.html")
```

**TODO: Discuss the result. Any thing different from RF explained by LIME?**

```{python verify_log_odds}
# Note that by default shap for lightgbm reports log-odds rather than probability.
# To verify this:
p = lgbooster.predict(X_test[test_id,:].toarray())
print(np.log(p / (1 - p)))
```

### Explain Neural Nets with Word Embeddings

Encountered a bug for DeepExplainer with TF 2.0:

+ https://github.com/slundberg/shap/issues/850
+ https://github.com/slundberg/shap/issues/885

```python
# shap does not support keras model in scikit-learn wrapper.
# Let's re-build the model and retain its Sequental class.
dl_model = model_fn()
metrics = dl_model.fit(
  x=seq_train_padded, y=y_train,
  batch_size=256, epochs=20,
  validation_data=(seq_test_padded, y_test),
  validation_steps=20,
  callbacks=[
    tf.keras.callbacks.EarlyStopping(monitor="val_loss", patience=2),
    tf.keras.callbacks.ModelCheckpoint(tr_model_file, monitor="val_loss", save_best_only=True)
  ],
  verbose=0)

dl_shap_explainer = shap.DeepExplainer(dl_model, seq_train_padded)
```

## On Tabular Data Classifier

**TBC**

## On Image Classifier

**TODO: Use a pre-trained model?**

# Explainable Boosting Machine

@nori2019interpretml publish the open source package `interpret` for a fast implementation of **Generalized Additive Models with Pairwise Interactions, or GA<sup>2</sup>M** (@lou2013accurate).
As of `r format(Sys.time(), '%Y-%m-%d')`, `interpret` is still in its alpha release with limited documentation.
The library contains two groups of modeling frameworks:

+ `glassbox`: explanable machine learning models
+ `blackbox`: machine learning explanation models (such as LIME and SHAP)

We've already covered the mainstream approach in the second group,
i.e.,
models that approximate (locally) the original model (supposed to be a blackbox) for better explainability.
The more interesting part of `interpret` is to bring about another type of model that is readily interpretable from its very origin,
and yet still competitively accurate:
**the Explainable Boosting Machine**, or EBM.

EBM is an additive model of the form:

$$
g(E(y)) = \beta_0 + \sum f_j (x_j) + \sum f_{ij}(x_i, x_j),
$$

where $g(\cdot)$ is a link function (sigmoid for binary classification, for an example),
$f_j$ is the *feature function* for the $j$-th feature,
learned by a gradient boosting machine with only that feature at a time and in a round-robin fashion for all features.
$f_{ij}$ is a *pairwise interaction* feature function to further boost the accuracy of the model while remain interpretability.

The model is interpretable since the contribution of any individual feature can be directly quantified by their corresponding feature function $f_j$.
Such explanation can extend up to pairwise interaction if pairwise feature functions are also estimated.

**TODO: How to detect pairwise interaction?**

```{python interpret_model}
from interpret.glassbox import ExplainableBoostingClassifier
from interpret.perf import ROC
from interpret import show

# EBM seems not suitable for bag of words model since too many "categorical" variables.
# Also it won't be able to fit a image model for the same reason.
# Maybe try a tabular dataset for the demo.

# interpret does not support sparse matrix.
#X_train_d = X_train.toarray()

# TODO: OOM...
#ebm = ExplainableBoostingClassifier(feature_names=sorted_vocab, n_estimators=16)
#ebm.fit(X_train_d, y_train)

#ebm = ExplainableBoostingClassifier(n_estimators=16, feature_names=sorted_vocab)
#ebm.fit(X_train[:100].toarray(), y_train[:100])

#data = X_train[:100].toarray()
#new_data = data

#ebm_perf = ROC(ebm.predict_proba).explain_perf(X_test, y_test, name="EBM")
#show(ebm_perf)
```

```{python}
#ebm_global = ebm.explain_global()
#show(ebm_global)

#ebm_local = ebm.explain_local(X_test, y_test)
#show(ebm_local)
```

# References
