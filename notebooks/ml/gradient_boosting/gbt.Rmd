---
title: "Introduction to Gradient Boosting Trees"
subtitle: "From Theory to Hands-On Examples"
author:
- name: Kyle Chung
  affiliation:
date: "`r format(Sys.time(), '%d %b %Y')` Last Updated (20 Dec 2019 First Uploaded)"
output:
  html_notebook:
    highlight: tango
    number_sections: yes
    theme: paper
    toc: yes
    toc_depth: 3
    toc_float: yes
    includes:
      in_header: /tmp/meta_header.html
  code_download: true
bibliography: gbt.bib
nocite: |
  @reticulate
  @friedman2001greedy
  @rpart
abstract: |
  TBC.
---

```{r meta, include=FALSE}
meta_header_file <- file("/tmp/meta_header.html")

# Add open graph meta.
meta <- c(
  '<meta name="author" content="Kyle Chung">',
  '<meta property="og:title" content="Introduction to Gradient Boosting Trees: From Theory to Hands-On Examples">',
  '<meta property="og:type" content="article">',
  '<meta property="og:url" content="https://everdark.github.io/k9/notebooks/ml/gradient_boosting/gbt.nb.html">',
  '<meta property="og:image" content="https://everdark.github.io/k9/assets/androidify.jpg">',
  '<meta property="og:description" content="A data science notebook about gradient boosting trees.">'
)
contents <- meta

# Add Github corner.
github_corner_svg <- "../../../assets/github_corner.html"
github_corner_conf <- list(github_link="https://github.com/everdark/k9/tree/master/notebooks/ml/gradient_boosting")
contents <- c(contents, stringr::str_interp(readLines(github_corner_svg), github_corner_conf))
writeLines(contents, meta_header_file)

close(meta_header_file)
```

```{r setup, include=FALSE}
library(reticulate)
r <- try(use_python(Sys.getenv("PYTHON_PATH"), required=TRUE), silent=TRUE)
if ( is(r, "try-error") ) {
  r <- try(use_virtualenv(Sys.getenv("PYTHON_PATH"), required=TRUE), silent=TRUE)
  if ( is(r, "try-error") ) use_condaenv(Sys.getenv("PYTHON_PATH"), required=TRUE)
}
```

---

This notebook is written with [`reticulate`](https://github.com/rstudio/reticulate),
a package that allows inter-operation between R and Python.

---

# Motivation

# Gradient Boosting Machines

Gradient boosting is a model ensembling technique.
It combines multiple models into one in an *iterative and additive* manner.
Each individual model is indeed a weak learner,
but jointly they end up providing powerful predictions.
Mathematically speaking,
the model can be expressed as:

$$
\hat{y} = F_k(x) = \sum_k f_k(x),
$$

where $k$ is number of iterations (and hence number of weak learners),
$f_k$ belongs to some model family and $x$ is the feature vector.

A weak learner can be a model with lesser features,
lesser parameters,
or a simpler structure.

## Functional Gradient Descent

Now if we rewrite the above model as:

$$
\begin{aligned}
y
&= F_k(x) + \epsilon \\
&= F_{k-1}(x) + f_k(x) + \epsilon,
\end{aligned}
$$

then we have:

$$
f_k(x) = y - F_{k-1}(x) - \epsilon.
$$

The equation tells us that the function added at round $k$ is fitting the *model residual* of the previous ensembled model ($F_{k-1}$).
If our loss is squared error,
model residual is indeed the negative gradient to the loss w.r.t. the function at the last step (ignoring the constant term):

$$
- \frac{\partial\frac{1}{2}\big[y - F_{k-1}\big]^2}{\partial F_{k-1}} = (y - F_{k-1}).
$$

To generalize the idea,
given any loss function $L(y, F(x))$,
to solve for the optimal function $f_k$ to add at round $k$ we are actually doing *functional gradient descent*:

$$
f_k(x) = - \frac{\partial L(y, F_{k-1}(x))}{\partial F_{k-1}(x)}.
$$

The negative gradient is also referred to as *pseudo-residual*.
When our loss function is not squared error,
we no longer literally fit a model on residuals but on the pseudo-residuals.

## Boosting v.s. Gradient Boosting

It can be easily confused about the name *gradient boosting*.
As we already seen,
graident boosting is indeed a gradient descent algorithm,
but operating at functional space.
That is,
to optimize the objective function we are not looking for parameters but looking for functions.
In the training phase we are iteratively searching for a function (a weak learner) to be included into our model.

*Boosting* is yet another different model ensembling technique,
not to be confused with gradient boosting.
The general idea of boosting is to also train a model iteratively.
In each round the training examples will be re-weighted based on the results (losses) from the last run.
Harder examples will gain higher weights so the subsequent training will shift focus onto them.
The most popular boosting algorithm is probably [AdaBoost](https://en.wikipedia.org/wiki/AdaBoost),
abbreviated from adaptive boosting.

Boosting is similar to gradient boosting in that both are model ensembling with additive weak learners and iterative training.
But they are very different in the way they search for the optimal function at each iteration.
Boosting fits a learner with examples re-weighted by loss generated from the previous ensemble.
Gradient boosting instead fits a learner with pseudo-residuals:
the negative functional gradient w.r.t. the loss.

## Linear Model as Weak Learner

The choice of weak learner has a huge impact on the effectiveness of gradient boosting machines.
Not all weak learners can gain equally from such ensemble.
In this section we demonstrate a model ensemble using a weak linear learner.
It turns out that gradient boosting with a linear model may do no good at all.

Let's create some simulated data baseed on a simple ground-truth (non-linear) data generating process:

```{r sim_data}
library(data.table)

set.seed(666)

# DGP: y = 3 x_1 + 6 x_2 -1.5 x_1x_2 + e
n <- 1000
x1 <- rnorm(n, 2, 4)
x2 <- runif(n)
e <- rnorm(n)
y <- 3 * x1 + 6 * x2 - 1.5 * x1 * x2 + e

# Train-test split.
is_test <- runif(n) < .2
D <- data.table(x1=x1, x2=x2, y=y)
head(D)
```

We fit a linear model with all features (consdier it as a "strong" learner):

```{r sim_data_fit}
# Fit a linear model with all features.
f <- lm(y ~ x1 + x2, data=D[!is_test])
D[, yhat:=predict(f, D)]


# MSE as our loss function.
mse <- function(y, yhat) {
  mean((y - yhat)^2)
}

# Compute testing loss.
with(D[is_test], mse(y, yhat))
```

Now let's ensemble a weak learner which only take one feature.
We train the model based on gradient boosting framework.
Since our loss is squared error,
we can simply fit the model residual of the previous ensemble:

```{r gbm_linear_learner}
f1 <- lm(y ~ x1, data=D[!is_test])
D[, yhat1:=predict(f1, D)]
D[, resid1:=y - yhat1]

with(D[is_test], mse(y, yhat1))

f2 <- lm(resid1 ~ x2, data=D[!is_test])
D[, yhat2:=predict(f2, D)]
D[, resid2:=y - yhat1 - yhat2]

with(D[is_test], mse(y, yhat1 + yhat2))

f3 <- lm(resid2 ~ x1, data=D[!is_test])
D[, yhat3:=predict(f3, D)]
D[, resid3:=y - yhat1 - yhat2 - yhat3]

with(D[is_test], mse(y, yhat1 + yhat2 + yhat3))

f4 <- lm(resid3 ~ x2, data=D[!is_test])
D[, yhat4:=predict(f4, D)]
D[, resid4:=y - yhat1 - yhat2 - yhat3 - yhat4]

with(D[is_test], mse(y, yhat1 + yhat2 + yhat3 + yhat4))
```

As one may realize,
the boosted model won't outperform a simple linear model with all features included.
Is it because of our naive built-from-scratch implementation?
Not really.
Let's use `xgboost`--one of the state-of-the-art gradient boosting frameworks--to train a GBM with linear learner:

```{r xgb_linear_learner}
library(xgboost)

Dx <- xgb.DMatrix(as.matrix(D[!is_test, .(x1, x2)]), label=D[!is_test, y])
Dt <- xgb.DMatrix(as.matrix(D[is_test, .(x1, x2)]), label=D[is_test, y])

bst <- xgb.train(params=list(objective="reg:squarederror", booster="gblinear"), 
                 data=Dx, watchlist=list(test=Dt),
                 nround=50, early_stopping_rounds=5, verbose=0)

yhat <- predict(bst, as.matrix(D[is_test, .(x1, x2)]))
mse(D[is_test, y], yhat)
```

The result is roughly the same.

In general,
gradient boosting does not help imporving a linear model.
This is by large due to the fact that GBM is an additive model.
Combining linear models additively will just result in yet another linear model.
Indeed,
using a linear model as the base learner will have the effect of solving a full linear system (a model with all features) in an iterative and approximated manner.
So at best the result is the same as a single linear model with all features.

## Tree Model as Weak Learner

Based on empirical findings,
the best weak learner in GBM is a tree.
Most of the gradient boosting (or just boosting) implementations use tree models as their base learner.

For our simulated data,
gradient boosted trees can easily outperform the linear model:

```{r xgb_tree_learner}
bst2 <- xgb.train(params=list(objective="reg:squarederror", booster="gbtree"), 
                  data=Dx, watchlist=list(test=Dt),
                  nround=50, early_stopping_rounds=5, verbose=0)

yhat <- predict(bst2, as.matrix(D[is_test, .(x1, x2)]))
mse(D[is_test, y], yhat)  # Smaller loss achieved.
```

For the rest of the notebook we will focus our discussion on tree-based gradient boosting.

# Tree Ensembles

Before we talk about gradient boosting trees,
let's have a brief review on tree models in general,
from a single tree to tree ensembles.

Tree models are non-differentiable.
Gradient-based optimizers won't directly work on tree models.
This makes them very different from other machine learning models such as neural networks.

Tree models are powerful because they are non-linear.
Features are allowed to interact with each other in a sequential and loss-guided manner.
And they are invariant to feature scale since a tree split is only ordinal.
This simplifies data preprocessing and hence the overall pipeline.

## A Single Tree

The idea of a single tree model is rather simple.
The model uses a split finding algorithm to enumerate over all the possible splits on all the features.
Once a best split is found based on a particular measure,
the model move on to find the next split given the current node,
growing the tree deeper and deeper until a certain criterion is met.

Here is a general pseudo code describing a single split algorithm in its *exact* form:

```
[Exact Tree Split Algorithm]

m: total number of features
n: total number of examples
s: score to measure the quality of a split
x_ij: the i-th example value on feature j

for feature j in 1 to m:
  sort x_j
  for example x_ij in 1 to n on feature j:
    compute s

do split using j* at x_ij* associated with maximum s
```

A Tree model involves multiple *sequential* splits to arrive at the terminal node which gives prediction.
Here are several important components that must be taken care for a detailed implementation:

#### Feature Iteration {-}

The algorithm is greedy.
To search for the optimal split all features are traversed.
The immediate consequence is the over-fitting nature of a single tree.
And it is such difficulty in generalization leads to the idea of tree ensembles.

#### Feature Value Iteration {-}

For each feature,
a sorting operation is required to traverse over the (distinct) sorted values of that feature.
The more values a feature can take the more computing resources are needed to find the best split.

#### Quality of Split Measure {-}

This is probably the most imnportant part of a split algorithm.
It directly affects the accuracy of the resulting tree.
The quality score measures how good a node is splitting the examples.
In the literature this is often refered to as *impurity*.
A classical impurity measure is Gini index for classification and residual sum of squares for regression.

#### Depth of the Tree {-}

How deep (how many splits) should we grow the tree?
This is a hyper-parameter that can affect model generalization.
For classification we can grow a tree as deep as until every node contains only one single example.
That is,
until no split can be further achieved.
A tree that is too deep will definitely suffer from over-fitting.

A single tree is usually very bad at generalization.
The only benefit is probably its *interpretability* since the tree structure itself is a set of human friendly decision rules.
However due to its poor generalization over unseen data,
such interpretability is practically of no use.

### A Regression Tree

Continue with our simulated dataset,
let's quickly build a single regression tree and evaluate its performance on the testing set.

```{r sim_data_tree_train}
library(rpart)  # Recursive PARTitioning 
library(rpart.plot)

f <- rpart(y ~ x1 + x2, data=D[!is_test], method="anova", model=TRUE)
rpart.plot(f, digits=3)  # For each node the mean value and population are shown.
```

The testing score is worse than a linear model:

```{r sim_data_tree_eval}
mse(D[is_test, y], predict(f, D[is_test]))
```

Even though our true model is non-linear,
a tree failed to outperform a linear model.
Does that make sense?
Our tree may be too simple to perform a task on predicting a continous value.
The tree is only capable of predicting 9 distinct values,
which make it difficult to compete with a linear model on a continous metric such as squared error.

Indeed it is very easy to build a more complicated (less regulated) tree to outperform the linear model:

```{r sim_data_better_tree}
f2 <- rpart(y ~ x1 + x2, data=D[!is_test], method="anova", model=TRUE,
            control=rpart.control(minsplit=10, cp=5e-4))
uniqueN(f2$where)  # How many terminal nodes?
mse(D[is_test, y], predict(f2, D[is_test]))
```

Put aside regularization and model complexity,
a common quality of split measure for a regression tree is the sum of squares $SS = \sum (y_i -\bar{y})^2$.
So for a given node we find a split such that the decrease in sum-of-squares $SS_T - (SS_L + SS_R)$ is the largest.
$SS_T$ is the $SS$ of current node,
$SS_R$ is $SS$ of the right-hand node resulted from the split,
$SS_L$ the $SS$ of left-hand node.
Note that the prediction of a regression tree node is its mean value.
Hence $SS$ is the squared error of that node.

### A Classification Tree

We use the old-school [UCI Iris](https://archive.ics.uci.edu/ml/datasets/iris) dataset to quickly demo a single classification tree.

```{r rpart_train}
confusion_matrix <- function(y_true, y_pred) {
  table(y_true, y_pred)
}

# Split IRIS dataset into train-test.
is_test <- runif(nrow(iris)) < .3
iris_train <- iris[!is_test,]
iris_test <- iris[is_test,]

# Grow a single classification tree.
f <- rpart(Species ~ ., data=iris_train, method="class")
rpart.plot(f)
```

```{r rpart_eval_train}
# Confusion matrix for the training set.
print(confusion_matrix(iris_train$Species, predict(f, iris_train, type="class")))
```

```{r rpart_eval_test}
# Confusion matrix for the testing set.
print(confusion_matrix(iris_test$Species, predict(f, iris_test, type="class")))
```

A single classification tree model can perform both well in training and testing set for Iris.
Since the dataset is a bit too trivial.
In any real world application,
a single tree is usually not enough to solve the problem.
And this is where we need *more* trees.

## From One Tree to Many Trees

To overcome the shortfall of a single tree,
ensembling technique is used to combine multiple trees into one single model.
There are two very successful ensembling techniques used for tree models:
bagging and boosting.
The general idea is to combine many simple (hence weak) trees as a tree committee for final prediction.
It turns out that the variability in many simple/weak and especially non-linear learners can jointly outperform a single (and usually over-fitting) complicated/strong learner.

### Bagging Ensembles

The most well-known bagging tree model is Random Forest (RF hereafter).
In each single tree in RF the split algorithm is modified to only iterate over a subsample of all features.
This reduces the ability of any single tree to over-fit the training data and create diversity among different trees.
RF also use boostrap sampling on the training dataset when doing feature value iteration.
Rather than search split on full training set it only search on a set of boostrap samples.
Some model variants even perform only a small number of random splits (instead of finding the best split with a full scan) on the feature value iteration phase.

Although it is claimed in theory that RF is strong against over-fitting due to both feature (column) subsampling and boostrap (row) sampling,
in reality RF can still easily over-fit without proper tunning.

One obvious advantage of RF,
in temrs of computational efficiency,
is that it is [embarrassingly parallel](https://en.wikipedia.org/wiki/Embarrassingly_parallel).
Most modern implementation of RF supports single-machine cpu parallel or even distributed computing.

### Boosting Ensembles

Another ensembling technique which has been proven to be even more power empirically is boosting.
We have already discussed the difference between boosting and gradient boosting.
Here we specifically refer to gradient boosting since it usually gives better results and it is our main topic in this notebook.

Unlike RF which trains multiple trees independently and aggregates the results,
gradient boosting trees (GBT hereafter) adopt an additive training procedure,
so each tree is iteratively added into the model.

Similar to RF or any other tree ensembling,
each individual tree is a very weak learner.
But jointly they become very good at generalization.
Boosting trees indeed have achieved quite some of the state-of-the-art performance real world applications.

Even though GBT has been introduced in the literature for almost 20 years already,
it is the recent 5 years that several very powerful open source implementations have emerged to really shape the practitioners' choice of toolbox.
In the following sections we'd like to deep-dive into two of them:
`xgboost` and `lightgbm`.
We will focus especially on their novelty in bettering GBT to the extreme.

# XGBoost

@Chen:2016:XST:2939672.2939785 open source the [`xgboost`](https://github.com/dmlc/xgboost) package,
probably one of the most influential GBT framework to date.

```{r xgb_ver}
print(packageVersion("xgboost"))
```


## Regularization on Impurity

> The name xgboost, though, actually refers to the engineering goal to push the limit of computations resources for boosted tree algorithms.
Which is the reason why many people use xgboost.
For model, it might be more suitable to be called as regularized gradient boosting.
^[Quoted from the creator of xgboost: https://www.quora.com/What-is-the-difference-between-the-R-gbm-gradient-boosting-machine-and-xgboost-extreme-gradient-boosting]

One important and novel feature of XGBoost is to introduce regularization in the impurity measure.
Before this,
most tree ensembles focus on using traditional way of regularization,
such as pruning, subsampling, or limiting number of leaves.

In `xgboost` the formulation of the objective function make its tree ensembles extremely flexible.
Loss can be plug-and-play as well as evaluation metrics.
In general,
the model has the following cost function:

$$
\text{Cost} = \sum_{i=1}^n L(y_i, \hat{y_i}) +
\frac{1}{2}\lambda\vert\vert W \vert\vert^2 +
\alpha\vert\vert W\vert\vert,
$$

where $L(y_i, \hat{y_i})$ is a per-instance loss,
$W$ is the weights of all tree leaves,
$\lambda$ and $\alpha$ are L2 and L1 regularization term,
respectively.

(By default `xgboost` has $\lambda = 1$ and $\alpha = 0$.)

The cost function indeed looks no different than any other differentiable machine learning model we already know.
What makes it novel is that `xgboost` use the cost as the impurity measure to implement the split finding algorithm in a tree,
which is not differentiable.
But how?

### Additive Training

### 2nd-Order Loss Approximation

loss function is approximated by taylor expansion up to 2nd order -> allow plug-and-play any loss function


regularization term -> impurity measure with regularization (this is novel in xgb)
column subsampling
row subsampling (boostrap bagging)
shrinkage for newly added weights

## Approximated Tree Split

The exact split finding algorithm is not scalable in large applications.
So instead XGBoost supports an approximated split algorithm described in the following pseudo code:
^[Approximated split search is not particularly novel in XGBoost.
It has been well studied in other academic works.]

```
[Approximated Tree Split Algorithm]

m: total number of features
s: score to measure the quality of a split
eps: percentile increment, dividing feature roughly into 1/eps buckets
x_pj: the p-th example percentile value on feature j

for feature j in 1 to m:
  divided feature j into buckets based on eps
  aggregate s for each bucket

for feature j in 1 to m:
  for all value x_pj on feature j:
    compute s based on bucketed aggregation

do split using j* at x_pj* associated with maximum s
```

The idea is to split only at percentile value,
introducing one additional parameter called `sketch_eps` (default at 0.03 in `xgboost`) to control the granularity of the buckets.
So the number of search for each continous variable will be capped roughly at 1/`sketch_eps` instead of the number of distinct values,
which can be huge.

By default in `xgboost` the bucketization is done only once at the model initialization phase.
All subsequent splits are performed on the same set of buckets for each feature.
It is also possible to re-propose new buckets after each optimal split,
as pointed out in the original paper,
but such control is not yet open to end user in `xgboost` interface as of `r format(Sys.time(), '%Y-%m-%d')`.

More specifically,
in `xgboost` the parameter `tree_method` controls the type of splitting algorithm:

+ `tree_method="exact"`: The exact splitting algorithm by default when dataset is small
+ `tree_method="approx"`: The approximated splitting algorithm
+ `tree_method="hist"`: The histogram tree approach (refer to [LightGBM section](#lgb) for details)

## Sparsity Awareness: Split on Missing

One novelty in `xgboost` is its capability of dealing with missing values.
It solves the missing value problem by learning a default branch (either the left or the right child node) for each node in a split.
During training,
in each node the missing value can go to either left or right branch.
Qaulity score for both branches are now computed based on the assumption that missing values should go to the other branch.
In the end the split algorithm returns not only optimal split but also optimal default branch for missing value.







out-of-core computation (memory efficient)
process data too large to fit into memory







# LightGBM {#lgb}

@ke2017lightgbm


As of `r format(Sys.time(), '%Y-%m-%d')` `lightgbm` is not yet available on [CRAN](https://cran.r-project.org).
And it can be tricky when installing on Windows machines without access to Visual Studio.
To make the notebook platform friendly here we instead will demo with `lightgbm`'s Python pacakge.

```{python}
import lightgbm

print(lightgbm.__version__)
```


## Histogram Tree Split

## Leaf-Wise Split



also include? DART: Dropouts meet Multiple Additive Regression Trees.
@rashmi2015dart
