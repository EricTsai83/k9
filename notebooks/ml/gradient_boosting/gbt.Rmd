---
title: "Demystify Modern Gradient Boosting Trees"
subtitle: "From Theory to Hands-On Examples"
author:
- name: Kyle Chung
  affiliation:
date: "`r format(Sys.time(), '%d %b %Y')` Last Updated (20 Dec 2019 First Uploaded)"
output:
  html_notebook:
    highlight: tango
    number_sections: yes
    theme: paper
    toc: yes
    toc_depth: 3
    toc_float: yes
    includes:
      in_header: /tmp/meta_header.html
  code_download: true
bibliography: gbt.bib
nocite: |
  @reticulate
  @friedman2001greedy
abstract: |
  TBC.
---

```{r meta, include=FALSE}
meta_header_file <- file("/tmp/meta_header.html")

# Add open graph meta.
meta <- c(
  '<meta name="author" content="Kyle Chung">',
  '<meta property="og:title" content="Demystify Modern Gradient Boosting Trees: From Theory to Hands-On Examples">',
  '<meta property="og:type" content="article">',
  '<meta property="og:url" content="https://everdark.github.io/k9/notebooks/ml/gradient_boosting/gbt.nb.html">',
  '<meta property="og:image" content="https://everdark.github.io/k9/assets/androidify.jpg">',
  '<meta property="og:description" content="A data science notebook about gradient boosting trees.">'
)
contents <- meta

# Add Github corner.
github_corner_svg <- "../../../assets/github_corner.html"
github_corner_conf <- list(github_link="https://github.com/everdark/k9/tree/master/notebooks/ml/gradient_boosting")
contents <- c(contents, stringr::str_interp(readLines(github_corner_svg), github_corner_conf))
writeLines(contents, meta_header_file)

close(meta_header_file)
```

# Motivation

# Gradient Boosting Machines

Gradient boosting is a model ensembling technique.
It combines multiple models into one in an *iterative and additive* manner.
Each individual model is indeed a weak learner,
but jointly they end up providing powerful predictions.
Mathematically speaking,
the model can be expressed as:

$$
\hat{y} = F_k(x) = \sum_k f_k(x),
$$

where $k$ is number of iterations (and hence number of weak learners),
$f_k$ belongs to some model family and $x$ is the feature vector.

A weak learner can be a model with lesser features,
lesser parameters,
or a simpler structure.

## Functional Gradient Descent

Now if we rewrite the above model as:

$$
\begin{aligned}
y
&= F_k(x) + \epsilon \\
&= F_{k-1}(x) + f_k(x) + \epsilon,
\end{aligned}
$$

then we have:

$$
f_k(x) = y - F_{k-1}(x) - \epsilon.
$$

The equation tells us that the function added at round $k$ is fitting the *model residual* of the previous ensembled model ($F_{k-1}$).
If our loss is squared error,
model residual is indeed the negative gradient to the loss w.r.t. the function at the last step (ignoring the constant term):

$$
- \frac{\partial\frac{1}{2}\big[y - F_{k-1}\big]^2}{\partial F_{k-1}} = (y - F_{k-1}).
$$

To generalize the idea,
given any loss function $L(y, F(x))$,
to solve for the optimal function $f_k$ to add at round $k$ we are actually doing *functional gradient descent*:

$$
f_k(x) = - \frac{\partial L(y, F_{k-1}(x))}{\partial F_{k-1}(x)}.
$$

The negative gradient is also referred to as *pseudo-residual*.
When our loss function is not squared error,
we no longer literally fit a model on residuals but on the pseudo-residuals.

## Boosting v.s. Gradient Boosting

It can be easily confused about the name *gradient boosting*.
As we already seen,
graident boosting is indeed a gradient descent algorithm,
but operating at functional space.
That is,
to optimize the objective function we are not looking for parameters but looking for functions.
In the training phase we are iteratively searching for a function (a weak learner) to be included into our model.

*Boosting* is yet another different model ensembling technique,
not to be confused with gradient boosting.
The general idea of boosting is to also train a model iteratively.
In each round the training examples will be re-weighted based on the results (losses) from the last run.
Harder examples will gain higher weights so the subsequent training will shift focus onto them.
The most popular boosting algorithm is probably [AdaBoost](https://en.wikipedia.org/wiki/AdaBoost),
abbreviated from adaptive boosting.

Boosting is similar to gradient boosting in that both are model ensembling with additive weak learners and iterative training.
But they are very different in the way they search for the optimal function at each iteration.
Boosting fits a learner with examples re-weighted by loss generated from the previous ensemble.
Gradient boosting instead fits a learner with pseudo-residuals:
the negative functional gradient w.r.t. the loss.

## Linear Model as Weak Learner

The choice of weak learner has a huge impact on the effectiveness of gradient boosting machines.
Not all weak learners can gain equally from such ensemble.
In this section we demonstrate a model ensemble using a weak linear learner.
It turns out that gradient boosting with a linear model may do no good at all.

Let's create some simulated data baseed on a simple ground-truth (non-linear) data generating process:

```{r sim_data}
library(data.table)

set.seed(666)

# DGP: y = 3 x_1 + 6 x_2 -1.5 x_1x_2 + e
n <- 1000
x1 <- rnorm(n, 2, 4)
x2 <- runif(n)
e <- rnorm(n)
y <- 3 * x1 + 6 * x2 - 1.5 * x1 * x2 + e

# Train-test split.
is_test <- runif(n) < .2
D <- data.table(x1=x1, x2=x2, y=y)
head(D)
```

We fit a linear model with all features (consdier it as a "strong" learner):

```{r sim_data_fit}
# Fit a linear model with all features.
f <- lm(y ~ x1 + x2, data=D[!is_test])
D <- D[, yhat:=predict(f, D)]


# MSE as our loss function.
mse <- function(y, yhat) {
  mean((y - yhat)^2)
}

# Compute testing loss.
with(D[is_test], mse(y, yhat))
```

Now let's ensemble a weak learner which only take one feature.
We train the model based on gradient boosting framework.
Since our loss is squared error,
we can simply fit the model residual of the previous ensemble:

```{r gbm_linear_learner}
f1 <- lm(y ~ x1, data=D[!is_test])
D <- D[, yhat1:=predict(f1, D)]
D <- D[, resid1:=y - yhat1]

with(D[is_test], mse(y, yhat1))

f2 <- lm(resid1 ~ x2, data=D[!is_test])
D <- D[, yhat2:=predict(f2, D)]
D <- D[, resid2:=y - yhat1 - yhat2]

with(D[is_test], mse(y, yhat1 + yhat2))

f3 <- lm(resid2 ~ x1, data=D[!is_test])
D <- D[, yhat3:=predict(f3, D)]
D <- D[, resid3:=y - yhat1 - yhat2 - yhat3]

with(D[is_test], mse(y, yhat1 + yhat2 + yhat3))

f4 <- lm(resid3 ~ x2, data=D[!is_test])
D <- D[, yhat4:=predict(f4, D)]
D <- D[, resid4:=y - yhat1 - yhat2 - yhat3 - yhat4]

with(D[is_test], mse(y, yhat1 + yhat2 + yhat3 + yhat4))
```

As one may realize,
the boosted model won't outperform a simple linear model with all features included.
Is it because of our naive built-from-scratch implementation?
Not really.
Let's use `xgboost`--one of the state-of-the-art gradient boosting frameworks--to train a GBM with linear learner:

```{r xgb_linear_learner}
library(xgboost)

Dx <- xgb.DMatrix(as.matrix(D[!is_test, .(x1, x2)]), label=D[!is_test, y])
Dt <- xgb.DMatrix(as.matrix(D[is_test, .(x1, x2)]), label=D[is_test, y])

bst <- xgb.train(params=list(objective="reg:squarederror", booster="gblinear"), 
                 data=Dx, watchlist=list(test=Dt),
                 nround=50, early_stopping_rounds=5, verbose=0)

yhat <- predict(bst, as.matrix(D[is_test, .(x1, x2)]))
mse(D[is_test, y], yhat)
```

The result is roughly the same.

In general,
gradient boosting does not help imporving a linear model.
This is by large due to the fact that GBM is an additive model.
Combining linear models additively will just result in yet another linear model.
Indeed,
using a linear model as the base learner will have the effect of solving a full linear system (a model with all features) in an iterative and approximated manner.
So at best the result is the same as a single linear model with all features.

## Tree Model as Weak Learner

Based on empirical findings,
the best weak learner in GBM is a tree.
Most of the gradient boosting (or just boosting) implementations use tree models as their base learner.

For our simulated data,
gradient boosted trees can easily outperform the linear model:

```{r xgb_tree_learner}
bst2 <- xgb.train(params=list(objective="reg:squarederror", booster="gbtree"), 
                  data=Dx, watchlist=list(test=Dt),
                  nround=50, early_stopping_rounds=5, verbose=0)

yhat <- predict(bst2, as.matrix(D[is_test, .(x1, x2)]))
mse(D[is_test, y], yhat)  # Smaller loss achieved.
```

For the rest of the notebook we will focus our discussion on tree-based gradient boosting.

# Tree Ensembles

Before we talk about gradient boosting trees,
let's have a brief review on tree models in general,
from a single tree to tree ensembles.

Tree models are non-differentiable.
Gradient-based optimizers won't directly work on tree models.
This makes them very different from other machine learning models such as neural networks.

Tree models are powerful because they are non-linear.
Features are allowed to interact with each other in a sequential and loss-guided manner.
And they are invariant to feature scale since a tree split is only ordinal.
This simplifies data preprocessing and hence the overall pipeline.

## A Single Tree

The idea of a single tree model is rather simple.
The model uses a split finding algorithm to enumerate over all the possible splits on all the features.
Once a best split is found based on a particular measure,
the model move on to find the next split given the current node,
growing the tree deeper and deeper until a certain criterion is met.

Here is a general pseudo code describing a single split algorithm in its *exact* form:

```
[Exact Tree Split Algorithm]

m: total number of features
n: total number of examples
s: score to measure the quality of a split
x_ij: the i-th example value on feature j

for feature j in 1 to m:
  sort x_j
  for example x_ij in 1 to n on feature j:
    compute s

do split using j* at x_ij* associated with maximum s
```

A Tree model involves multiple *sequential* splits to arrive at the terminal node which gives prediction.
Here are several important components that must be taken care for a detailed implementation:

#### Feature Iteration {-}

The algorithm is greedy.
To search for the optimal split all features are traversed.
The immediate consequence is the over-fitting nature of a single tree.
And it is such difficulty in generalization leads to the idea of tree ensembles.

#### Feature Value Iteration {-}

For each feature,
a sorting operation is required to traverse over the (distinct) sorted values of that feature.
The more values a feature can take the more computing resources are needed to find the best split.

#### Quality of Split Measure {-}

This is probably the most imnportant part of a split algorithm.
It directly affects the accuracy of the resulting tree.
The quality score measures how good a node is splitting the examples.
In the literature this is often refered to as *impurity*.
A classical impurity measure is Gini index for classification and residual sum of squares for regression.

#### Depth of the Tree {-}

How deep (how many splits) should we grow the tree?
This is a hyper-parameter that can affect model generalization.
For classification we can grow a tree as deep as until every node contains only one single example.
That is,
until no split can be further achieved.
A tree that is too deep will definitely suffer from over-fitting.

A single tree is usually very bad at generalization.
The only benefit is probably its *interpretability* since the tree structure itself is a set of human friendly decision rules.
However due to its poor generalization over unseen data,
such interpretability is practically of no use.

### A Regression Tree

Continue with our simulated dataset,
let's quickly build a single regression tree and evaluate its performance on the testing set.

```{r sim_data_tree_train}
library(rpart)  # Recursive PARTitioning 
library(rpart.plot)

f <- rpart(y ~ x1 + x2, data=D[!is_test], method="anova", model=TRUE)
rpart.plot(f, digits=3)  # For each node the mean value and population are shown.
```

The testing score is worse than a linear model:

```{r sim_data_tree_eval}
mse(D[is_test, y], predict(f, D[is_test]))
```

Even though our true model is non-linear,
a tree failed to outperform a linear model.
Does that make sense?
Our tree may be too simple to perform a task on predicting a continous value.
The tree is only capable of predicting 9 distinct values,
which make it difficult to compete with a linear model on a continous metric such as squared error.

Indeed it is very easy to build a more complicated (less regulated) tree to outperform the linear model:

```{r sim_data_better_tree}
f2 <- rpart(y ~ x1 + x2, data=D[!is_test], method="anova", model=TRUE,
            control=rpart.control(minsplit=10, cp=5e-4))
uniqueN(f2$where)  # How many terminal nodes?
mse(D[is_test, y], predict(f2, D[is_test]))
```

Put aside regularization and model complexity,
a common quality of split measure for a regression tree is the sum of squares $SS = \sum (y_i -\bar{y})^2$.
So for a given node we find a split such that the decrease in sum-of-squares $SS_T - (SS_L + SS_R)$ is the largest.
$SS_T$ is the $SS$ of current node,
$SS_R$ is $SS$ of the right-hand node resulted from the split,
$SS_L$ the $SS$ of left-hand node.
Note that the prediction of a regression tree node is its mean value.
Hence $SS$ is the squared error of that node.

### A Classification Tree

We use the old-school [UCI Iris](https://archive.ics.uci.edu/ml/datasets/iris) dataset to quickly demo a single classification tree.

```{r rpart_train}
confusion_matrix <- function(y_true, y_pred) {
  table(y_true, y_pred)
}

# Split IRIS dataset into train-test.
is_test_iris <- runif(nrow(iris)) < .3
iris_train <- iris[!is_test_iris,]
iris_test <- iris[is_test_iris,]

# Grow a single classification tree.
f <- rpart(Species ~ ., data=iris_train, method="class")
rpart.plot(f)
```

```{r rpart_eval_train}
# Confusion matrix for the training set.
print(confusion_matrix(iris_train$Species, predict(f, iris_train, type="class")))
```

```{r rpart_eval_test}
# Confusion matrix for the testing set.
print(confusion_matrix(iris_test$Species, predict(f, iris_test, type="class")))
```

A single classification tree model can perform both well in training and testing set for Iris.
Since the dataset is a bit too trivial.
In any real world application,
a single tree is usually not enough to solve the problem.
And this is where we need *more* trees.

## From One Tree to Many Trees

To overcome the shortfall of a single tree,
ensembling technique is used to combine multiple trees into one single model.
There are two very successful ensembling techniques used for tree models:
bagging and boosting.
The general idea is to combine many simple (hence weak) trees as a tree committee for final prediction.
It turns out that the variability in many simple/weak and especially non-linear learners can jointly outperform a single (and usually over-fitting) complicated/strong learner.

### Bagging Ensembles

The most well-known bagging tree model is Random Forest (RF hereafter).
In each single tree in RF the split algorithm is modified to only iterate over a subsample of all features.
This reduces the ability of any single tree to over-fit the training data and create diversity among different trees.
RF also use boostrap sampling on the training dataset when doing feature value iteration.
Rather than search split on full training set it only search on a set of boostrap samples.
Some model variants even perform only a small number of random splits (instead of finding the best split with a full scan) on the feature value iteration phase.

Although it is claimed in theory that RF is strong against over-fitting due to both feature (column) subsampling and boostrap (row) sampling,
in reality RF can still easily over-fit without proper tunning.

One obvious advantage of RF,
in terms of computational efficiency,
is that it is [embarrassingly parallel](https://en.wikipedia.org/wiki/Embarrassingly_parallel).
Most modern implementation of RF supports single-machine cpu parallel or even distributed computing.

### Boosting Ensembles

Another ensembling technique which has been proven to be even more power empirically is boosting.
We have already discussed the difference between boosting and gradient boosting.
Here we specifically refer to gradient boosting since it usually gives better results and it is our main topic in this notebook.

Unlike RF which trains multiple trees independently and aggregates the results,
gradient boosting trees (GBT hereafter) adopt an additive training procedure,
so each tree is iteratively added into the model.

Similar to RF or any other tree ensembling,
each individual tree is a very weak learner.
But jointly they become very good at generalization.
Boosting trees indeed have achieved quite some of the state-of-the-art performance in real world applications.

Even though GBT has been introduced in the literature for almost 20 years already,
it is the recent 5 years that several very powerful open source implementations have emerged to really shape the practitioners' choice of toolbox.
In the following sections we'd like to deep-dive into two of them:
`xgboost` and `lightgbm`.
We will focus especially on their novelty in bettering GBT to the extreme.

# XGBoost

@Chen2016 open source the [`xgboost`](https://github.com/dmlc/xgboost) package,
probably one of the most influential GBT framework to date.
It also comes with the most variety of langauge API support:
Python, R, Julia, Java, ..., etc.
It is also more or less integrated with distributed computing platform such as Spark,
and cloud solution such as GCP and AWS.

```{r xgb_ver}
print(packageVersion("xgboost"))
```

## Regularization on Impurity

> The name xgboost, though, actually refers to the engineering goal to push the limit of computations resources for boosted tree algorithms.
Which is the reason why many people use xgboost.
For model, it might be more suitable to be called as regularized gradient boosting.
^[Quoted from the creator of xgboost: https://www.quora.com/What-is-the-difference-between-the-R-gbm-gradient-boosting-machine-and-xgboost-extreme-gradient-boosting]

One important and novel feature of XGBoost is to introduce regularization in the impurity measure.
Before this,
most tree ensembles focus on using traditional way of regularization,
such as pruning, subsampling, or limiting number of leaves.

In `xgboost` the formulation of the objective function makes its tree ensembles extremely flexible.
Loss can be plug-and-played as well as evaluation metrics.
In general,
the model has the following cost function:

$$
\text{Cost} = \sum_{i=1}^n L(y_i, \hat{y_i}) +
\frac{1}{2}\lambda\vert\vert W \vert\vert^2 +
\alpha\vert\vert W\vert\vert,
$$

where $L(y_i, \hat{y_i})$ is a per-instance loss,
$W$ is the weights of all tree leaves,
$\lambda$ and $\alpha$ are L2 and L1 regularization term,
respectively.

(By default `xgboost` has $\lambda = 1$ and $\alpha = 0$.)

The cost function indeed looks no different than any other differentiable machine learning model we already know.
What makes it novel is that `xgboost` uses the cost as the impurity measure to implement the split finding algorithm in a tree,
which is not differentiable.
But how?

## Additive Training

For convenience let's re-state the gradient boosting model function here,
with a bit more detailed notations:

$$
\begin{aligned}
\hat{y}^{(k)} = F_k(x)
&= \sum_k f_k(x) \\
&= \sum_{t=1}^{k-1}f_t(x) + f_k(x),
\end{aligned}
$$

where $\hat{y}^{(k)}$ is the model prediction for model at round $k$.

Define the loss function (without regularization term for now for simplicity) at round $k$ for $i = 1, ..., n$ examples to be:

$$
\begin{aligned}
\text{Loss} &= \sum_{i=1}^n L(y_i, \hat{y_i}^{(k)}) \\
&= \sum_{i=1}^n L(y_i, \hat{y_i}^{(k-1)} + f_k(x_i)).
\end{aligned}
$$

Here $\hat{y_i}^{(k-1)}$ is purely determined by model already trained in the previous round,
we only care about picking up a tree model $f_k(x_i)$ which can further reduce the loss the most.

When the loss is a squared error,
the function is somewhat trackable:

$$
\begin{aligned}
\text{MSE-Loss} &= \frac{1}{n}\sum_{i=1}^n (y_i - \hat{y_i}^{(k)})^2 \\
&= \frac{1}{n}\sum_{i=1}^n \bigg[
\underbrace{(y_i - \hat{y}^{(k-1)})^2}_\text{Constant}
- 2(y_i - \hat{y_i}^{(k-1)})f_k(x_i)
+ f_k(x_i)^2\bigg] \\
&= \frac{1}{n}\sum_{i=1}^n \bigg[
- 2\underbrace{(y_i - \hat{y_i}^{(k-1)}}_\text{Residual at k-1})f_k(x_i)
+ f_k(x_i)^2\bigg] + \text{Constant}.
\end{aligned}
$$

But when the loss is, say, a cross-entropy loss (which is fairly common),
the function becomes much harder to deal with.

## Second-Order Loss Approximation

In order to allow a general plug-and-play framework for different loss functions,
instead of solving the exact functional form of a particular loss,
`xgboost` approximates the loss with a second-order [Taylor series]((https://en.wikipedia.org/wiki/Taylor_series)) expansion.

The 2nd-order expansion of a function $L(x)$ at point $x_0$ can be written as:

$$
L(x) \approx L(x_0) + L'(x_0)(x - x_0) + \frac{1}{2}L''(x_0)(x - x_0)^2.
$$

Now define:

$$
x - x_0 = a,
$$

we then have:

$$
L(x_0 + a) \approx L(x_0) + L'(x_0)a + \frac{1}{2}L''(x_0)a^2,
$$

or simply:

$$
L(x + a) \approx L(x) + L'(x)a + \frac{1}{2}L''(x)a^2.
$$

Use the form we can re-write our loss function:

$$
\begin{aligned}
\text{Loss}
&= \sum_{i=1}^n L(y_i, \hat{y_i}^{(k)}) \\
&= \sum_{i=1}^n
L(y_i, \hat{y_i}^{(k-1)} + f_k(x_i)), \\
\text{Approx. Loss} &= \sum_{i=1}^n
\Bigg[
\underbrace{
\vphantom{\frac{\partial L(y_i, \hat{y_i}^{(k-1)})}{\partial \hat{y_i}^{(k-1)}}}
L(y_i, \hat{y_i}^{(k-1)})}_\text{Constant} +
\underbrace{\frac{\partial L(y_i, \hat{y_i}^{(k-1)})}{\partial \hat{y_i}^{(k-1)}}}_{g_i}f_k(x_i) +
\underbrace{\frac{\partial^2 L(y_i, \hat{y_i}^{(k-1)})}{\partial^2 \hat{y_i}^{(k-1)}}}_{h_i}f_k(x_i)^2
\Bigg] \\
&= \sum_{i=1}^n
\Bigg[
g_if_k(x_i) +
h_if_k(x_i)^2
\Bigg] + \text{Constant}.
\end{aligned}
$$

We follow the author's notation on the first order term as $g_i$ (g for gradient) and the second order term as $h_i$ (h for [Hessian](https://en.wikipedia.org/wiki/Hessian_matrix)).
Together with the regularization term it becomes the quality of split measure which ultimately guides the model to pick up the next learner $f_k$ at round $k$.

Essentially XGBoost employs a functional gradient descent up to the second-order derivative.
This makes it more precise in finding the next best learner.

Note that the value of $g_i$ and $h_i$ both remain the same for the current round,
meaning that the computing cost is largely reduced when we are evaluating among different tree splits.
This is one of the reason why XGBoost is so much faster than any other vanilla implementation of GBT.

## Tree Split Optimization

Now let's formally parameterize a tree model $f_k(x_i)$ with $T$ terminal nodes (leaves),
$w_j$ denotes the weight for $j$-th leaf,
and $I_j$ the set of examples assigned to leaf $j$.
The approximated loss (with regularization term this time) at round $k$ can then be re-written in a *group-by-leaf* summation:

$$
\begin{aligned}
Cost^k &=
\sum_{i=1}^n
\Bigg[
g_if_k(x_i) + h_if_k(x_i)^2
\Bigg] +
\frac{1}{2}\lambda\vert\vert W \vert\vert^2 +
\alpha\vert\vert W\vert\vert \\
&= \sum_{j=1}^T
\Bigg[
\sum_{i \in I_j} g_iw_j + \frac{1}{2}\sum_{i \in I_j}h_iw_j^2
\Bigg] +
\frac{1}{2}\lambda\sum_{j=1}^T w_j^2 + \alpha\sum_{j=1}^T w_j \\
&= \sum_{j=1}^T
\Bigg[
\sum_{i \in I_j} g_iw_j + \frac{1}{2}(\sum_{i \in I_j}h_i + \lambda)w_j^2 + \alpha\vert w_j\vert
\Bigg].
\end{aligned}
$$

Given the cost function we can solve for optimal weight $w_j^*$ minimizing the cost with a first-order-condition:

$$
\frac{\partial Cost^k}{\partial w_j} = 0,
$$

which gives:

$$
\sum_{i \in I_j} g_i + (\sum_{i \in I_j}h_i + \lambda)w_j + \alpha\frac{w_j}{\vert w_j\vert} = 0.
$$

For a null L1 penalty ($\alpha = 0$) the solution is simply:

$$
\begin{aligned}
w_j^* &= - \frac{\sum_{i \in I_j} g_i}{\sum_{i \in I_j}h_i + \lambda}, \\
Cost^{k*} &= - \frac{1}{2} \sum_{j=1}^T \frac{\big(\sum_{i \in I_j} g_i\big)^2}{\sum_{i \in I_j}h_i + \lambda}.
\end{aligned}
$$

Now we have solved the weights that minimize the cost once $T$ is determined.
We will also need the optimized cost to determine the best split.
That is,
the quality of split measure.
The best split should result in two nodes such that the total cost of two nodes is reduced the most from that of their parent node.

Here is a pseudo code describing the core algorithm:

```
[XGBoost Additive Training Algorithm]

F: initial model
M: total number of features
N: total number of examples
K: total number of iterations (tree models)
T: total number of leaves
g_ik: g_i for example i at round k
h_ik: h_i for example i at round k
g_ijk: g_i for example i at round k assigned to leaf j
h_ijk: h_i for example i at round k assigned to leaf j
cost: the score at leaf j, sum(g_ijk)^2 / (sum(h_ijk) + lambda)
x_im: the i-th example value on feature m

for round k in 1 to K:
  while number of split < T:
    compute g_ik, h_ik for all i
    for feature j in 1 to m:
      sort x_j
      for example x_im in i = 1 to n on feature m:
        split by x_im into left and right leaves
        compute sum of cost on left and right leaves
    find best split using feature m* at x_im* associated with minimum cost
  update model F <- F + tree learned
```

### Common Loss Functions

#### Squared Loss {-}

$$
\begin{aligned}
g_i &= \frac{\partial(y_i - \hat{y}_i^{(k-1)})^2}{\partial\hat{y}_i^{(k-1)}} = 2(\hat{y}_i^{(k-1)} - y_i), \\
h_i &= \frac{\partial^2(y_i - \hat{y}^{(k-1)})^2}{\partial^2\hat{y}^{(k-1)}} = 2.
\end{aligned}
$$

We can ignore constant so the gradient $g_i$ is simply $\hat{y} - y$ and the hessian is unity.

Let's use R's *symbolic derivatives* to verify our derivation:

```{r symbolic_deriv_squared_loss}
# Demonstrate symbolic derivatives.
dy2y <- deriv3(~ (y - yhat)^2, c("yhat", "y"))  #  (y - yhat)^2, since y is constant we simply use 1.
yhat <- rnorm(5)
y <- rnorm(5)

2*(yhat - y)  # Gradient by hands.

eval(dy2y)  # By symbolic derivatives.
```

#### Cross-Entropy Loss {-}

$$
\begin{aligned}
g_i &= \frac{\partial\big[- y_i\ln\hat{y}_i^{(k-1)} - (1 - y_i)\ln(1 - \hat{y}_i^{(k-1)})\big]}
{\partial\hat{y}_i^{(k-1)}} =
\frac{\hat{y}_i^{(k-1)} - y_i}{(1 - \hat{y}_i^{(k-1)})\hat{y}_i^{(k-1)}}, \\
h_i &= \frac{\partial^2\big[- y_i\ln\hat{y}_i^{(k-1)} - (1 - y_i)\ln(1 - \hat{y}_i^{(k-1)})\big]}
{\partial^2\hat{y}_i^{(k-1)}} = \frac{1 - y_i}{(1 - \hat{y}_i^{(k-1)})^2} + \frac{y_i}{(\hat{y}_i^{(k-1)})^2}.
\end{aligned}
$$

```{r symbolic_deriv_log_loss}
# Demonstrate symbolic derivatives.
dq2q <- deriv3(~ - (y*log(q) + (1-y)*log(1 - q)), c("q", "y"))
y <- sample(c(0, 1), 5, replace=TRUE)
q <- runif(5)

(1 - y)/(1 - q) - y / q  # Gradient by hands.

(1 - y)/(1-q)^2 + y / q^2  # Hessian by hands.

eval(dq2q)  # By symbolic derivatives.
```

The above derivation assumes $\hat{y}$ is the sigmoid model output (probability).
While actually in `xgboost`'s implementation for logistic regression the term $\hat{y}$ is *before* sigmoid transformation (what the auther refers to as *margin*).
So the model weights are raw scores.
This makes sense since the model is additive.
It is better to do the add operation before the final sigmoid transformation.
That is,
each tree predict raw scores and the final model ensemble is the sigmoid of the sum of all the raw scores (from the assigned leaves).
This also largely simplifies the derivatives because now we will be doing:

$$
g_i = \frac{\partial L}{\partial t}\frac{\partial t}{\partial\hat{y}},
$$

where $t$ is the sigmoid $t(\hat{y}) = \frac{1}{1 + e^{-\hat{y}}}$.

Based on the fact that $\frac{\partial t}{\partial \hat{y}} = t(1 - t)$,
we finally have (simplying $\hat{y}_i^{(k-1)}$ as $\hat{y}$ to save typings):

$$
\begin{aligned}
g_i &= \frac{t - y_i}{t(1 - t)} \times t(1-t) \\
&= t(\hat{y}) - y_i, \\
h_i &= t(\hat{y})(1 - t(\hat{y})).
\end{aligned}
$$

### Exercise: Implement a Single XGB Split

```{r xgb_from_scratch}
lambd <- 1  # L2 regularization.

dy2y <- deriv3(~ (y - yhat)^2, c("yhat", "y"))  #  (y - yhat)^2, since y is constant we simply use 1.
all_features <- c("x1", "x2")

cost <- function(df) {
  # The XGB optimal cost based on 2nd-order Taylor expansion.
  - sum(df$g)^2 / 2*(sum(df$h) + lambd)
}

cost_on_split <- function(D, split_ind) {
  # Split the data into left and right child nodes.
  DL <- D[1:split_ind, .(g, h)]
  DR <- D[(split_ind + 1):nrow(D), .(g, h)]
  cost(DL) + cost(DR)
}

feature_iteration <- function(D, all_features) {
  feature_best_costs <- list()
  best_split_ind <- list()
  for ( x in all_features ) {
    # Sort examples by x.
    setorderv(D, x)

    # Feature value iteration.
    cur_cost <- cost(D)
    split_cost <- numeric(nrow(D) - 1)
    for ( i in seq_len(nrow(D) - 1) ) {
      split_cost[i] <- cost_on_split(D, i)
    }
    feature_best_costs[[x]] <- min(split_cost)
    best_split_ind[[x]] <- which.min(split_cost)
  }

  # Determine which feature gains the largest reduction in cost.
  best_cost <- min(unlist(feature_best_costs))
  split_col <- names(which.min(feature_best_costs))
  split_ind <- best_split_ind[[split_col]]
  split_val <- D[[split_col]][split_ind]
  list(split_col=split_col,
       split_ind=split_ind,
       split_val=split_val,
       best_cost=best_cost)
}


y <- D$y
yhat <- 0
dy <- eval(dy2y)
D[, g:=attr(dy, "gradient")[,"yhat"]]
D[, h:=attr(dy, "hessian")[,"yhat","yhat"]]

# Feature iteration on the first layer split.
r <- feature_iteration(D, all_features)
DR <- D[get(r$split_col) > r$split_val]
DL <- D[get(r$split_col) <= r$split_val]
```



```{r xgb_tree_learner_verify}
# Verify the result with a real xgboost implementation.
bst3 <- xgb.train(params=list(objective="reg:squarederror", max_depth=1),
                  data=Dx, nround=1, verbose=0)


```


### Parallelization


### V.S. Vanilla Gradient Boosting Trees

**TODO: discuss the novelty of xGB in tree split.**

### With L1 Regularization

For non-zero $\alpha$:

$$
\begin{aligned}
w_j^* &=\frac{-\alpha - \sum_{i \in I_j} g_i}{\sum_{i \in I_j}h_i + \lambda}, \\
&\text{or} \\
w_j^* &=\frac{\alpha - \sum_{i \in I_j} g_i}{\sum_{i \in I_j}h_i + \lambda}.
\end{aligned}
$$

## Approximated Tree Split

The exact split finding algorithm is not scalable in large applications.
So instead XGBoost supports an approximated split algorithm described in the following pseudo code:
^[Approximated split search is not particularly novel in XGBoost.
It has been well studied in other academic works.]

```
[Approximated Tree Split Algorithm]

m: total number of features
s: score to measure the quality of a split
eps: percentile increment, dividing feature roughly into 1/eps buckets
x_pj: the p-th example percentile value on feature j

for feature j in 1 to m:
  divided feature j into buckets based on eps
  aggregate s for each bucket

for feature j in 1 to m:
  for all value x_pj on feature j:
    compute s based on bucketed aggregation

do split using j* at x_pj* associated with maximum s
```

The idea is to split only at percentile value,
introducing one additional parameter called `sketch_eps` (default at 0.03 in `xgboost`) to control the granularity of the buckets.
So the number of search for each continous variable will be capped roughly at 1/`sketch_eps` instead of the number of distinct values,
which can be huge.

By default in `xgboost` the bucketization is done only once at the model initialization phase.
All subsequent splits are performed on the same set of buckets for each feature.
It is also possible to re-propose new buckets after each optimal split,
as pointed out in the original paper,
but such control is not yet open to end user in `xgboost` interface as of `r format(Sys.time(), '%Y-%m-%d')`.

More specifically,
in `xgboost` the parameter `tree_method` controls the type of splitting algorithm:

+ `tree_method="exact"`: The exact splitting algorithm by default when dataset is small
+ `tree_method="approx"`: The approximated splitting algorithm
+ `tree_method="hist"`: The histogram tree approach (refer to [LightGBM section](#lgb) for details)

## Sparsity Awareness: Split on Missing

One novelty in `xgboost` is its capability of dealing with missing values.
It solves the missing value problem by learning a default branch (either the left or the right child node) for each node in a split.
During training,
in each node the missing value can go to either left or right branch.
Qaulity score for both branches are now computed based on the assumption that missing values should go to the other branch.
In the end the split algorithm returns not only optimal split but also optimal default branch for missing value at each node.

`xgboost` is indeed not the first tree model to handle missing values in its own way.
For example,
`rpart` also handles missing value in a particular manner.
In `rpart`,
each split will come with additional surrogate variables to handle cases where the split feature value is missing at inference time.
A surrogate variable is a feature other than the best split feature but used to classify the resulting child nodes passing a minimum quality criterion.
For more details readers may refer to @rpart.

Comparing to `rpart`'s surrogate variable approach,
which increases computation cost,
`xgboost`'s approach is more computationally efficient and seems can also result in better model accuracy.

## Out-of-Core Computation

`xgboost` is memory efficient.
It is able to process data too large to fit into memory.


column subsampling
row subsampling (boostrap bagging)
shrinkage for newly added weights


# LightGBM {#lgb}

@ke2017lightgbm


As of `r format(Sys.time(), '%Y-%m-%d')` `lightgbm` is not yet available on [CRAN](https://cran.r-project.org).
And it can be tricky when installing on Windows machines without access to Visual Studio.
To make the notebook platform friendly here we instead will demo with `lightgbm`'s Python pacakge.

```python
import lightgbm

print(lightgbm.__version__)
```

**TODO: Make this nb R-only for simplicity.**

```{r import_lgb}
library(lightgbm)
packageVersion("lightgbm")
```

## Histogram Tree Split

## Leaf-Wise Split



also include? DART: Dropouts meet Multiple Additive Regression Trees.
@rashmi2015dart
