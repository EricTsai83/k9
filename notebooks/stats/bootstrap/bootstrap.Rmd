---
title: "Bootstrap Sampling 101"
subtitle: ""
author:
- name: Kyle Chung
  affiliation:
date: "`r format(Sys.time(), '%d %b %Y')` Last Updated (10 Jan 2020 First Uploaded)"
output:
  html_notebook:
    highlight: tango
    number_sections: yes
    theme: paper
    toc: yes
    toc_depth: 3
    toc_float: yes
    includes:
      in_header: /tmp/meta_header.html
  code_download: true
bibliography: bootstrap.bib
nocite: |
  @boot2
link-citations: yes
abstract: |
  TBC.
---

```{r meta, include=FALSE}
meta_header_file <- file("/tmp/meta_header.html")

# Add open graph meta.
meta <- c(
  '<meta name="author" content="Kyle Chung">',
  '<meta property="og:title" content="Bootstrap Sampling">',
  '<meta property="og:type" content="article">',
  '<meta property="og:url" content="https://everdark.github.io/k9/notebooks/stats/bootstrap/bootstrap.nb.html">',
  '<meta property="og:image" content="https://everdark.github.io/k9/assets/androidify.jpg">',
  '<meta property="og:description" content="A data science notebook about bootstrap sampling.">'
)
contents <- meta

# Add Github corner.
github_corner_svg <- "../../../assets/github_corner.html"
github_corner_conf <- list(github_link="https://github.com/everdark/k9/tree/master/notebooks/stats/bootstrap")
contents <- c(contents, stringr::str_interp(readLines(github_corner_svg), github_corner_conf))
writeLines(contents, meta_header_file)

close(meta_header_file)
```

# Statistical Inference

At the core of statistical inference is about to use an estimator $\hat{\theta}(\cdot)$ to guess the unknown value of $\theta$ a population parameter of interest.
The most common case of $\theta$ will be the population mean $\mu = E(x)$ where $x$ denotes the population following an unknown distribution $x \sim F$.
Other popular parameters can be a median,
a confidence interval of the mean,
or a regression model coefficient,
a confidence interval of the regression coefficient, ..., etc.

How do we know our guess is a good one?
How do we measure the error of our estimator in statistical inference,
without knowing the ground truth of the population?
The probability distribution function (PDF) of population $F$ is in general unknown.
As a result,
to make our statistical inference from a sample dataset we rely on the [Central Limit Theorem](https://en.wikipedia.org/wiki/Central_limit_theorem).
In its vanilla version CLT only requires the population to have finite second moment and our sample dataset is identically and independently distributed (i.i.d.).
It allows us to derive the [limiting distribution](https://en.wikipedia.org/wiki/Asymptotic_distribution) of a sample mean,
which in turn can be used to measure the error of our guess $\hat{\theta}$ to the unknown population parameter $\theta$.

Different versions of CLT can be formulated based on the estimator we are using.
So we also have CLT applicable to, say, a regression model coefficient.
The problem is that,
we may not have analytical solution for every possible estimators.
The more complicated the estimator the harder it can be analyzed in a trackable way.
And some estimators can be harder to deal with even if they are simple in their own.
One such example is the median,
which requires order statistics come to play.

This is where bootstrap sampling starts to shine.

## Empirical Distribution

From [Wikipedia](https://en.wikipedia.org/wiki/Empirical_distribution_function):

>In statistics,
an empirical distribution function is the distribution function associated with the empirical measure of a sample.
This cumulative distribution function is a step function that jumps up by 1/n at each of the n data points.
Its value at any specified value of the measured variable is the fraction of observations of the measured variable that are less than or equal to the specified value.

Specifically,
the empirical distribution function (EDF) and empirical PDF of a sample $x \sim F$ of size $n$ can be written as:

$$
\begin{aligned}
\hat{F}(x) &= \frac{1}{n} \sum_{i=1}^nI(x_i \leq x), \\
\hat{p}(x) &= \frac{d\hat{F}}{dx} = \frac{1}{n},
\end{aligned}
$$

where $I$ is an indicator function.

Intuitively and loosely speaking,
$\hat{F}$ is just an estimator for $F$ the true population distribution.
But here we are not guesing a single population parameter but the entire distribution.
That is,
we are not guesing a scalar but a function.
^[Here our wording is a bit *Fisherian*.
Because in a Baysian point of view even a population parameter is a function (a random variable),
not a constant scalar.]

## Parameters as Functions of CDF

Why introducing the notion of EDF?
To see the value,
it's better to step back and examine our best-friend parameter:
population mean $E(x)$.
By definition the derivative of a CDF $F$ is a PDF $p$:

$$
\frac{dF(x)}{dx} = p(x).
$$

Now we observe that the population mean can be re-written as:

$$
E(x) = \int xp(x)dx = \int xdF(x).
$$

That is to say,
population mean is indeed *a function of CDF*.
We can easily extend this to the variance as a function of CDF as well,
and to many other statistics.

Of course we don't know CDF of the population.
But we do know the EDF from our observed sample dataset,
which itself is an estimator for CDF.
This immediately gives us one estimator of the form:

$$
\begin{aligned}
E(x) \approx \int xd\hat{F}(x)
&= \int x\hat{p}(x)dx \\
&= \sum_{i=1}^n x \cdot \frac{1}{n} \\
&= \frac{1}{n}\sum_{i=1}^nx_i,
\end{aligned}
$$

to guess the population mean.
^[This is based on the [plug-in principle](https://en.wikipedia.org/wiki/Plug-in_principle).]

To no surprise,
this estimator is exactly what we call the *sample mean*,
proven to be a very good estimator for the population mean.
But the idea generalizes to *any* estimator,
as long as the estimator can be expressed as a function of CDF,
which in turn will be approximated by EDF.
In the literature such estimators are also referred to as *statistical functionals*.

Up to now we've roughly reviewed all we need to initiate the journey of bootstrap sampling,
or simply bootstrapping.

# Bootstrapping

@efron1992bootstrap propose the idea of bootstrapping as a solution to estimate the variance (or the confidence interval) of a given estimator,
such that we can draw conclusion about the quaility of our inference,
especially when there is no analytical solution to derive the variance of that estimator.

Bootstrapping can be simply defined as drawing i.i.d. sample of the same size $n$ from a given empirical distribution.
In some literatures this is also called the *resample*.
To strictly follow i.i.d. the sampling must be done *with replacement*.

To do bootstrap we essentially take the random sample $X$ from the original population $F$,
and do the i.i.d. sampling from that sample *as if* the sample itself is another population $\hat{F}$.
Only in this time the population is finite and has a discrete distribution function,
which is the EDF of $X$.

## Monte Carlo Bootstrapping

Now given a particular estimator $\hat{\theta}$ as a random variable,
we are concerning about its expectation and variance.
The expectation tells us whether the estimator is *unbiased* or not.
The variance tells us how volatile our guess is,
and also if it can be *consistent*--converge to the true parameter of interest.

By definition,
the variance of a random variable $\hat{\theta}$ is:

$$
\begin{aligned}
Var(\hat{\theta})
&= E\big[(\hat{\theta} - E(\hat{\theta}))^2\big] \\
&= E(\hat{\theta}^2) - E(\hat{\theta})^2.
\end{aligned}
$$

Now to approximate the above result with Monte Carlo of $M$ repetitions,
we can simply do:

$$
Var(\hat{\theta}) \approx
\frac{1}{M}\sum_m \hat{\theta}_m^2 -
\bigg(\frac{1}{M}\sum_m \hat{\theta}_m\bigg)^2 ,
$$

where each $\hat{\theta}_m$ is derived from a bootstrapped sample at the $m$-th repetition.

The above approximation holds thanks to the [Law of Large Numbers](https://en.wikipedia.org/wiki/Law_of_large_numbers) (as $M \rightarrow \infty$):

$$
\begin{aligned}
\frac{1}{M}\sum_m \hat{\theta}_m
&\overset{p}\rightarrow E(\hat{\theta}), \\
\frac{1}{M}\sum_m \hat{\theta}_m^2
&\overset{p}\rightarrow E(\hat{\theta}^2).
\end{aligned}
$$

## The Error of Measuring Error

There are two sources of error that can arise from this approach:
simulation error and estimation error.

Simulation error occurred because we are using Monte Carlo finite repetitions.
Increasing the number of repetitions can effectively decrease such error.
Thanks to the wide-spread computing power nowadays,
this is not a big issue.

Estimation error is the error due to statistical noise when we use  $\hat{F}$ to approximate $F$ and by nature cannot be eliminated.

## Hands-On: The Sample Mean

Let's demonstrate the use of bootstrapping with our best friend sample mean.
Assume a Beta distribution as our unknown population with the PDF:

```{r beta_population_pdf}
beta_a <- 2
beta_b <- 10

beta_pdf <- function(x) dbeta(x, beta_a, beta_b)
curve(beta_pdf, from=0, to=1, ylab="Density",
      main=sprintf("(Unknown) Population Beta(%s, %s) PDF", beta_a, beta_b))
```

and the CDF:

```{r beta_population_cdf}
beta_cdf <- function(x) pbeta(x, beta_a, beta_b)
curve(beta_cdf, from=0, to=1, ylab="Density",
      main=sprintf("(Unknown) Population Beta(%s, %s) CDF", beta_a, beta_b))
```

Now we draw a random sample with fair size from the population.
We plot the resulting histogram (probability mass function, or PMF):

```{r beta_sample_pmf}
set.seed(666)
n <- 5000
x <- rbeta(n, beta_a, beta_b)
hist(x, main="Sample X", probability=TRUE)
```

and the EDF:

```{r beta_sample_edf}
plot(ecdf(x), main="Empirical Distribution Function of X", xlim=c(0, 1))
```

### Analytical Solution

The variance of sample mean is analytically trackable,
so indeed we don't really need bootstrap to tackle this problem.
But we can use it to verify if bootstrap can result in the correct approximation to our analytical solution.

Here is the derivation of the variance of sample mean,
provided that the sample is i.i.d.:

$$
\begin{aligned}
Var(\frac{1}{n}\sum_ix_i)
&= \frac{1}{n^2}Var(\sum_ix_i) \\
&= \frac{1}{n^2}\sum_iVar(x_i) &\text{(due to independently distributed)} \\
&= \frac{1}{n^2}\cdot n Var(x) &\text{(due to identically distributed)} \\
&= \frac{Var(x)}{n}.
\end{aligned}
$$

Obviously the variance goes to zero as sample size goes to infinity.
Hence sample mean as an estimator is *consistent*.

We can readily calculate the standard error of sample mean given our sample:

```{r sample_mean_analytical}
# Assume knowing population s.d. (the groud truth).
var_beta <- (beta_a * beta_b) / ((beta_a + beta_b)^2*(beta_a + beta_b + 1))
sqrt(var_beta / n)
```

But remember that we don't know our population so there is no way we have access to the variance of population (the numerator).
So instead we will almost always use sample variance to approximate the solution:

```{r sample_mean_analytical_2}
# Plugin sample standard deviation since population s.d. is unknown.
sd(x) / sqrt(n)
```

### Monte Carlo with Multiple Samples

Now let's say we don't know the analytical solution.
If we can draw more than just one sample dataset,
we are able to use Monte Carlo to approximate the error of our estimator:
^[Here we didn't adjust the denominator to $n-1$ in the formula we used.
using function `sd` will do that by default.
Theoretically the adjustment make the estimation unbiased.
The effect is negligible though since we control the number of repetitions,
which is usually a big enough number.]

```{r sample_mean_mc}
M <- 10000

mc_xbar <- rep(NA_real_, M)
for ( i in seq_len(M) ) {
  xm <- rbeta(n, beta_a, beta_b)
  mc_xbar[i] <- mean(xm)
}
sqrt(sum(mc_xbar^2) / M - (sum(mc_xbar) / M)^2)  # Or simply sd(mc_xbar).
```

The simulated distribution is the [*sampling distribution*](https://en.wikipedia.org/wiki/Sampling_distribution) of our estimator.
CLT tells us that the sampling distribution of sample mean is asymptotically Normal.
Here without even knowing that,
we can just use Monte Carlo to approximate the sampling distribution.
But this is hypothetical only.
In reality we don't have access to multiple random samples since we don't have access to the population,
so this approach only exists in theory.

Here we can plot the simulated estimates.
It is a finite sample approximation to the sampling distribution.
We also plot the theoretical limiting distribution given by CLT in blue curve.

```{r sample_mean_mc_dist}
x_ <- seq(min(mc_xbar), max(mc_xbar), length=100) 
y_ <- dnorm(x_, beta_a / (beta_a + beta_b), sqrt(var_beta/n))
hist(mc_xbar, main="Sampling Distribution (Sample Mean)", xlab="X", probability=TRUE)
lines(x_, y_, col="blue", lwd=2)
```

When CLT applies,
we can bypass the need to use multiple sample datasets and instead use only one to derive our statistical inference.
(See how closely the simulated sampling distribution in histogram follow the theoretical limiting distribution in blue curve.)
Bootstrapping is yet another route to the solution.
Even though we only have one sample dataset,
and without applying CLT,
we can use bootstrapping to generate as many *resamples* as we want.
And this is totally feasible given enough computing power.

### Monte Carlo Bootstrapping

The same Monte Carlo setup,
but now each sample is generated by bootstrapping from the only sample we have at hand:

```{r sample_mean_boot}
M <- 10000

bs_xbar <- rep(NA_real_, M)
for ( i in seq_len(M) ) {
  bs <- sample(x, size=length(x), replace=TRUE)  # The bootstrap.
  bs_xbar[i] <- mean(bs)
}
sqrt(sum(bs_xbar^2) / M - (sum(bs_xbar) / M)^2)  # Or simply sd(bs_xbar).
```

As one can see,
the approximation is as good as if we hypothetically created multiple samples from the original population.

Below we plot all bootstrapped sample means.
We also plot a green curve as the sampling distribution as if the original sample is the population.
(That is, the distribution will center at the original sample mean since now it is the population mean.)

```{r sample_mean_bs_dist}
x_ <- seq(min(bs_xbar), max(bs_xbar), length=100) 
y_ <- dnorm(x_, mean(x), sqrt(var_beta/n))
hist(bs_xbar, probability=TRUE, main="Distribution of Bootstrapped Sample Mean", xlab="X")
lines(x_, y_, col="green", lwd=2)
```

Rather than implement from scratch,
we can also take advantage of the built-in package `boot` [@boot] in R to simplify our code:

```{r sample_mean_boot_2}
library(boot)  # This is a built-in package.

bs_r <- boot(x, function(x, i) mean(x[i]), R=M, parallel="multicore")
print(bs_r)
```

The bias reported by `boot` is simply the difference between the average of our simulated estimates and the original estimate:

```{r verify_bias}
mean(bs_r$t) - mean(x)
```

It should be very close to zero if nothing went wrong.

Since we now have access to all simulated estimates,
we can easily construct the confidence interval as well:

```{r sample_mean_boot_ci}
quantile(bs_r$t, probs=c(.025, .975))
```

Of course for simple estimator such as sample mean,
we do have the analytical solution for its confidence interval.
But bootstrapping is able to do the approximation without knowing the solution.

Another possible (though higly similar to variance) metric we may want to look at is the mean squared error (MSE).
We can approximate MSE of our estimator using bootstrapping:

$$
\text{MSE}(\hat{\theta}) = \frac{1}{M}\sum_m(\hat{\theta}_m - \hat{\theta})^2.
$$

So for the sample mean example,
we can do:

```{r sample_mean_mse}
# We use root mean squared error instead since the number is small.
sqrt(mean((bs_r$t - mean(x))^2))
```

# Bootstrapping for Regression Models

**TODO: residual bootstrapping**

# References
