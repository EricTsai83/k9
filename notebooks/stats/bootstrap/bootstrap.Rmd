---
title: "Bootstrap Sampling 101"
subtitle: ""
author:
- name: Kyle Chung
  affiliation:
date: "`r format(Sys.time(), '%d %b %Y')` Last Updated (10 Jan 2020 First Uploaded)"
output:
  html_notebook:
    highlight: tango
    number_sections: yes
    theme: paper
    toc: yes
    toc_depth: 3
    toc_float: yes
    includes:
      in_header: /tmp/meta_header.html
  code_download: true
bibliography: bootstrap.bib
nocite: |
  @boot2
link-citations: yes
abstract: |
  We review Monte Carlo bootstrap sampling approach, a powerful technique to measure error of a statistical inference task when the variance of our estimator is not analytically trackable. We explain why bootstrapping works with minimum amount of theoretical derivation and demonstration of hands-on examples.
---

```{r meta, include=FALSE}
meta_header_file <- file("/tmp/meta_header.html")

# Add open graph meta.
meta <- c(
  '<meta name="author" content="Kyle Chung">',
  '<meta property="og:title" content="Bootstrap Sampling">',
  '<meta property="og:type" content="article">',
  '<meta property="og:url" content="https://everdark.github.io/k9/notebooks/stats/bootstrap/bootstrap.nb.html">',
  '<meta property="og:image" content="https://everdark.github.io/k9/assets/androidify.jpg">',
  '<meta property="og:description" content="A data science notebook about bootstrap sampling.">'
)
contents <- meta

# Add Github corner.
github_corner_svg <- "../../../assets/github_corner.html"
github_corner_conf <- list(github_link="https://github.com/everdark/k9/tree/master/notebooks/stats/bootstrap")
contents <- c(contents, stringr::str_interp(readLines(github_corner_svg), github_corner_conf))
writeLines(contents, meta_header_file)

close(meta_header_file)
```

# Statistical Inference

At the core of statistical inference is about to use an estimator $\hat{\theta}(\cdot)$ to guess the unknown value of $\theta$ a population parameter of interest.
The most common case of $\theta$ will be the population mean $\mu = E(x)$ where $x$ denotes the population following an unknown distribution $x \sim F$.
Other popular parameters can be a median,
a confidence interval of the mean,
or a regression model coefficient,
a confidence interval of the regression coefficient, ..., etc.

How do we know our guess is a good one?
How do we measure the error of our estimator in statistical inference,
without knowing the ground truth of the population?
The probability distribution function (PDF) of population $F$ is in general unknown.
As a result,
to make our statistical inference from a sample dataset we rely on the [Central Limit Theorem](https://en.wikipedia.org/wiki/Central_limit_theorem).
In its vanilla version CLT only requires the population to have finite second moment and our sample dataset is identically and independently distributed (i.i.d.).
It allows us to derive the [limiting distribution](https://en.wikipedia.org/wiki/Asymptotic_distribution) of the [*sampling distribution*](https://en.wikipedia.org/wiki/Sampling_distribution) of a sample mean,
which in turn can be used to measure the error of our guess $\hat{\theta}$ (formally termed as a *statistic*) to the unknown population parameter $\theta$.

Different versions of CLT can be formulated based on the estimator we are using,
in order to derive the sampling distribution of that estimator.
So we also have CLT applicable to, say, a regression model coefficient.
The problem is that,
we may not have analytical solution for every possible estimators.
The more complicated the estimator the harder it can be analyzed in a trackable way.
And some estimators can be harder to deal with even if they are simple in their own.
One such well-known example is the median,
which requires order statistics come to play.

This is where bootstrap sampling starts to shine.
The idea of boostrap sampling is to use the sample and the only sample as a surrogate population,
to approximate the underlying sampling distribution which otherwise is untrackable.

Before we dive into boostrap sampling,
there are a couple of topics we need to review to prepare ourselves.

## Empirical Distribution

From [Wikipedia](https://en.wikipedia.org/wiki/Empirical_distribution_function):

>In statistics,
an empirical distribution function is the distribution function associated with the empirical measure of a sample.
This cumulative distribution function is a step function that jumps up by 1/n at each of the n data points.
Its value at any specified value of the measured variable is the fraction of observations of the measured variable that are less than or equal to the specified value.

Specifically,
the empirical distribution function (EDF) and empirical PDF of a sample $x \sim F$ of size $n$ can be written as:

$$
\begin{aligned}
\hat{F}_n(x) &= \frac{1}{n} \sum_{i=1}^nI(x_i \leq x), \\
\hat{p}_n(x) &= \frac{d\hat{F}_n}{dx} = \frac{1}{n},
\end{aligned}
$$

where $I$ is an indicator function.

Intuitively and loosely speaking,
$\hat{F}$ is just an estimator for $F$ the true population distribution.
But here we are not guesing a single population parameter but the entire distribution.
That is,
we are not guesing a scalar but a function.
^[Here our wording is a bit *Fisherian*.
Because in a Baysian point of view even a population parameter is a function (a random variable),
not a constant scalar.]

## Parameters as Functions of CDF

Why introducing the notion of EDF?
To see the value,
it's better to step back and examine our best-friend parameter:
population mean $E(x)$.
By definition the derivative of a CDF $F$ is a PDF $p$:

$$
\frac{dF(x)}{dx} = p(x).
$$

Now we observe that the population mean can be re-written as:

$$
E(x) = \int xp(x)dx = \int xdF(x).
$$

That is to say,
population mean is indeed *a function of CDF*.
We can easily extend this to the variance as a function of CDF as well,
and to many other statistics.

Of course we don't know CDF of the population.
But we do know the EDF from our observed sample dataset,
which itself is an estimator for CDF.
This immediately gives us one estimator of the form:

$$
\begin{aligned}
E(x) \approx \int xd\hat{F}_n(x)
&= \int x\hat{p}_n(x)dx \\
&= \sum_{i=1}^n x \cdot \frac{1}{n} \\
&= \frac{1}{n}\sum_{i=1}^nx_i,
\end{aligned}
$$

to guess the population mean.
^[This is based on the [plug-in principle](https://en.wikipedia.org/wiki/Plug-in_principle).]

To no surprise,
this estimator is exactly what we call the *sample mean*,
proven to be a very good estimator for the population mean.
But the idea generalizes to *any* estimator,
as long as the estimator can be expressed as a function of CDF,
which in turn will be approximated by EDF.
In the literature such estimators are also referred to as *statistical functionals*.

More formally,
under i.i.d. sample we have:

$$
\begin{aligned}
E\big[\hat{F}_n(a)\big]
&= \frac{1}{n}E\big[\sum_iI(x_i \leq a)\big] \\
&= \frac{1}{n}\sum_iE\big[I(x_i \leq a)\big] \\
&= \frac{1}{n} \cdot n \cdot \Pr(x \leq a) \\
&= F(a).
\end{aligned}
$$

Hence EDF is *unbiased* in every value $a$.
This means that any statistical functional which can be expressed as a linear function of EDF $G(\hat{F}_n)$,
will be unbiased as well:

$$
E\big[G(\hat{F}_n)\big] = G(F).
$$

## DKW Inequality

One thing remains unclear for those who are skeptical:
How good does $\hat{F}_n$ approximate $F$?

The good news is,
we do have a sound theoretical ground describing the bounds how close EDF will be to CDF.
This is referred to as the [Dvoretzky–Kiefer–Wolfowitz inequality](https://en.wikipedia.org/wiki/Dvoretzky%E2%80%93Kiefer%E2%80%93Wolfowitz_inequality):

$$
\Pr\big(
\sup_{x \in R}\vert \hat{F}_n(x) - F(x) \vert > \epsilon
\big) \leq 2e^{-2n\epsilon^2},\forall\epsilon > 0.
$$

In plain words,
EDF converges uniformly to CDF in $n$ with exponential speed.
^[Notation $\sup$ denotes the [supremum](https://en.wikipedia.org/wiki/Infimum_and_supremum):
The smallest number that is greater than or equal to every number in the set.]

## Sampling Distribution Approximation

The sampling distribution of our estimator is the ground to measure the quality of that estimator.
As we already mentioned the classical way of deriving sampling distribution is through CLT whenver applicable.
Now based on the fact that the empirical distribution is a good estimator for the population distribution,
another solution comes to play: Bootstrapping.

In short,
bootstrapping is a sampling technique to draw estimates that,
under certain conditions,
will approximate the sampling distribution of that estimator,
without knowing the closed form solution to that distribution.

# Bootstrapping

@efron1992bootstrap propose the idea of bootstrapping as a solution to estimate the variance (or the confidence interval) of a given estimator,
such that we can draw conclusion about the quaility of our inference,
especially when there is no analytical solution to derive the variance of that estimator.

Bootstrapping can be simply defined as drawing i.i.d. sample of the same size $n$ from a given empirical distribution.
In some literatures this is also called the *resample*.
To strictly follow i.i.d. the sampling must be done *with replacement*.

To do bootstrap we essentially take the random sample $X$ from the original population $F$,
and do the i.i.d. sampling from that sample *as if* the sample itself is another population $\hat{F}$.
Only in this time the population is finite and has a discrete distribution function,
which is the EDF $\hat{F}_n(x)$.

## Bootstrap Distribution

The resulting distribution from a sample is sometimes referred to as the bootstrap distribution.
It has been proven that the limiting distribution (as $n \rightarrow \infty$) of a bootstrap distribution is the same as that of the sampling distribution,
for a variety of statistics widely used for inference. [@singh1981asymptotic]

To be clear let's put it in notation:

$$
\underbrace{(\hat{\theta}_b - \hat{\theta})}_\text{Bootstrap Distribution}
\overset{d}\rightarrow
\underbrace{(\hat{\theta} - \theta)}_\text{Sampling Distribution},
$$

where $\hat{\theta}$ is the estimate derived from our only sample to infer the parameter $\theta$,
and $\hat{\theta}_b$ is the estimate derived from the bootstrap sample out of the sample.
^[Though we didn't discuss the technical details,
the proof of this convergence is indeed based on CLT.]

Due to realistic reason we only have one sample to derive $\hat{\theta}$,
but we can do as many as bootstrap samples from that sample to derive many $\hat{\theta}_b$.
Knowing that under large sample size (of the original sample) the bootstrap distribution will approximate the sampling distribution,
we can estimate the variance of our estimator without knowing its closed form solution.
We've already discussed the intuition why this is the case.
It can be mostly attributed to the fact that sample EDF is a good estimator of population CDF.

## Monte Carlo Bootstrapping

Now given a particular estimator $\hat{\theta}$ as a random variable,
we are concerning about its expectation and (especially) variance.
The expectation tells us whether the estimator is *unbiased* or not.
The variance tells us how volatile our guess is,
and also if it can be *consistent*--converge to the true parameter of interest.

By definition,
the variance of a random variable $\hat{\theta}$ is:

$$
\begin{aligned}
Var(\hat{\theta})
&= E\big[(\hat{\theta} - E(\hat{\theta}))^2\big] \\
&= E(\hat{\theta}^2) - E(\hat{\theta})^2.
\end{aligned}
$$

Now to approximate the above result with Monte Carlo of $M$ repetitions,
we can simply do:

$$
Var(\hat{\theta}) \approx
\frac{1}{M}\sum_m \hat{\theta}_m^2 -
\bigg(\frac{1}{M}\sum_m \hat{\theta}_m\bigg)^2 ,
$$

where each $\hat{\theta}_m$ is derived from a bootstrapped sample at the $m$-th repetition.

The above approximation holds thanks to the [Law of Large Numbers](https://en.wikipedia.org/wiki/Law_of_large_numbers) (as $M \rightarrow \infty$):

$$
\begin{aligned}
\frac{1}{M}\sum_m \hat{\theta}_m
&\overset{p}\rightarrow E(\hat{\theta}), \\
\frac{1}{M}\sum_m \hat{\theta}_m^2
&\overset{p}\rightarrow E(\hat{\theta}^2).
\end{aligned}
$$

Remember that now we express the estimator $\hat{\theta}(x)$ as function of EDF: $\hat{\theta}(\hat{F}_n(x))$,
i.e.,
a statistical functional.
Now to draw random sample from this function we do i.i.d. sampling with replacement against $\hat{F}_n$.
Once the sample is constructed,
we calculate the corresponding estimate value,
and we do this for many many times to derive the entire distribution of the resulting estimates.

## The Error of Measuring Error

There are two sources of error that can arise from this approach:
simulation error and estimation error.

Simulation error occurred because we are using Monte Carlo finite repetitions.
Increasing the number of repetitions can effectively decrease such error.
Thanks to the wide-spread computing power nowadays,
this is not a big issue at all.

Estimation error is the error due to statistical noise when we use  $\hat{F}_n$ to approximate $F$ and by nature cannot be fully eliminated.
To reduce this error we will need a larger $n$.
Remember that the DKW inequality tells us that the convergence is at exponential speed in $n$.

## Hands-On: The Sample Mean

Let's demonstrate the use of bootstrapping with our best friend sample mean.
Assume a Beta distribution as our unknown population with the PDF:

```{r beta_population_pdf}
beta_a <- 2
beta_b <- 10
mu_beta <- beta_a / (beta_a + beta_b)
var_beta <- (beta_a * beta_b) / ((beta_a + beta_b)^2*(beta_a + beta_b + 1))

beta_pdf <- function(x) dbeta(x, beta_a, beta_b)
curve(beta_pdf, from=0, to=1, ylab="Density",
      main=sprintf("(Unknown) Population Beta(%s, %s) PDF", beta_a, beta_b))
```

and the CDF:

```{r beta_population_cdf}
beta_cdf <- function(x) pbeta(x, beta_a, beta_b)
curve(beta_cdf, from=0, to=1, ylab="Density",
      main=sprintf("(Unknown) Population Beta(%s, %s) CDF", beta_a, beta_b))
```

Now we draw a random sample with fair size from the population.
We plot the resulting histogram (probability mass function, or PMF):

```{r beta_sample_pmf}
set.seed(666)
n <- 5000
x <- rbeta(n, beta_a, beta_b)
hist(x, main="Probability Mass Function of X", probability=TRUE)
```

and the EDF:

```{r beta_sample_edf}
plot(ecdf(x), main="Empirical Distribution Function of X", xlim=c(0, 1))
```

### Analytical Solution

The variance of sample mean is analytically trackable,
so indeed we don't really need bootstrap to tackle this problem.
But we can use it to verify if bootstrap can result in the correct approximation to our analytical solution.

Here is the derivation of the variance of sample mean,
provided that the sample is i.i.d.:

$$
\begin{aligned}
Var(\frac{1}{n}\sum_ix_i)
&= \frac{1}{n^2}Var(\sum_ix_i) \\
&= \frac{1}{n^2}\sum_iVar(x_i) &\text{(due to independently distributed)} \\
&= \frac{1}{n^2}\cdot n Var(x) &\text{(due to identically distributed)} \\
&= \frac{Var(x)}{n}.
\end{aligned}
$$

Obviously the variance goes to zero as sample size goes to infinity.
Hence sample mean as an estimator is *consistent*.

We can readily calculate the standard error of sample mean given our sample:

```{r sample_mean_analytical}
# Assume knowing population s.d. (the ground truth).
sqrt(var_beta / n)
```

But remember that we don't know our population so there is no way we have access to the variance of population (the numerator).
So instead we will almost always use sample variance to approximate the solution:

```{r sample_mean_analytical_2}
# Plugin sample standard deviation since population s.d. is unknown.
sd(x) / sqrt(n)
```

### Monte Carlo with Multiple Samples

Now let's say we don't know the analytical solution.
If we can draw more than just one sample dataset,
we are able to use Monte Carlo to approximate the error of our estimator:
^[Here we didn't adjust the denominator to $n-1$ in the formula we used.
using function `sd` will do that by default.
Theoretically the adjustment make the estimation unbiased.
The effect is negligible though since we control the number of repetitions,
which is usually a big enough number.]

```{r sample_mean_mc}
M <- 10000

mc_sampling <- function(M, n, print=TRUE) {
  mc_xbar <- rep(NA_real_, M)
  for ( i in seq_len(M) ) {
    xm <- rbeta(n, beta_a, beta_b)
    mc_xbar[i] <- mean(xm)
  }
  v <- sqrt(sum(mc_xbar^2) / M - (sum(mc_xbar) / M)^2)  # Or simply sd(mc_xbar).
  if ( print ) print(v)
  mc_xbar
}

mc_xbar <- mc_sampling(M, n)
```

The simulated distribution is the sampling distribution of our estimator.
CLT tells us that the sampling distribution of sample mean is asymptotically Normal.
Here without even knowing that,
we can just use Monte Carlo to approximate the sampling distribution.
But this is hypothetical only.
In reality we don't have access to multiple random samples since we don't have access to the population,
so this approach only exists in theory.

Here we can plot the simulated estimates.
It is a finite sample approximation to the sampling distribution.
We also plot the theoretical limiting distribution given by CLT in blue curve.

```{r sample_mean_mc_dist}
x_mc <- seq(min(mc_xbar), max(mc_xbar), length=100) 
y_mc <- dnorm(x_mc, mu_beta, sqrt(var_beta / n))
hist(mc_xbar, main="Sampling Distribution (Sample Mean)", xlab="X", probability=TRUE)
lines(x_mc, y_mc, col="blue", lwd=2)
```

When CLT applies,
we can bypass the need to use multiple sample datasets and instead use only one to derive our statistical inference.
(See how closely the simulated sampling distribution in histogram follow the theoretical limiting distribution in blue curve.)
Bootstrapping is yet another route to the solution.
Even though we only have one sample dataset,
and without applying CLT,
we can use bootstrapping to generate as many *resamples* as we want.
And this is totally feasible given enough computing power.

### Monte Carlo Bootstrapping

The same Monte Carlo setup,
but now each sample is generated by bootstrapping from the only sample we have at hand:

```{r sample_mean_boot}
M <- 10000

bs_sampling <- function(x, M, print=TRUE) {
  bs_xbar <- rep(NA_real_, M)
  for ( i in seq_len(M) ) {
    bs <- sample(x, size=length(x), replace=TRUE)  # The bootstrap.
    bs_xbar[i] <- mean(bs)
  }
  v <- sqrt(sum(bs_xbar^2) / M - (sum(bs_xbar) / M)^2)  # Or simply sd(bs_xbar).
  if ( print ) print(v)
  bs_xbar
}

bs_xbar <- bs_sampling(x, M)
```

As one can see,
the approximation is as good as if we hypothetically created multiple samples from the original population.

Below we plot all bootstrapped sample means.
We also plot a green curve as the sampling distribution as if the original sample is the population.
(And the original sample mean becomes the "population mean".)

```{r sample_mean_bs_dist}
x_bs <- seq(min(bs_xbar), max(bs_xbar), length=100)
y_bs <- dnorm(x_bs, mean(x), sqrt(var_beta / n))
hist(bs_xbar, probability=TRUE,
     main="Distribution of Bootstrapped Sample Mean",
     sub=sprintf("Sample Size = %i", n), xlab="X")
lines(x_mc, y_mc, col="blue", lwd=2)
lines(x_bs, y_bs, col="green", lwd=2)
```

Notice that we also plot the blue curve as the limiting distribution of sample mean.
Our previous Monte Carlo exercise has already confirmed this is the sampling distribution based on CLT.
Clearly the bootstrapped estimates does NOT magically approximate the sampling distribution.
There is a bias introduced by the original sample mean which is statistically deviated from the true mean.
But they do share the same variance (shape).
And since we are asking for the variance of our estimator,
such bias does not affect our estimation of variance at all!
^[In reality we don't have access to the population so we will never be able to use either Monte Carlo or CLT to derive the true mean.
This is where we further use [hypothesis testing](https://en.wikipedia.org/wiki/Statistical_hypothesis_testing) to draw statistical insight about the population based on our sample.]

### Bootstrap v.s. Sampling Distribution

So the question is,
can the resulting bootstrapped estimates approximate the sampling distribution in both shape and location?
The answer is yes,
but we need a large enough sample size.
Let's double up our original sample size and redo the entire exercise:

```{r double_sample_size}
n2 <- 10000
x2 <- rbeta(n2, beta_a, beta_b)

mc_xbar2 <- mc_sampling(M, n2, print=FALSE)
bs_xbar2 <- bs_sampling(x2, M, print=FALSE)

x_mc2 <- seq(min(mc_xbar2), max(mc_xbar2), length=100)
y_mc2 <- dnorm(x_mc2, mu_beta, sqrt(var_beta / n2))
x_bs2 <- seq(min(bs_xbar2), max(bs_xbar2), length=100)
y_bs2 <- dnorm(x_bs2, mean(x2), sqrt(var_beta / n2))
hist(bs_xbar2, probability=TRUE,
     main="Distribution of Bootstrapped Sample Mean",
     sub=sprintf("Sample Size = %i", n2), xlab="X")
lines(x_mc2, y_mc2, col="blue", lwd=2)
lines(x_bs2, y_bs2, col="green", lwd=2)
```

Now the two distributions are getting closer to each other.
We also observe that the requirement on sample size for bootstrap to approximate (both shape and location) the limiting distribution is much stricter than that for CLT to be applicable.
^[Some researchers recommend $n^2$ repetitions where $n$ is the original sample size.
Or $n\ln n$ if $n^2$ is prohibitive.]

To show this clearly,
we run another Monte Carlo simulation with only sample size of 100,
and we can see how well the sampling distribution is approximating the limiting distribution already:

```{r clt_small_sample}
n3 <- 100
x3 <- rbeta(n3, beta_a, beta_b)
mc_xbar3 <- mc_sampling(M, n3, print=FALSE)
x_mc3 <- seq(min(mc_xbar3), max(mc_xbar3), length=100) 
y_mc3 <- dnorm(x_mc3, mu_beta, sqrt(var_beta / n3))
hist(mc_xbar3, main="Sampling Distribution (Sample Mean)",
     sub=sprintf("Sample Size = %i", n3), xlab="X", probability=TRUE)
lines(x_mc3, y_mc3, col="blue", lwd=2)
```

This suggests that whenver we do have analytical solution for the limiting distribution of our estimator based on the corresponding CLT,
we should just use that for our inference task instead of trying bootstrapping.
^[Any old-school textbooks may mention the rule-of-thumb sample size of 30 as a generally accepted large enough number.
This is over-simplification and can be dangerous in practice.
Whether a sample size is big enough for CLT to have good approximation really depends on the nature of the population distribution,
which unfortunately is unknown in the first place.
There are some heuristics, though.
For example the more symetric the population the less sample size required to have a good approximation.]

So, yes, for a classical statistic like sample mean,
there is not much value added using bootstrap.
But it can serve as a very good educational case study.

### Using Package `boot`

Rather than implementing the sampling procedure from scratch,
we can also take advantage of the built-in package `boot` [@boot] in R to simplify our code:

```{r sample_mean_boot_2}
library(boot)  # This is a built-in package.

bs_r <- boot(x, function(x, i) mean(x[i]), R=M, parallel="multicore")
print(bs_r)
```

The bias reported by `boot` is simply the difference between the average of our simulated estimates and the original estimate:

```{r verify_bias}
mean(bs_r$t) - mean(x)
```

It should be very close to zero if nothing went wrong.

#### Confidence Interval {-}

Since we now have access to all simulated estimates,
we can easily construct the confidence interval as well:

```{r sample_mean_boot_ci}
quantile(bs_r$t, probs=c(.025, .975))
```

Of course for simple estimator such as sample mean,
we do have the analytical solution for its confidence interval.
But bootstrapping is able to do the approximation without knowing the solution.

Be aware that when sample size is small,
as what we have shown above,
it can be the case that only the variance of the sampling distribution is approximated well but the location is not.
So the confidence interval may well be biased.

#### Mean Squared Error {-}

Another possible (though higly similar to variance) metric we may want to look at is the mean squared error (MSE).
We can approximate MSE of our estimator using bootstrapping:

$$
\text{MSE}(\hat{\theta}) = \frac{1}{M}\sum_m(\hat{\theta}_m - \hat{\theta})^2.
$$

So for the sample mean example,
we can do:

```{r sample_mean_mse}
# We use root mean squared error instead since the number is small.
sqrt(mean((bs_r$t - mean(x))^2))
```

## Hands-On: T-Statistic

The [t-statistic](https://en.wikipedia.org/wiki/T-statistic) is:

$$
t = \frac{\bar{X} - \mu}{s / \sqrt{n}},
$$

where $s$ is the standard deviation of sample $X$.

For our demo example the groud truth of t-statistic is:

```{r t_stat_true}
(mean(x) - mu_beta) / (sd(x) / sqrt(n))  # T-stat.
```

Or we can use the built-in test function to also compute the confidence interval conveniently:

```{r t_stat_ci}
t.test(x, mu=mu_beta)
```

But in reality of course we don't know $\mu$,
so the above information is not available.
Instead we can use the bootstrap t-statistic to approximate the unknown true t-statistic:

$$
t_b = \frac{\bar{X}_b - \bar{X}}{s_b / \sqrt{n}},
$$

The original t-statistic gives the following interval:

$$
\mu = \bar{X}\pm t\times \frac{s}{\sqrt{n}}.
$$

It is possible to derive the approximated confidence interval of t-statistic using bootstrap without knowing the parameter $mu$.
All we need to do is to replace $t$ with $t_b$ in the above equation:

$$
\mu \approx \bar{X}\pm t_b\times \frac{s}{\sqrt{n}}.
$$

The exact exercise follows.

```{r t_stat_bs}
# Re-define the bootstrapper since this time we need access to sd(x_b) as well.
t_bs_sampling <- function(x, M) {
  bs_xbar <- rep(NA_real_, M)
  bs_sd <- rep(NA_real_, M)
  for ( i in seq_len(M) ) {
    bs <- sample(x, size=length(x), replace=TRUE)  # The bootstrap.
    bs_xbar[i] <- mean(bs)
    bs_sd[i] <- sd(bs)  # Compute also standard deiviation for each resample.
  }
  list(xbar=bs_xbar, sd=bs_sd)
}

res <- t_bs_sampling(x, M)
bs_t <- (res$xbar - mean(x)) / (res$sd/sqrt(n))  # Bootstrap t-statistics.

# Bootstrapped confidence interval of mu using t-statistic.
mean(x) + (sd(x) / sqrt(n)) * quantile(bs_t, c(.025, .975))
```

Remember that in this simulated dataset our bootstrap distribution is a bit biased to the right in absolute term.
But that seems not affect our approximation of confidence interval using t-statistic.
Why?
This is referred to as second-order correction of bootstrap in the literature.
It is the result of the de-mean in numerator and the division by standard error.
In the previous visualization what we plot is the distribution $\hat{\theta}$ v.s. $\hat{\theta}_b$.
But the approximation is better for $(\hat{\theta}_b - \hat{\theta})$ v.s. $(\hat{\theta} - \theta)$,
and even better for $\frac{(\hat{\theta}_b - \hat{\theta})}{se_b}$ v.s. $\frac{(\hat{\theta} - \theta)}{se}$.
To keep the scope manageable we will skip the proof here.

# Bootstrapping for Regression Models

**TODO: residual bootstrapping**

# References
